{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "coursework-load-data",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1XbnJi6EIxrtxha25vYcGcYUIYvJmG4s_",
      "authorship_tag": "ABX9TyMZL5FUhjjn05s2LFYieWim"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "83577cb3d79c4b338a9b6f4eb714b997": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9914ea982f74584a4ca498bfe8f361e",
              "IPY_MODEL_65a6d81c8e534680a17bff3a0439da67"
            ],
            "layout": "IPY_MODEL_937fe692969d43e793a49bb84c27dc0c"
          }
        },
        "d9914ea982f74584a4ca498bfe8f361e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": " 47%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34912d639aa843cc8e0cb5c602ffc601",
            "max": 11060,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36844d4e382947f8877173ae6be78366",
            "value": 5173
          }
        },
        "65a6d81c8e534680a17bff3a0439da67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14e0d1864ae74d67b207add340616754",
            "placeholder": "​",
            "style": "IPY_MODEL_6502e1dae6ac4c6faf6fb2195217f9d3",
            "value": " 5173/11060 [46:38&lt;55:29,  1.77it/s]"
          }
        },
        "937fe692969d43e793a49bb84c27dc0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34912d639aa843cc8e0cb5c602ffc601": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36844d4e382947f8877173ae6be78366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "14e0d1864ae74d67b207add340616754": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6502e1dae6ac4c6faf6fb2195217f9d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e167b07d961e46a0b0a61fefce000704": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3e871f28b6e4d5b849944714fbe6aa5",
              "IPY_MODEL_f9849409d52c48dcaa4c5aed1d3aed4b"
            ],
            "layout": "IPY_MODEL_242d45d64b3f42009716c78623e4e917"
          }
        },
        "f3e871f28b6e4d5b849944714fbe6aa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aaacf402388846f7a46010ca9e4b2368",
            "max": 2739,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1cbfeb784374238be51ee63498864bf",
            "value": 2739
          }
        },
        "f9849409d52c48dcaa4c5aed1d3aed4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62dd8575711a4d3cb5824b04661382bf",
            "placeholder": "​",
            "style": "IPY_MODEL_48e85d0abb524dacbbd14e855f62e59d",
            "value": " 2739/2739 [36:19&lt;00:00,  1.26it/s]"
          }
        },
        "242d45d64b3f42009716c78623e4e917": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaacf402388846f7a46010ca9e4b2368": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1cbfeb784374238be51ee63498864bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "62dd8575711a4d3cb5824b04661382bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48e85d0abb524dacbbd14e855f62e59d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18ecb5d991bb4580a4ffcb4361f16a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4163aa1c6c7f4153a9cb509649b27b81",
              "IPY_MODEL_c5cc4451459b4c8d802bab213bef9fb2"
            ],
            "layout": "IPY_MODEL_4870230e7edc49b4a9fbb9f95831d40b"
          }
        },
        "4163aa1c6c7f4153a9cb509649b27b81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b473178814743aab8215130df0a6b18",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_73666d7517a34b1681ad253b7a92345b",
            "value": 300
          }
        },
        "c5cc4451459b4c8d802bab213bef9fb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22abc27fd1164da6a78f22c09fb9f216",
            "placeholder": "​",
            "style": "IPY_MODEL_d832abb6af2f4cf9a6910576c3b4630b",
            "value": " 300/300 [01:23&lt;00:00,  3.60it/s]"
          }
        },
        "4870230e7edc49b4a9fbb9f95831d40b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b473178814743aab8215130df0a6b18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73666d7517a34b1681ad253b7a92345b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "22abc27fd1164da6a78f22c09fb9f216": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d832abb6af2f4cf9a6910576c3b4630b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4b5c156d444445fa505fdf771702829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_30761ddd7a814f40bb6ed831324298fb",
              "IPY_MODEL_9a1bcc87de5846e7bab3b8760d2d618c"
            ],
            "layout": "IPY_MODEL_44a89a41de3d40b184f42ab35eae8519"
          }
        },
        "30761ddd7a814f40bb6ed831324298fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f04701b7c314d36832f3deedd4a4b09",
            "max": 11008,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c85275fcb9241c6b3e839c5d12ca0d8",
            "value": 11008
          }
        },
        "9a1bcc87de5846e7bab3b8760d2d618c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e155aec75ddc4e87a4904696a64f2062",
            "placeholder": "​",
            "style": "IPY_MODEL_80f2959b267c4e4bbd627252e6f6d7f8",
            "value": " 11008/11008 [05:52&lt;00:00, 31.26it/s]"
          }
        },
        "44a89a41de3d40b184f42ab35eae8519": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f04701b7c314d36832f3deedd4a4b09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c85275fcb9241c6b3e839c5d12ca0d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "e155aec75ddc4e87a4904696a64f2062": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80f2959b267c4e4bbd627252e6f6d7f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0e5aa7993d243fa908a604a90d8c4f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dbfd2029a2a541579837784f565244ea",
              "IPY_MODEL_34d50ed2938346889be334cbbecd1a29"
            ],
            "layout": "IPY_MODEL_f4bdadff7b534a48972f17c70c798adf"
          }
        },
        "dbfd2029a2a541579837784f565244ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4134cda753e240a29c25b87b3e57e27d",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c8ae31f210d434080b4c8af54640f22",
            "value": 231508
          }
        },
        "34d50ed2938346889be334cbbecd1a29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e556e9a292d145cc8aaf39fb54e4f9a4",
            "placeholder": "​",
            "style": "IPY_MODEL_e52ac86f7a6f41688c90a17dfbb84b43",
            "value": " 232k/232k [00:05&lt;00:00, 42.5kB/s]"
          }
        },
        "f4bdadff7b534a48972f17c70c798adf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4134cda753e240a29c25b87b3e57e27d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c8ae31f210d434080b4c8af54640f22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "e556e9a292d145cc8aaf39fb54e4f9a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e52ac86f7a6f41688c90a17dfbb84b43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3220976c6ad47dc9b5edd6a01828d22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f1c10c1f2d8480592b3b2f221996156",
              "IPY_MODEL_e4ed6965b2fb480e8734a7ec65acc0f5"
            ],
            "layout": "IPY_MODEL_6ad496b7b6464cc1ba0cb7cb8c35b560"
          }
        },
        "6f1c10c1f2d8480592b3b2f221996156": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87b5f5bf532b42cfbd3523125dd8c4ec",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7140c7f880346be9873a99dc16e0189",
            "value": 28
          }
        },
        "e4ed6965b2fb480e8734a7ec65acc0f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7389db9f7abb4f2891be354a4cbea558",
            "placeholder": "​",
            "style": "IPY_MODEL_fd2df6ff62034cbdb126be575bb98fdb",
            "value": " 28.0/28.0 [00:01&lt;00:00, 14.8B/s]"
          }
        },
        "6ad496b7b6464cc1ba0cb7cb8c35b560": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87b5f5bf532b42cfbd3523125dd8c4ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7140c7f880346be9873a99dc16e0189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "7389db9f7abb4f2891be354a4cbea558": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd2df6ff62034cbdb126be575bb98fdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e82a5590c5ac49c3a425cfc671106478": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7be986d13ddd48a7b965a707cf78a33c",
              "IPY_MODEL_cc6e8847ca43485eb44a627e2985ca91"
            ],
            "layout": "IPY_MODEL_3a59823dfc654cf29fecebaaa509449b"
          }
        },
        "7be986d13ddd48a7b965a707cf78a33c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e05477a38dd4b1c84cb8fe61578da29",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a9a81a2717ed437db505388aea442fe3",
            "value": 466062
          }
        },
        "cc6e8847ca43485eb44a627e2985ca91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30d8cf457c674bbdbf4932954e78dbfc",
            "placeholder": "​",
            "style": "IPY_MODEL_55423178eeb9446da3aad9524034c4a6",
            "value": " 466k/466k [00:00&lt;00:00, 1.16MB/s]"
          }
        },
        "3a59823dfc654cf29fecebaaa509449b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e05477a38dd4b1c84cb8fe61578da29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9a81a2717ed437db505388aea442fe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "30d8cf457c674bbdbf4932954e78dbfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55423178eeb9446da3aad9524034c4a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a59e587ca1174d47b9011be0fb899aa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_595b6fa954d348dbb18ce58b3af1be8c",
              "IPY_MODEL_84da8a59f47740bb85787d834befc41c"
            ],
            "layout": "IPY_MODEL_e6bec9e3cb2f4c8b807cb492515103fa"
          }
        },
        "595b6fa954d348dbb18ce58b3af1be8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8501d12d75643b099f51c52a5819f45",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aee717de80684b5fbee68f9d1160e112",
            "value": 570
          }
        },
        "84da8a59f47740bb85787d834befc41c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6380f2ed225341aa9ec74dfa4f65fa88",
            "placeholder": "​",
            "style": "IPY_MODEL_3832437630a74e8ba3d8ac85f6c169da",
            "value": " 570/570 [00:01&lt;00:00, 491B/s]"
          }
        },
        "e6bec9e3cb2f4c8b807cb492515103fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8501d12d75643b099f51c52a5819f45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aee717de80684b5fbee68f9d1160e112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "6380f2ed225341aa9ec74dfa4f65fa88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3832437630a74e8ba3d8ac85f6c169da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "042668cd13cd4dfc8d2fa8f483d941db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_70e86a1373604403a8df70ec0994dd4f",
              "IPY_MODEL_3d805436b0da40ef9e1d04041cfdaadc"
            ],
            "layout": "IPY_MODEL_a03901d001fb4df88ec42e185be93022"
          }
        },
        "70e86a1373604403a8df70ec0994dd4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73eadd9aefed40dcacbfef055c1802d0",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e79d9d39244442f9d3c7403f4884a67",
            "value": 440473133
          }
        },
        "3d805436b0da40ef9e1d04041cfdaadc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2b120ed8cbc40b18c9279f155fb15f1",
            "placeholder": "​",
            "style": "IPY_MODEL_c19ce4b7750843db9679773bbaf4d195",
            "value": " 440M/440M [00:21&lt;00:00, 20.5MB/s]"
          }
        },
        "a03901d001fb4df88ec42e185be93022": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73eadd9aefed40dcacbfef055c1802d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e79d9d39244442f9d3c7403f4884a67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "c2b120ed8cbc40b18c9279f155fb15f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c19ce4b7750843db9679773bbaf4d195": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57ca74699e994dea970fbbd5575a9b34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_208114147c2744b2a7b0949eebcf6727",
              "IPY_MODEL_c919d3a7b365437b9ea124587252367a"
            ],
            "layout": "IPY_MODEL_d1179e63a23846208bf9007a2b88fd68"
          }
        },
        "208114147c2744b2a7b0949eebcf6727": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fef476fb41d44d6ea7f7a72ca527865b",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_683a948edd1c421098c464d7291c6243",
            "value": 10
          }
        },
        "c919d3a7b365437b9ea124587252367a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d6732461d5e49cd9eb9b97a4baf0638",
            "placeholder": "​",
            "style": "IPY_MODEL_5d69ecbad56f4d4e891a831789e398ab",
            "value": " 10/10 [33:40&lt;00:00, 202.04s/it]"
          }
        },
        "d1179e63a23846208bf9007a2b88fd68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fef476fb41d44d6ea7f7a72ca527865b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "683a948edd1c421098c464d7291c6243": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "9d6732461d5e49cd9eb9b97a4baf0638": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d69ecbad56f4d4e891a831789e398ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7d588adaab34d8b8a77c29685d3358e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_debaab3fe05c42ed8ad7a62b8deddfd3",
              "IPY_MODEL_bc18abca13fa4e2ab150509ab7ce4025"
            ],
            "layout": "IPY_MODEL_e7898a2cc9c742bd89973f27f1c27adf"
          }
        },
        "debaab3fe05c42ed8ad7a62b8deddfd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db4b4418f1a64684a19e1a2d9e11ca4c",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c8ef3771fa1d439a9ffe316ea8aa7484",
            "value": 231508
          }
        },
        "bc18abca13fa4e2ab150509ab7ce4025": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_889f66a524644ed381b07ed6449da1fb",
            "placeholder": "​",
            "style": "IPY_MODEL_f771589126a74cb99215bf730cc1a3f7",
            "value": " 232k/232k [00:03&lt;00:00, 70.7kB/s]"
          }
        },
        "e7898a2cc9c742bd89973f27f1c27adf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db4b4418f1a64684a19e1a2d9e11ca4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8ef3771fa1d439a9ffe316ea8aa7484": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "889f66a524644ed381b07ed6449da1fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f771589126a74cb99215bf730cc1a3f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f452547d7ac348ccb37360b2b8b4e032": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ca0ac02bb9f4d4aa3d55eedf9e350ef",
              "IPY_MODEL_b772469f80ea4e5cbab2c9b682be24f4"
            ],
            "layout": "IPY_MODEL_2515f2a05922438d86113dc9a07c2b53"
          }
        },
        "6ca0ac02bb9f4d4aa3d55eedf9e350ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46e7e7935afe46c980713ab216f79dc0",
            "max": 383,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_18a310b239094341ac59180a9b0ff1f3",
            "value": 383
          }
        },
        "b772469f80ea4e5cbab2c9b682be24f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b52968ca34724a56b6f6e85112740f3e",
            "placeholder": "​",
            "style": "IPY_MODEL_195df3ba5df3451da3f607cb12a9a485",
            "value": " 383/383 [00:05&lt;00:00, 75.4B/s]"
          }
        },
        "2515f2a05922438d86113dc9a07c2b53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46e7e7935afe46c980713ab216f79dc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18a310b239094341ac59180a9b0ff1f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "b52968ca34724a56b6f6e85112740f3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "195df3ba5df3451da3f607cb12a9a485": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b56d3b3c0bc34841b9bb0cd93afd0d9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ef8dfcb8adcb44f5ad4a6329e386dc76",
              "IPY_MODEL_88f5849a66874f06ae4f148636b2f3c0"
            ],
            "layout": "IPY_MODEL_cf16a1a6ec294d8da9e6ee807165c2cc"
          }
        },
        "ef8dfcb8adcb44f5ad4a6329e386dc76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49b050e3caa346499be7adf7424227be",
            "max": 25710911,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09e9668247fd4cd3a47bea79738c3b1f",
            "value": 25710911
          }
        },
        "88f5849a66874f06ae4f148636b2f3c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c0c1c7093244eeea46b960756411a48",
            "placeholder": "​",
            "style": "IPY_MODEL_6a7f0432d63349df90b381c0f47aa184",
            "value": " 25.7M/25.7M [00:03&lt;00:00, 8.32MB/s]"
          }
        },
        "cf16a1a6ec294d8da9e6ee807165c2cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49b050e3caa346499be7adf7424227be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09e9668247fd4cd3a47bea79738c3b1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "4c0c1c7093244eeea46b960756411a48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a7f0432d63349df90b381c0f47aa184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIsh_AIYoHT6"
      },
      "source": [
        "# Берём тексты"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHqc8VqOdXuX"
      },
      "source": [
        "import json\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter, defaultdict, OrderedDict\n",
        "import glob\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdlYiYmMLDNN"
      },
      "source": [
        "with open('drive/MyDrive/Coursework2021/english_corpus.txt', 'r') as f:\n",
        "    documents = f.readlines()\n",
        "documents = list(map(lambda doc: doc.rstrip('\\n'), documents))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaXy9vCBSKMj",
        "outputId": "97df6d9c-5b97-400e-c030-bdcf334a33b0"
      },
      "source": [
        "print('суммарная длинна текстов', sum(map(lambda doc: len(re.split('\\s+', doc)), documents)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "суммарная длинна текстов 234401541\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gLQaXZPcfGZG",
        "outputId": "5eab73cc-97d1-428d-8ecb-79f25b1dfd78"
      },
      "source": [
        "documents[22][-100:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'n with little space for domestic pleasure and home comfort know -PRON- child more PERSON1 sister be '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "p-4V6K0n83vr",
        "outputId": "e62d4a13-e7a6-4d8f-d54e-37a56c82e1cf"
      },
      "source": [
        "doc_sizes = list(map(lambda doc: len(re.split('\\s+', doc)), documents))\n",
        "doc_sizes = [sz for sz in doc_sizes if sz < 10000000]\n",
        "plt.hist(doc_sizes, bins=100)\n",
        "plt.title('Распределение текстов данной длины')\n",
        "print(f'Всего текстов {len(documents)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Всего текстов 11008\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmTj25vDN0MA",
        "outputId": "01c9c185-b6b8-4442-88aa-db112f8eed63"
      },
      "source": [
        "print('*0-е квантили размеров:', np.quantile(np.array(doc_sizes, dtype=int), q=np.linspace(0, 1, 11)).astype(int))\n",
        "print('Средний размер', np.mean(doc_sizes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*0-е квантили размеров: [     1     99    136    210    369    990   3157   8524  39583  77580\n",
            " 392138]\n",
            "Средний размер 21293.74464026163\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEK-emy9NZ22"
      },
      "source": [
        "### Аналитика распределения размеров сырых текстов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7O1Y1COP-7x",
        "outputId": "fc7a2812-5413-4881-8d7b-ecc6369ad232"
      },
      "source": [
        "prepared_file_list = glob.glob('/content/drive/My Drive/Coursework2021/EnLit/EnLit/*/*')\n",
        "print('total files', len(prepared_file_list))\n",
        "print('not poetry', sum('poetry' in name.lower() for name in prepared_file_list))\n",
        "\n",
        "# for doc in glob.:\n",
        "#     if len(re.split('\\s', doc)) < 100:\n",
        "#         print(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total files 11049\n",
            "not poetry 6069\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294,
          "referenced_widgets": [
            "83577cb3d79c4b338a9b6f4eb714b997",
            "d9914ea982f74584a4ca498bfe8f361e",
            "65a6d81c8e534680a17bff3a0439da67",
            "937fe692969d43e793a49bb84c27dc0c",
            "34912d639aa843cc8e0cb5c602ffc601",
            "36844d4e382947f8877173ae6be78366",
            "14e0d1864ae74d67b207add340616754",
            "6502e1dae6ac4c6faf6fb2195217f9d3"
          ]
        },
        "id": "8PF9Dl1jgc6G",
        "outputId": "c6bd4a5d-37d7-4af2-dbb5-574f312dbc48"
      },
      "source": [
        "# checking for russian\n",
        "prepared_file_list = glob.glob('/content/drive/My Drive/Coursework2021/EnLit/EnLit/*/*')\n",
        "print('total files', len(prepared_file_list))\n",
        "for fname in tqdm(prepared_file_list):\n",
        "    with open(fname, 'r') as f:\n",
        "        if re.search('[а-яА-Я]', f.read()):\n",
        "            print(fname)\n",
        "print(prepared_file_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total files 11060\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83577cb3d79c4b338a9b6f4eb714b997",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=11060.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "IsADirectoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-41b3fcd49bd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'total files'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepared_file_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepared_file_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[а-яА-Я]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/content/drive/My Drive/Coursework2021/EnLit/EnLit/Corpus/19'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "e167b07d961e46a0b0a61fefce000704",
            "f3e871f28b6e4d5b849944714fbe6aa5",
            "f9849409d52c48dcaa4c5aed1d3aed4b",
            "242d45d64b3f42009716c78623e4e917",
            "aaacf402388846f7a46010ca9e4b2368",
            "a1cbfeb784374238be51ee63498864bf",
            "62dd8575711a4d3cb5824b04661382bf",
            "48e85d0abb524dacbbd14e855f62e59d"
          ]
        },
        "id": "zE8pf7Yhq0gk",
        "outputId": "236cf246-e7b8-4c49-bac7-b9e6c6597129"
      },
      "source": [
        "# file sizes\n",
        "\n",
        "\"\"\"\n",
        "prepared_file_list = glob.glob('/content/drive/My Drive/Coursework2021/EnLit/EnLit/*/*.txt')\n",
        "print('total files', len(prepared_file_list))\n",
        "\"\"\"\n",
        "\n",
        "file_sizes = {}\n",
        "for fname in tqdm(prepared_file_list[8220:]):\n",
        "    with open(fname, 'r') as f:\n",
        "        file_sizes[fname] = len(re.split('\\s+', f.read()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e167b07d961e46a0b0a61fefce000704",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2739.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "sxSTW-Br2z93",
        "outputId": "4f657d59-0f80-4266-dac8-8792616d95aa"
      },
      "source": [
        "fsz = list(file_sizes.values())\n",
        "plt.hist(fsz, bins=100)\n",
        "plt.title('Распределение размеров всех текстов')\n",
        "\n",
        "print('Средний размер:', np.mean(fsz))\n",
        "print('Квантили размеров:', np.quantile(fsz, np.linspace(0, 1, 6)).astype(int))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Средний размер: 21914.72106608251\n",
            "Квантили размеров: [     1    127    245   1430  33242 362260]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "eMV7qc8k25sh",
        "outputId": "a0e90e6d-2fb7-4c46-c29e-0460765e6767"
      },
      "source": [
        "clean_fsz = list(sz for fname, sz in file_sizes.items() if 'poetry' not in fname.lower())\n",
        "plt.hist(clean_fsz, bins=100)\n",
        "plt.title('Распределение размеров текстов не poetry')\n",
        "\n",
        "print('Средний размер:', np.mean(clean_fsz))\n",
        "print('Квантили размеров:', np.quantile(clean_fsz, np.linspace(0, 1, 6)).astype(int))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Средний размер: 54668.37988826816\n",
            "Квантили размеров: [     1   4229  18700  56333  94664 362260]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3iCQrLV-hRU"
      },
      "source": [
        "with open('/content/drive/My Drive/Coursework2021/english_file_sizes.txt', 'w') as f:\n",
        "    json.dump(file_sizes, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDZoxHC4If2J"
      },
      "source": [
        "with open('/content/drive/My Drive/Coursework2021/english_file_sizes.txt', 'r') as f:\n",
        "    file_sizes = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "BhM02qCd-kPP",
        "outputId": "c193203c-489b-4093-9d22-b510d7cd7dbc"
      },
      "source": [
        "# выбираю подвыборку английских текстов для препрогона\n",
        "\n",
        "UPPER_BOUND = 20000\n",
        "fnames_no_poetry = [fname for fname, sz in file_sizes.items() if 'poetry' not in fname.lower() and 50 < sz < UPPER_BOUND]\n",
        "\n",
        "fname_sample_idx = np.random.choice(len(fnames_no_poetry), 300, replace=False)\n",
        "\n",
        "plt.hist([file_sizes[fnames_no_poetry[i]] for i in fname_sample_idx], bins=50)\n",
        "plt.title(f'Распределение длин выбранных небольших текстов (<{UPPER_BOUND} слов)')\n",
        "print(f'Всего слов {sum(file_sizes[fnames_no_poetry[i]] for i in fname_sample_idx)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Всего слов 1811610\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb0AAAEICAYAAADLKSqCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAd30lEQVR4nO3deZhlVXmo8fdjlEkGabmAQKNxQiNDOoiKQ9QwqmBiiISroHJbDDhErgYkN5IbUVABJRoRHhAUROAqguKEOMUY0G5EZJTBZmygsRkdiA3f/WOtQ+8+fU5VdXV1nXNqv7/nqafO2WsP315nn/XtvfbaVZGZSJLUBqsNOgBJkqaLSU+S1BomPUlSa5j0JEmtYdKTJLWGSU9DKSJWiwiPz3FEscag45BGhY2KhkZE/HVE/Cgi7gAeBHYZdEzDKCL+IiK+FRG3AQ8Bbxh0TFoxETErIq6PiHUGHctMEBHHR8Q7JjLvuEkvIhZExO8j4pGIuCcizoiI9Vc+TGmpiNgfOAE4EtgqMzfIzJ8MOKyhExG7AucCnwK2rfX0pQGHpRV3BHBGZv5+RRaKiI9HxI0R8XBNmm/uKt8hIuZHxO/q7x0aZRERx0XEb+rPcRERU7HsEPg48IGIWGu8GSd6pffazFwf2AmYA/zTSgQn9fJhYL/M/M/0LyaM5UPAOzPz65n52KCD0YqLiLWBA4GzJjj/Zo23vwVeC2xY1/HJiHhxnW8t4MK63o2BM4ELG4lgLrAvsD3wgrqet6/sssMgMxcC1wOvm8jMY/4AC4BXN95/DPh6ff0W4DrgYeAW4O1dy+4DXEnpgrkZ2KNO/wHwB+CR+vN7YEHXNo8ErgXuBz4HPKlR/pq63geAnwAv6NruWcB/N9Z9R6NsbcpZwW3APcDJwDqN8tlANmJ7DDi4lq1GOUO7GfgNcB6wSddya3TFcXR9/YquOPar8x/cmPbWWp/3A98Gthnns7mj7t8jdX/P6ipv1vMfgB/3ihXYub7/UK9Y67QfAwf1ieNo4I91Ow8AFwAb1LKDOtvtWuZPgE5+eyrly/x54D7gVsqJ1WqNdfwn5ermQcrB/arGuvoeh+PtS6/4ar2+orFvZ/WI/9XUYxZ4BrAY2Km+3wJY1FnHBL5TBwM/aLx/DnBJXecNlJOBTtnvgNOAu4G7gE8AazfK/xdwU132ImCLHtte7pjpVU89vhNrdO97I/7vNt4n8Cf19dZ1e53t/C3wa+DJ9f2edV9m9djuL1j6HX6cpd/JD0ygns5g6fH8FEpb8o6x2ibg/Y1tPN6op2vqMhtSjtFF9D5GH6vzPwR8D9iyT32+DLhpnO/2usCb6nquHWO+i4DD6+vdgDuBaJTfxtJ29yfA3EbZ24DLVnbZPnH1bPsbn02/9vnFwM8o3/OfAS/u057dCxzTtc2jgM+NVa+ZuWL39CJiK2Av4Od10r2UBPRkSsNzYkTsVOfdmXKAvA/YqH7QCxqrOywz189yBfnaHps7ANid0qA8i3p1GRE7AqdTzjKeAnwWuKiePT0Raq2Q9SlfqqZj6/p2oDS8WwL/3Cjv1MmGdfn/aJS9k3K283JKw3Y/8OkesY8pItYE/hVY2Ji2D/AB4K+AWXW754y3KsrBtD7lSqnbasChtfyQMdbzMcoBvzLOrdvZGtiWchY6UevWnw3rsi8H3kw5pjpeSPnybAp8EPhKRGxSy/oeh9MhM28G/hE4KyLWpZyknZmZP1jRdUXEepSG/IuUk4E3Av8eEdvVWdYBnkc5296ecsLS+W68EvgI5YRqc0rD3N31uRrwmjGOman2r5QTRAAy81xKA3pSRDyFksAPzsxF3Qtm5vaN7/BdnfYiMz88gXoCoN6K+Sbwxcz8TJ3Ws23KzI822qTbqD1cmfm8urp/oxyjT6f3MfpfddmnAo8C/9CnTv6UkqSXExEviohTKd/HN9f6+bM+864D/DlwTZ30POCqrBmguqpO75T/olH2i66yyS7bHdd4bf9qwHHd7XP9Pl8MnERp208ALq7HScdhdbldgcMj4vmNsuso34kxTTTpfTUiHqCcIf+Q+mXJzIsz8+Ysfgh8B3hpXeZtwOmZeUlmPp6Zd2bm9RPcHsCnMvP2zFwMHAPsX6fPBT6bmZdn5mOZeSblAGsOeliHciaxjNoHPRf4h8xcnJkP1315Y2O2tYDHs3fX0SHAUZl5R2Y+SrkKeMMkRs+9Hbgc+FXXuj+Smddl5pIa1w4Rsc0Y6+m5nw1rjVNORLyGkjy/O5HAJ2B1ynH1m/Fm7OHIzHw4MxcAx1POdDvuBT6RmX+sDecNwN4w7nE4LTLzVMoV1uWUhHPUJFf1GkoD/LnMXJKZPwe+DPxNY57/m5n31kTxLyytpwMo37kr6vF5JPCiiJjdWHbcY2KqRMQLgBdRusqaDgVeSTlz/1pmfn0Sq59IPa0NfBW4LjM/1Ji+wm1TRKxOaSfGOkY7VmPs78BGlF6J5vr3i4jrKVdBvwb+NDP/MjPPzv73/U6mJJ9v1/frU66Qmh4ENuhT/iCwfm0XV2bZbuPVb79jcG/gxsz8Qv1Mz6H06vS6KFqDcmXdjOlhSt2OaaJJb9/M3Cgzt8nMv+98CBGxZ0RcFhGLa1Lci3ImDrAV5cx8sm5vvL6VcmUFsA0lwz/Q+anb2qIx//+gdEF0m0W5opjfWPZbdXrHJpQruF62AS5oLHsdpeKbfe73Ncr3615BRGxA6Ub5Pz3W/cnGsospyWjLXoHUK9uN+uznRPYFSoL6SI2n2xZddTzeSMr96nyLKF2VX2uU7VLXszgifhIRc7qWfbT+vrUx7VaW3fc7u85CnzgmxjkOJ7Ivu3SVb9FVvl8tuy8iLomIp/epg1OB5wP/VpPOWL7a2N5JjenbAC/siucAyjENpbHorqdOvFs0yzLzEUrDuyU8cdK3Ef2PiU493R8RP4+I3cfZh/EcRznO/9icmJkPAOdT6ur4Sa57vHqCklzXA14cy46SnEzbtCmwJmMfo7vUOB6g9Fic0Wdd97M0mXQ8ra7rSkoiu3usYCLiY5T626/xvXiE0tvR9GSWJtju8icDj9TlV2bZbuPVb792aZnjt+qu45NqHV9DSazNPLEBpe7HNOlHFmqj+2XK/bHNMnMj4BuUhhpK0nrGZNdPqbiOrSn3LzrrPaYm4c7PuvWsoNN1+HyWvRTvuI/Sh/y8xrKdbsyOZ7HsFVjT7cCeXdt+UmY2uwY37ZRR7vl1ex9wXmZ2f7i3U+5FNde9TvYfwbgD5YD8da/CegN6mzH2BUoX5A2ZeVmPsruasQC95mk6r863LvBLlm3MLqtlsyhdUp/qWvYeSmPevKrdmmW7XLfsOqvcGrhrAsfhRPblsq7yu7rKO/u2BaXba7luwdqN9glKd9TRja7XfvZtbO9djem3Az/sOg7Wz8zOcOzbWL6eOvHe1SyrXYBPYWk9bkM5Q76lT0x31Xg2oXTldV+hrYhX1m0v9x2oowLfSum+P6m7fILGqyco3agvpdwbOqZr2RVtm+6jJO+xjtHOcf4kyv38M/qs6ypKO/OEzDyB0rhfSukluCMiTqy3c5YREf9C6RbcLTMfahRdA7yg63vyApZ2f17Dst1/23eVTXbZbuPVb782dpnjt+qu43c1jtFdo4z67nguvdv9ZazMc3prUboPFgFLImJPys3QjtOAt0TEq6I8aLxlRDxnBdZ/aEQ8rTYeR1GGaUM5mz4kIl4YxXoRsXe9goLSx343MK97hZn5eF3+xIh4KkCNa/f6eivg3ZQukV5OBo7pdDlGedZmnxXYpw1qfMf0KDsZODIinlfXvWFE/E2P+Yjy0PY7gfN7dcNGxJMo9ylvysyxkt5RlC6wqfQ4ZSDDrO6CGuuDdB139XM5l1K3G9T6fS/Ljm57KvCuiFiz1stzKcltvONwymRm5+Z7r+/NJ4F5mXkw5b7EyZPczNeBZ0XEm+q+rhkRfx4Rz63l5wD/VI+9TSmf81mNsrdEGXq+NiU5X56ZC+r344PAdzLzd+PsZ1LOmFemfTgaeH/3lUA9Ns+i3L9+C+Vk5u8nsf7x6glKElpCOanYPyJeVKevcNtUj93zGPsYfWJ2Sg/Qct+B6qfARhGxTC9OZj6Umadk5osp9wz/AHwtIi7tzBMRRwJ/RxkI1d19+oO63XdFxNoRcVid/r36+/PAe+v+bgEcztLEvDLLdutZvxGxRkQcQukq/Y8ey32D8pn+XZ33b4HtKJ91t8dYvp15OeX+7dhynJEudI006yo7lHKW/gDwBcpN8w81yl9POat5mHK/Y/dcOgqnOWqxezTYApaO3nyAcsa5bqN8D8rZ2wOUwSDnUxLKAbUiOiMJmyO/Tq7LPonSGNxCGVl0HeXsgbq9E4E1G9t6IlZKI/Beyv2khymX8B/OHiPc6rTu0ZsJvK/Xuuv7N1Gukh6inC2d3qfeT6nrao6A/e+63wdQhrV/D3huY5mDWH705qcb5WcwNaM3H6Qc0M9pbPdRyojIO4D5lPs8T4zerPNtDJxNOaO+jdKY9xu9+SvKWe64x+F4+8LERm/+tk67s+7bs1h29OY+tawzknd9yvF+wES+Uyw/evPZlMS5iNI9+T1gh8bx+ynKid1CypVSc2TzIZTjcjGlsXha4/O9kMZoThojU2s9LWl8TtdSBnLA0uOlU7aoa97FLD968+I+2zkR+GajbPu6/DPHaIOW+wwnUE9nsGxb9AbKd33tsdqmsdo9yjF6Vt3e7Sx/jDZHb86nMfKwR+wfA/5xAu3vasCLuur2UZZ+758YzVrLd6zb/j1wBbBjoyyAj9b6Xlxfx1Qs2yPu5eqXpWMZXtLvs6UMUJlP+Z7PB3btai87bd5iyiCm9WrZ5pRjca3x6jTqAkMlIhZQksEKDa6IiIOA2Zl5dNf0p1G+AAdNUYgDFRFnUB5s/UHX9P9JSbpnDCCsVaZ+rgdn5q6DjkWaChHRGaG9Y67gA+paXkQcD9ycmf8+3rwz7W/2/ZZyltVtCeXMYKZYzNLBH02/ZeZ9ptKMk2Xk7Yrc7tEYMvPwic47oxrIzDy/z/S7Kd2SM0Jm9tyXzLxgumORpFEylN2bkiStCv6XBUlSa8yo7s1um266ac6ePXvQYUjSSJk/f/59mdnvkYuRNqOT3uzZs5k3b7nH9SRJY4iI7j+eMWPYvSlJag2TniSpNUx6kqTWMOlJklrDpCdJag2TniSpNUx6kqTWMOlJklrDpCdJao0Z/RdZptvsIy7uOX3BsXtPcySSpF680pMktYZJT5LUGiY9SVJrmPQkSa1h0pMktYZJT5LUGiY9SVJrmPQkSa1h0pMktYZJT5LUGiY9SVJrDF3Si4itIuL7EXFtRFwTEe+u04+OiDsj4sr6s9egY5UkjZZh/IPTS4DDM/OKiNgAmB8Rl9SyEzPz4wOMTZI0woYu6WXmQmBhff1wRFwHbDnYqCRJM8HQdW82RcRsYEfg8jrpsIi4KiJOj4iN+ywzNyLmRcS8RYsWTVOkkqRRMLRJLyLWB74MvCczHwI+AzwD2IFyJXh8r+Uy85TMnJOZc2bNmjVt8UqSht9QJr2IWJOS8M7OzK8AZOY9mflYZj4OnArsPMgYJUmjZ+iSXkQEcBpwXWae0Ji+eWO21wNXT3dskqTRNnQDWYCXAG8CfhkRV9ZpHwD2j4gdgAQWAG8fTHiSpFE1dEkvM38MRI+ib0x3LJKkmWXoujclSVpVTHqSpNYYuu7NmWj2ERf3nL7g2L2nORJJajev9CRJrWHSkyS1hklPktQaJj1JUmuY9CRJrWHSkyS1hklPktQaJj1JUmuY9CRJrWHSkyS1hklPktQaJj1JUmuY9CRJrWHSkyS1hklPktQaJj1JUmuY9CRJrWHSkyS1hklPktQaJj1JUmuY9CRJrWHSkyS1hklPktQaJj1JUmuY9CRJrWHSkyS1xtAlvYjYKiK+HxHXRsQ1EfHuOn2TiLgkIm6svzcedKySpNEydEkPWAIcnpnbAbsAh0bEdsARwKWZ+Uzg0vpekqQJG7qkl5kLM/OK+vph4DpgS2Af4Mw625nAvoOJUJI0qoYu6TVFxGxgR+ByYLPMXFiL7gY267PM3IiYFxHzFi1aNC1xSpJGw9AmvYhYH/gy8J7MfKhZlpkJZK/lMvOUzJyTmXNmzZo1DZFKkkbFUCa9iFiTkvDOzsyv1Mn3RMTmtXxz4N5BxSdJGk1Dl/QiIoDTgOsy84RG0UXAgfX1gcCF0x2bJGm0rTHoAHp4CfAm4JcRcWWd9gHgWOC8iHgbcCuw34DikySNqKFLepn5YyD6FL9qOmORJM0sQ9e9KUnSqmLSkyS1xtB1b46C2UdcPOgQJEmT4JWeJKk1THqSpNYw6UmSWsOkJ0lqDZOeJKk1THqSpNYw6UmSWsOkJ0lqDZOeJKk1THqSpNYw6UmSWsOkJ0lqDZOeJKk1THqSpNYw6UmSWsOkJ0lqDZOeJKk1THqSpNYw6UmSWsOkJ0lqDZOeJKk1THqSpNYw6UmSWsOkJ0lqDZOeJKk1THqSpNYw6UmSWmMok15EnB4R90bE1Y1pR0fEnRFxZf3Za5AxSpJGz1AmPeAMYI8e00/MzB3qzzemOSZJ0ogbyqSXmT8CFg86DknSzDKUSW8Mh0XEVbX7c+NeM0TE3IiYFxHzFi1aNN3xSZKG2Cglvc8AzwB2ABYCx/eaKTNPycw5mTln1qxZ0xmfJGnIjUzSy8x7MvOxzHwcOBXYedAxSZJGy8gkvYjYvPH29cDV/eaVJKmXNQYdQC8RcQ7wCmDTiLgD+CDwiojYAUhgAfD2gQUoSRpJQ5n0MnP/HpNPm/ZAJEkzysh0b0qStLKG8kqv7WYfcXHP6QuO3XtK5pektvJKT5LUGiY9SVJrmPQkSa1h0pMktYZJT5LUGiY9SVJrmPQkSa3hc3oD1O/5OknSquGVniSpNUx6kqTWMOlJklrDpCdJag2TniSpNUx6kqTWMOlJklrDpCdJag2TniSpNUx6kqTWMOlJklrDpCdJag2TniSpNUx6kqTW8F8LjZA2/iuifvu84Ni9pzkSSTOBV3qSpNYw6UmSWsOkJ0lqDZOeJKk1hjLpRcTpEXFvRFzdmLZJRFwSETfW3xsPMkZJ0ugZyqQHnAHs0TXtCODSzHwmcGl9L0nShA1l0svMHwGLuybvA5xZX58J7DutQUmSRt4oPae3WWYurK/vBjbrNVNEzAXmAmy99dbTFNpw8hk3SVrWUF7pjSczE8g+Zadk5pzMnDNr1qxpjkySNMxGKendExGbA9Tf9w44HknSiBmlpHcRcGB9fSBw4QBjkSSNoKFMehFxDvBfwLMj4o6IeBtwLPCXEXEj8Or6XpKkCRvKgSyZuX+foldNayCSpBllKK/0JElaFUx6kqTWGMruTa1aPr8nqa280pMktYZJT5LUGiY9SVJrmPQkSa1h0pMktYZJT5LUGj6yoCcM8lGGftuWpKnklZ4kqTVMepKk1jDpSZJaw6QnSWoNk54kqTVMepKk1jDpSZJaw+f0+vC5sYkZtn9TNFXxDNt+SZoaXulJklrDpCdJag2TniSpNUx6kqTWMOlJklrDpCdJag2TniSpNXxOT+OaCc8sDvL5vRVdxmcEJ8+603i80pMktYZJT5LUGiY9SVJrjNw9vYhYADwMPAYsycw5g41IkjQqRi7pVX+RmfcNOghJ0mixe1OS1BqjeKWXwHciIoHPZuYpzcKImAvMBdh6660HEJ7GMlWPP8yExyimisP0B2+qHksZaxlNjVG80ts1M3cC9gQOjYiXNQsz85TMnJOZc2bNmjWYCCVJQ2nkkl5m3ll/3wtcAOw82IgkSaNipJJeRKwXERt0XgO7AVcPNipJ0qgYtXt6mwEXRASU2L+Ymd8abEiSpFExUkkvM28Bth90HJKk0TRS3ZuSJK0Mk54kqTVGqntTo8Pn6CQNI6/0JEmtYdKTJLWGSU+S1BomPUlSa5j0JEmtYdKTJLWGSU+S1Bo+pydNgck8lzhTn2Wcyv8VN1X/K3BF69r/aTdzeaUnSWoNk54kqTVMepKk1jDpSZJaw6QnSWoNk54kqTV8ZEGaoabqkYh+w/dn6iMXMHWPSkyHUYp1GHilJ0lqDZOeJKk1THqSpNYw6UmSWsOkJ0lqDZOeJKk1THqSpNaIzBx0DKvMnDlzct68eZNadiY/gySNujY+O7iiVuY5vYiYn5lzpjCcoeGVniSpNUx6kqTWMOlJklrDpCdJao2RS3oRsUdE3BARN0XEEYOOR5I0OkYq6UXE6sCngT2B7YD9I2K7wUYlSRoVI5X0gJ2BmzLzlsz8b+BLwD4DjkmSNCJG7f/pbQnc3nh/B/DC5gwRMReYW98+EhE3rOA2NgXum3SEq9awxjasccHwxjasccHwxvZEXHHcgCNZ3tDVWa2jyca1zZQGM0RGLemNKzNPAU6Z7PIRMW9YH8oc1tiGNS4Y3tiGNS4Y3tiGNS4Y3tiGNa5BGrXuzTuBrRrvn1anSZI0rlFLej8DnhkR20bEWsAbgYsGHJMkaUSMVPdmZi6JiMOAbwOrA6dn5jVTvJlJd41Og2GNbVjjguGNbVjjguGNbVjjguGNbVjjGpgZ/QenJUlqGrXuTUmSJs2kJ0lqDZNew3T/ibOI2Coivh8R10bENRHx7jr96Ii4MyKurD97NZY5ssZ3Q0Tsvipjj4gFEfHLGsO8Om2TiLgkIm6svzeu0yMiTqrbvyoidmqs58A6/40RceBKxvTsRr1cGREPRcR7BlVnEXF6RNwbEVc3pk1ZHUXEn9XP4Ka6bKxEXB+LiOvrti+IiI3q9NkR8ftG3Z083vb77eNKxDZln1+UgW6X1+nnRhn0Ntm4zm3EtCAirpzuOov+7cTAj7ORlJn+lPuaqwM3A08H1gJ+AWy3ire5ObBTfb0B8CvKn1c7GvjfPebfrsa1NrBtjXf1VRU7sADYtGvaR4Ej6usjgOPq672AbwIB7AJcXqdvAtxSf29cX288hZ/Z3ZQHaQdSZ8DLgJ2Aq1dFHQE/rfNGXXbPlYhrN2CN+vq4Rlyzm/N1rafn9vvt40rENmWfH3Ae8Mb6+mTgHZONq6v8eOCfp7vO6N9ODPw4G8Ufr/SWmvY/cZaZCzPzivr6YeA6yl+d6Wcf4EuZ+Whm/hq4qcY9nbHvA5xZX58J7NuY/vksLgM2iojNgd2BSzJzcWbeD1wC7DFFsbwKuDkzbx0n3lVWZ5n5I2Bxj22udB3Vsidn5mVZWqbPN9a1wnFl5ncyc0l9exnlOde+xtl+v32cVGxjWKHPr16hvBL4fysa21hx1fXuB5wz1jpWRZ2N0U4M/DgbRSa9pXr9ibOxEtCUiojZwI7A5XXSYbVr4vRGN0i/GFdV7Al8JyLmR/nzbgCbZebC+vpuYLMBxQblOc1mIzQMdQZTV0db1terIsa3Us7oO7aNiJ9HxA8j4qWNePttv98+royp+PyeAjzQSO5TVWcvBe7JzBsb06a9zrraiVE4zoaOSW8IRMT6wJeB92TmQ8BngGcAOwALKd0qg7BrZu5E+a8Wh0bEy5qF9axwIM+81Ps0rwPOr5OGpc6WMcg66icijgKWAGfXSQuBrTNzR+C9wBcj4skTXd8U7eNQfn4N+7PsCda011mPdmKl1tdWJr2lBvInziJiTcqBfHZmfgUgM+/JzMcy83HgVEpXzlgxrpLYM/PO+vte4IIaxz21O6TTlXPvIGKjJOIrMvOeGuNQ1Fk1VXV0J8t2Qa50jBFxEPAa4IDaUFK7Dn9TX8+n3Ct71jjb77ePkzKFn99vKN15a3RNn7S6rr8Czm3EO6111qudGGN9Az/OhplJb6lp/xNn9T7BacB1mXlCY/rmjdleD3RGk10EvDEi1o6IbYFnUm5AT3nsEbFeRGzQeU0ZBHF1XW9n1NeBwIWN2N5cR47tAjxYu16+DewWERvXLqvd6rSVtcyZ9zDUWcOU1FEteygidqnHypsb61phEbEH8H7gdZn5u8b0WVH+VyUR8XRKHd0yzvb77eNkY5uSz68m8u8Db5iq2IBXA9dn5hNdgNNZZ/3aiTHWN9DjbOityKiXmf5DGfX0K8pZ21HTsL1dKV0SVwFX1p+9gC8Av6zTLwI2byxzVI3vBhojrKY6dsqouF/Un2s666TcM7kUuBH4LrBJnR6Uf/B7c419TmNdb6UMQLgJeMsUxLYe5Yx+w8a0gdQZJfEuBP5IuRfytqmsI2AOJQHcDHyK+leUJhnXTZR7Op1j7eQ671/Xz/hK4ArgteNtv98+rkRsU/b51WP3p3V/zwfWnmxcdfoZwCFd805bndG/nRj4cTaKP/4ZMklSa9i9KUlqDZOeJKk1THqSpNYw6UmSWsOkJ0lqDZOeJKk1THqSpNb4/z/P/D7IUmw4AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gvUk6yz-kF7"
      },
      "source": [
        "# статистика по маленькой выборке\n",
        "fnames_sample = [fnames_no_poetry[i] for i in fname_sample_idx]\n",
        "authors = Counter([re.findall('/content/drive/My Drive/Coursework2021/EnLit/EnLit/(.*)/', fname)[0] for fname in fnames_sample])\n",
        "text_styles = Counter([re.findall('\\((.*?)\\)', fname)[0] for fname in fnames_sample])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB2Q0rSIKnes",
        "outputId": "bf9eeb96-1908-432c-82ee-8bb937f27d25"
      },
      "source": [
        "authors.most_common()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Nathaniel Hawthorne', 56),\n",
              " ('Gilbert Keith Chesterton', 42),\n",
              " ('Willa Cather', 18),\n",
              " ('John Donne', 14),\n",
              " ('Gertrude Franklin Horn Atherton', 13),\n",
              " ('Eleanor H. Porter', 12),\n",
              " ('Anthony Trollope', 12),\n",
              " ('Arthur Conan Doyle', 11),\n",
              " ('Sherwood Anderson', 10),\n",
              " ('Edward Payson Roe', 9),\n",
              " ('James M. Barrie', 9),\n",
              " ('John Fox Jr', 9),\n",
              " ('William Makepeace Thackeray', 8),\n",
              " ('Thomas Hardy', 8),\n",
              " ('Frank Norris', 8),\n",
              " ('Maria Edgeworth', 7),\n",
              " ('Robert Louis Stevenson', 6),\n",
              " ('Christopher Marlowe', 5),\n",
              " ('Kate Douglas Wiggin', 5),\n",
              " ('Wilkie Collins', 5),\n",
              " ('Mary Roberts Rinehart', 4),\n",
              " ('Virginia Woolf', 4),\n",
              " ('J. M. Synge', 4),\n",
              " ('Rabindranath Tagore', 4),\n",
              " ('Daniel Defoe', 3),\n",
              " ('John Kendrick Bangs', 3),\n",
              " ('Sir Walter Scott', 2),\n",
              " ('D.H. Lawrence', 2),\n",
              " ('Aldous Huxley', 2),\n",
              " ('William Somerset Maugham', 1),\n",
              " ('H. Rider Haggard', 1),\n",
              " ('Stephen Leacock', 1),\n",
              " ('T. S. Eliot', 1),\n",
              " ('Irving Bacheller', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nN-XtlWGLjbk",
        "outputId": "52523199-e660-40c1-d555-39065a229ce0"
      },
      "source": [
        "text_styles.most_common()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Short Stories', 185),\n",
              " ('Essays', 64),\n",
              " ('Fiction', 29),\n",
              " ('Plays', 17),\n",
              " ('Non-Fiction', 4),\n",
              " ('1900', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP5NWHO4LjUB"
      },
      "source": [
        "# записываю название текстов на диск (кэш)\n",
        "\n",
        "with open('/content/drive/My Drive/Coursework2021/en_corpus/2mil.txt', 'w') as f:\n",
        "    for fname in fnames_sample:\n",
        "        print(fname, file=f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gakdlRm6KnWf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poeupNLLKm5q"
      },
      "source": [
        "# считываю обратно (кэш)\n",
        "\n",
        "with open('/content/drive/My Drive/Coursework2021/en_corpus/2mil.txt', 'r') as f:\n",
        "    fnames_sample = f.read().split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "18ecb5d991bb4580a4ffcb4361f16a0d",
            "4163aa1c6c7f4153a9cb509649b27b81",
            "c5cc4451459b4c8d802bab213bef9fb2",
            "4870230e7edc49b4a9fbb9f95831d40b",
            "3b473178814743aab8215130df0a6b18",
            "73666d7517a34b1681ad253b7a92345b",
            "22abc27fd1164da6a78f22c09fb9f216",
            "d832abb6af2f4cf9a6910576c3b4630b"
          ]
        },
        "id": "uXtK9IkE68uf",
        "outputId": "23e0cc2d-9029-4831-ac66-b3a7c07fb5be"
      },
      "source": [
        "ngrams = set()\n",
        "for fname in tqdm(fnames_sample):\n",
        "    with open(fname, 'r') as f:\n",
        "        words = f.read().split()\n",
        "        for i in range(2, len(words)):\n",
        "            ngrams.add(' '.join(words[i - 2:i]))\n",
        "print(len(ngrams))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18ecb5d991bb4580a4ffcb4361f16a0d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=300.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "745960\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vDA7o3669mv"
      },
      "source": [
        "# print(ngrams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_II8f_q669dF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnqwkdTbo-Qg"
      },
      "source": [
        "### Выбор текстов для автоенкодера (1-2 млн токенов)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "GNwTqktE69Vq",
        "outputId": "c93dbc0f-468a-42ea-9165-7ef3843ca519"
      },
      "source": [
        "# выбираю подвыборку английских текстов для препрогона\n",
        "\n",
        "UPPER_BOUND = 200_000\n",
        "fnames_no_poetry = [fname for fname, sz in file_sizes.items() if 'poetry' not in fname.lower() and 10000 < sz < UPPER_BOUND]\n",
        "\n",
        "fname_sample_idx = np.random.choice(len(fnames_no_poetry), 30, replace=False)\n",
        "\n",
        "plt.hist([file_sizes[fnames_no_poetry[i]] for i in fname_sample_idx], bins=50)\n",
        "plt.title(f'Распределение длин выбранных небольших текстов (<{UPPER_BOUND} слов)')\n",
        "print(f'Всего слов {sum(file_sizes[fnames_no_poetry[i]] for i in fname_sample_idx)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Всего слов 1889666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojb64FNm69Ge",
        "outputId": "64b051d6-32ac-4df6-be4f-55ba77a9c503"
      },
      "source": [
        "fnames_sample = [fnames_no_poetry[i] for i in fname_sample_idx]\n",
        "authors = Counter([re.findall('/content/drive/My Drive/Coursework2021/EnLit/EnLit/(.*)/', fname)[0] for fname in fnames_sample])\n",
        "text_styles = Counter([re.findall('\\((.*?)\\)', fname)[0] for fname in fnames_sample])\n",
        "print('Число текстов на автора')\n",
        "authors.most_common()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Число текстов на автора\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Gilbert Keith Chesterton', 3),\n",
              " ('Joseph A. Altsheler', 3),\n",
              " ('Arthur Conan Doyle', 2),\n",
              " ('John Kendrick Bangs', 2),\n",
              " ('Nathaniel Hawthorne', 2),\n",
              " ('Maria Edgeworth', 2),\n",
              " ('Frank Norris', 1),\n",
              " ('Daniel Defoe', 1),\n",
              " ('Edward Payson Roe', 1),\n",
              " ('Aldous Huxley', 1),\n",
              " ('Maynard Barbour', 1),\n",
              " ('Thomas Hardy', 1),\n",
              " ('Christopher Marlowe', 1),\n",
              " ('H. Rider Haggard', 1),\n",
              " ('D.H. Lawrence', 1),\n",
              " ('Irving Bacheller', 1),\n",
              " ('William MacLeod Raine', 1),\n",
              " ('Lucy Maud Montgomery', 1),\n",
              " ('Robert Louis Stevenson', 1),\n",
              " ('Christopher Morley', 1),\n",
              " ('Mary Roberts Rinehart', 1),\n",
              " ('William Makepeace Thackeray', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnyz_sSipnmf",
        "outputId": "d9197b2c-d47d-4003-d21e-ff41cb16ebd0"
      },
      "source": [
        "word_count = defaultdict(int)\n",
        "authors_list = [re.findall('/content/drive/My Drive/Coursework2021/EnLit/EnLit/(.*)/', fname)[0] for fname in fnames_sample]\n",
        "for author, fname in zip(authors_list, fnames_sample):\n",
        "    word_count[author] += file_sizes[fname]\n",
        "\n",
        "sorted_word_count = sorted(word_count.items(), key=lambda kv: kv[1], reverse=True)\n",
        "\n",
        "# sorting dict\n",
        "OrderedDict(sorted_word_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('Joseph A. Altsheler', 267400),\n",
              "             ('D.H. Lawrence', 185757),\n",
              "             ('Gilbert Keith Chesterton', 148664),\n",
              "             ('Thomas Hardy', 129973),\n",
              "             ('Nathaniel Hawthorne', 127909),\n",
              "             ('Maria Edgeworth', 120180),\n",
              "             ('Maynard Barbour', 115887),\n",
              "             ('Lucy Maud Montgomery', 107268),\n",
              "             ('Arthur Conan Doyle', 106269),\n",
              "             ('Irving Bacheller', 101862),\n",
              "             ('Daniel Defoe', 80542),\n",
              "             ('William MacLeod Raine', 75566),\n",
              "             ('John Kendrick Bangs', 59730),\n",
              "             ('H. Rider Haggard', 53923),\n",
              "             ('Christopher Morley', 52451),\n",
              "             ('Frank Norris', 51539),\n",
              "             ('Mary Roberts Rinehart', 35702),\n",
              "             ('William Makepeace Thackeray', 18449),\n",
              "             ('Robert Louis Stevenson', 14569),\n",
              "             ('Christopher Marlowe', 14303),\n",
              "             ('Aldous Huxley', 11038),\n",
              "             ('Edward Payson Roe', 10685)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "DpFxHOEgrGPM",
        "outputId": "a896f6bc-e476-4b34-b4bc-ef5665cb7bd2"
      },
      "source": [
        "# выбранные файлы:\n",
        "\n",
        "fnames_sample"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[\"/content/drive/My Drive/Coursework2021/EnLit/EnLit/Frank Norris/Blix(Fiction).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/Arthur Conan Doyle/The Mystery of Cloomber(Fiction).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/Daniel Defoe/Military Memoirs of Capt. George Carleton(Non-Fiction).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/Edward Payson Roe/A Native Author Called Roe(Essays).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/Aldous Huxley/The Gioconda Smile(Short Stories).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/Gilbert Keith Chesterton/The Man Who Was Thursday(Fiction).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/Joseph A. Altsheler/The Lords of the Wild(Fiction).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/Maynard Barbour/The Award of Justice(Fiction).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/Gilbert Keith Chesterton/The Defendant(Non-Fiction).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/Thomas Hardy/A Pair of Blue Eyes(Fiction).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/Gilbert Keith Chesterton/Orthodoxy(Non-Fiction).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/John Kendrick Bangs/Mrs. Raffles(Fiction).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/Nathaniel Hawthorne/Chiefly About War Matters(Essays).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/John Kendrick Bangs/Mr. Munchausen(Fiction).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/Joseph A. Altsheler/The Riflemen of the Ohio(Fiction).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/Christopher Marlowe/The Tragedy of Dido Queene of Carthage(Short Stories).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/H. Rider Haggard/Elissa(Fiction).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/D.H. Lawrence/The Rainbow(Fiction).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/Irving Bacheller/A Man for the Ages(Fiction).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/William MacLeod Raine/Man Size(Fiction).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/Maria Edgeworth/The Absentee(Fiction).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/Lucy Maud Montgomery/Short Stories  1909 - 1922(Fiction).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/Arthur Conan Doyle/Uncle Bernac(Fiction).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/Maria Edgeworth/The Rose  Thistle  and Shamrock(Plays).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/Robert Louis Stevenson/Fables(Short Stories).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/Christopher Morley/Mince Pie(Fiction).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/Mary Roberts Rinehart/The Case of Jennie Brice(Fiction).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/Joseph A. Altsheler/The Sword of Antietam(Fiction).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/Nathaniel Hawthorne/Our Old Home(Non-Fiction).txt\", \"/content/drive/My Drive/Coursework2021/EnLit/EnLit/William Makepeace Thackeray/The Wolves and the Lamb(Plays).txt\"]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "qk5zBedZs3gK",
        "outputId": "0b938537-955b-4666-9810-4235c9ab621f"
      },
      "source": [
        "# скачиваю файлы для переноса на ск\n",
        "\n",
        "from google.colab import files\n",
        "for fname in fnames_sample[15:]:\n",
        "    files.download(fname)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_0294391d-9b48-4b66-9c04-0c5955668cb3\", \"The Tragedy of Dido Queene of Carthage(Short Stories).txt\", 79433)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_f5feb102-7613-4968-9c9b-997e3c127400\", \"Elissa(Fiction).txt\", 291909)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_8c3edc53-f267-4846-b6a7-808c552b92ea\", \"The Rainbow(Fiction).txt\", 1032455)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_884914da-5ad0-4b7e-a0e8-51df1a7eda99\", \"A Man for the Ages(Fiction).txt\", 544765)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_04bcad08-2a55-4846-8556-b250f75d2df2\", \"Man Size(Fiction).txt\", 420501)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_f1d98326-958b-4b06-afd0-5d1b9a132443\", \"The Absentee(Fiction).txt\", 577574)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_c8ccc200-8ede-4fdc-a442-9295b1547952\", \"Short Stories  1909 - 1922(Fiction).txt\", 590667)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_8603e192-78d4-48be-85c4-95543e866515\", \"Uncle Bernac(Fiction).txt\", 309386)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_67b6d4ef-d2d4-42e3-ad2e-09335b3f0603\", \"The Rose  Thistle  and Shamrock(Plays).txt\", 100642)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_317ca122-bd18-427c-be72-5b9e1a842572\", \"Fables(Short Stories).txt\", 76499)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_340f04c3-b937-4a6b-9e0f-f4bf9bc46f60\", \"Mince Pie(Fiction).txt\", 294235)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_97620703-bca2-41db-a183-c127b03e7eed\", \"The Case of Jennie Brice(Fiction).txt\", 193000)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_63351664-6d70-4d83-aa34-ea8d2fb8cb2e\", \"The Sword of Antietam(Fiction).txt\", 488097)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_6b1c3707-4672-43e8-b5ff-d704200076dd\", \"Our Old Home(Non-Fiction).txt\", 681341)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_81c433db-300c-4cd7-9201-3888c0c28e23\", \"The Wolves and the Lamb(Plays).txt\", 107033)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "733FEUwItYc3",
        "outputId": "34654026-1516-4f96-f11e-b1bac0287f37"
      },
      "source": [
        "len(fnames_sample)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utDIeMONNP7b"
      },
      "source": [
        "### Аналитика на `english_corpus.txt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxfSoOPnehP7",
        "outputId": "96fad780-b456-47d9-e717-749c0de487f2"
      },
      "source": [
        "print('Слов во всех текстах:', sum(doc_sizes))\n",
        "word_sets = [set(doc.split()) for doc in documents]\n",
        "unique_words = set()\n",
        "for st in word_sets:\n",
        "    unique_words |= st\n",
        "print('Уникальных:', len(unique_words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Слов во всех текстах: 234390533\n",
            "Уникальных: 315436\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbbJhL39U6aJ",
        "outputId": "9ecc8c7a-3558-4044-b4a6-fe81d6948951"
      },
      "source": [
        "len(documents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11008"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "vnJ8e55tViEK",
        "outputId": "f8ecbd90-0845-49d7-df33-e5716a6819e7"
      },
      "source": [
        "documents[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ch the early king of GPE1 the country -PRON- now know as GPE1 be the tract of land shut in by ORG1 the bay of biscay the GPE1 the LOC1 and the LOC1 but this country only gain the name of GPE1 by degree in DATE1 of which -PRON- have any account -PRON- be people by the NORP1 and -PRON- be know to the NORP1 as part of a large country which bear the name of PERSON1 after all of -PRON- save LOC1 or what -PRON- now call GPE1 have be conquer and settle by the NORP1 -PRON- be overrun by tribe of the great NORP1 race the same family to which NORP1 belong of these tribe the goth settle in the province to the south the NORP1 in the LOC1 around the LOC1 while the frank come over the river in -PRON- unprotected LOC1 corner and make -PRON- master of a far wide territory break up into CARDINAL1 kingdom that of the eastern franks in what be now GPE1 and that of the western franks reach from the LOC1 to ORG1 subdue all the other NORP1 conqueror of PERSON1 while -PRON- adopt the religion the language and some of the civilization of the romanize NORP1 who become -PRON- subject under the ORDINAL1 NORP1 dynasty the empire be renew in the LOC1 where -PRON- have be for a time put an end to by these NORP1 invasion and the then NORP1 king PERSON1 take -PRON- place as emperor at -PRON- head but in the time of -PRON- grandson the various kingdom and nation of which the empire be compose fall apart again under different descendant of -PRON- CARDINAL1 of these PERSON1 bald be make king of the western franks in what be term the NORP1 or not eastern GPE1 from which the present GPE1 have spring this GPE1 in name cover all the country LOC1 of LOC1 but practically the NORP1 king have little power south of the GPE1 and ORG1 be never include in -PRON- the ORG1 of GPE1 the great danger which this NORP1 GPE1 have to meet come from the NORP1 or as -PRON- be call in GPE1 the NORP1 these ravage in GPE1 as -PRON- ravage in GPE1 and a large part of the northern coast include the mouth of the seine be give by PERSON1 bald to PERSON1 or rollo CARDINAL1 of -PRON- leader whose land become know as the northman be land or GPE1 what most check the ravage of these pirate be the resistance of GPE1 a town which command the road along LOC1 and -PRON- be in defend the city of GPE1 from the NORP1 that a warrior name PERSON1 the strong gain the trust and affection of the inhabitant of the NORP1 GPE1 -PRON- and -PRON- family become count i e judge and protector of GPE1 and duke or leader of the franks CARDINAL1 generation of -PRON- be really great man PERSON1 the strong odo and PERSON1 and when the descendant of PERSON1 have die out a duke of the PERSON1 be in crown king of the frank all the after king of GPE1 down to PERSON1 be descendant of PERSON1 by this change however -PRON- gain little in real power for though -PRON- claim to rule over the whole country of ORG1 -PRON- authority be little heed save in the domain which -PRON- have possess as count of GPE1 include the city of GPE1 GPE1 and GPE1 the coronation place -PRON- be guardian too of the great abbeys of PERSON1 and PERSON1 of tours the duke of GPE1 and the count of GPE1 to the LOC1 the count of GPE1 to the north the count of champagne to the LOC1 and the duke of GPE1 to the south pay -PRON- homage but be the only actual ruler in -PRON- own domain the GPE1 of PERSON1 the language of PERSON1 be GPE1 be clip NORP1 the peasantry and townsman be mostly gaulish the noble be almost entirely PERSON1 there be an understanding that the king could only act by -PRON- consent and must be choose by -PRON- but matter go more by old custom and the right of the strong than by any law a salic law so call from the place whence the franks have come be suppose to exist but this have never be use by -PRON- subject whose law remain that of the old NORP1 empire both of these system of law however fall into disuse and be replace by rude body of custom which gradually grow up the habit of the time be exceedingly rude and ferocious the frank have be the fierce and most untamable of all the NORP1 nation and only submit -PRON- to the influence of NORP1 and civilization from the respect which GPE1 inspire PERSON1 have try to bring in LOC1 but -PRON- find -PRON- reproach the young frank in -PRON- school with let -PRON- be surpass by the NORP1 whom -PRON- despise and in the disorder that follow -PRON- death barbarism increase again the convent alone keep up any remnant of culture but as the fury of the NORP1 be chiefly direct to -PRON- number have be destroy and there be more ignorance and wretchedness than at any other time in the duchy of GPE1 much more of the old ORG1 survive both among the city and the nobility and the NORP1 newly settle in the north have bring with -PRON- the vigour of -PRON- race -PRON- have take up such dead or die culture as -PRON- find in GPE1 and be carry -PRON- further so as in some degree to awaken -PRON- neighbour king and -PRON- great vassal could generally read and write and understand the NORP1 in which all record be make but few except the clergy study at all there be school in convent and already at GPE1 a university be grow up for the study of theology grammar law philosophy and music the science which be hold to form a course of education the doctor of these science lecture the scholar of low degree live beg and struggle as best -PRON- could and gentleman be lodge with clergy who serve as a sort of private tutor early king of ORG1 nor the next CARDINAL1 king PERSON1 be able man and -PRON- be almost helpless among the fierce noble of -PRON- own domain and the great count and duke around -PRON- castle be build of huge strength and serve as nest of plunderer who prey on traveller and make war on each other grievously torment CARDINAL1 another be villein as the peasant be term man could travel nowhere in safety and horrid ferocity and misery prevail the ORDINAL1 CARDINAL1 king be good and pious man but too weak to deal with -PRON- ruffian noble PERSON1 call the pious be extremely devout but weak -PRON- become embroil with the pope on account of have marry PERSON1 a lady pronounce to be within the degree of affinity prohibit by the church -PRON- be excommunicate but hold out till there be a great religious reaction produce by the belief that the world would end in in this expectation many person leave -PRON- land untilled and the consequence be a terrible famine follow by a pestilence and the misery of GPE1 be probably unequalled in this reign when -PRON- be hardly possible to pass safely from CARDINAL1 to another of the CARDINAL1 royal city GPE1 and tour beggar swarm and the king give to -PRON- everything -PRON- could lay -PRON- hand on and even wink at -PRON- steal gold off -PRON- dress to the great wrath of a ORDINAL1 wife the imperious PERSON1 of PERSON1 who come from the more luxurious and corrupt south hated and despise the roughness and asceticism of -PRON- husband -PRON- be a fierce and passionate woman and bring an element of cruelty into the court in this reign the ORDINAL1 instance of persecution to the death for heresy take place the victim have be the queen be confessor but so far be -PRON- from pity -PRON- that -PRON- strike out CARDINAL1 of -PRON- eye with -PRON- staff as -PRON- be lead past -PRON- to the hut where -PRON- be shut in and burn on PERSON1 be death PERSON1 take part against -PRON- son PERSON1 i on behalf of -PRON- young brother but PERSON1 prevail during -PRON- reign the clergy succeed in proclaim what be call the truce of god which forbid war and bloodshed at certain season of DATE1 and on DATE1 and make church and clerical land place of refuge and sanctuary which often indeed protect the lawless but which also save the weak and oppress -PRON- be during these reign that the papacy be begin the great struggle for temporal power and freedom from the influence of the empire which result in the increase independence and power of the clergy the religious fervour which have begin with DATE1 lead to the foundation of many monastery and to much grand church architecture in the reign of ORG1 of GPE1 obtain GPE1 and thus become far more powerful than -PRON- suzerain PERSON1 a weak man of vicious habit who lie for DATE1 of -PRON- life under sentence of excommunication for an adulterous marriage with PERSON1 countess of GPE1 the power of the king and of the law be probably at the very low ebb during the time of WORK_OF_ART1 though mind and manner be less debased than in DATE1 the ORDINAL1 crusade pilgrimage to the holy land have now become CARDINAL1 great mean by which the man of the LOC1 seek pardon for -PRON- sin GPE1 have long be hold by the NORP1 who have treat the pilgrim well but these have be conquer by a fierce turcoman tribe who rob and oppress the pilgrim PERSON1 the hermit return from a pilgrimage persuade PERSON1 that -PRON- would be well to stir up ORG1 to drive back the NORP1 power and deliver GPE1 and ORG1 accordingly when hold a council at GPE1 in GPE1 permit PERSON1 to describe in glowing word the misery of pilgrim and the profanation of the holy place cry break out god will -PRON- and multitude throng to receive crosse cut out in cloth which be fasten to the shoulder and pledge the wearer to the holy war or crusade as -PRON- be call WORK_OF_ART1 take no interest in the cause but -PRON- brother PERSON1 count of ORG1 count of PERSON1 of GPE1 and PERSON1 GPE1 join the expedition which be make under PERSON1 of PERSON1 duke of PERSON1 or what -PRON- now call the GPE1 the crusade prove successful GPE1 be gain and a GPE1 of detached city and fort be found in GPE1 of which PERSON1 become the ORDINAL1 king the whole of the LOC1 be suppose to keep up the defence of the holy land but in fact most of those who go as armed pilgrim be either NORP1 NORP1 or NORP1 and the man of the LOC1 call all alike frank CARDINAL1 order of monk who be also knights become the permanent defender of the GPE1 the knights of PERSON1 also call hospitaller because -PRON- also lodge pilgrim and tend the sick and the knights templars both have establishment in different country in LOC1 where youth be train to the rule of -PRON- order the old custom of solemnly gird a young warrior with -PRON- sword be develop into a system by which the nobly bear man be train through the rank of page and squire to full knighthood and make to take vow which bind -PRON- to honourable custom to equal though unhappily no account be take of -PRON- inferior PERSON1 and PERSON1 be son PERSON1 or the fat be the ORDINAL1 able man whom the line of PERSON1 have produce since -PRON- mount the throne -PRON- make the ORDINAL1 attempt at curb the noble assist by suger PERSON1 of PERSON1 the only possibility of do this be to obtain the aid of CARDINAL1 party of noble against another and when any unusually flagrant offence have be commit GPE1 call together the noble bishop and abbot of -PRON- domain and obtain -PRON- consent and assistance in make war on the guilty man and overthrow -PRON- castle thus in some degree lessen the sense of utter impunity which have cause so many violence and such savage recklessness -PRON- also permit a few of the city to purchase the right of self government and freedom from the ill usage of the count who from -PRON- guardian have become -PRON- tyrant but in this -PRON- seem not to have be so much guide by any fixed principle as by -PRON- private interest and feeling towards the individual city or lord in question however ORG1 have begin to be respect by when PERSON1 die have just effect the marriage of -PRON- son PERSON1 with PERSON1 the heiress of the duke of GPE1 thus hope to make the crown really more powerful than the great prince who owe -PRON- homage at this time live the great st PERSON1 abbot of PERSON1 who have a wonderful influence over man be mind -PRON- be a time of much thought and speculation and PERSON1 an able student of ORG1 hold a controversy with PERSON1 in which -PRON- see the ORDINAL1 struggle between intellect and authority PERSON1 rouse the young king PERSON1 to go on the ORDINAL1 crusade which be undertake by the emperor and the other prince of LOC1 to relieve the distress of GPE1 have no ORG1 so the war be by land through the rugged hill of LOC1 minor where the army be almost destroy by the PERSON1 though GPE1 do reach GPE1 -PRON- be with weakened force -PRON- could effect nothing by -PRON- campaign and PERSON1 who have accompany -PRON- seem to have be entirely corrupt by the evil habit of the frank settle in the LOC1 soon after -PRON- return GPE1 dissolve -PRON- marriage and PERSON1 become the wife of PERSON1 GPE1 who soon after inherit GPE1 as -PRON- PERSON1 as well as the duchy of GPE1 and betroth -PRON- ORDINAL1 son to the heiress of PERSON1 be marriage seem to undo all that PERSON1 have do in raise the royal power for PERSON1 completely overshadow GPE1 whose only resource be in feeble endeavour to take part against -PRON- in -PRON- many family quarrel the whole reign of GPE1 the young the title that adhere to -PRON- on account of -PRON- simple childish nature be only a record of weakness and disaster till -PRON- die in what life go on in GPE1 go on principally in the south the land of GPE1 and PERSON1 have never drop the old classical love of poetry and art a soft form of broken NORP1 be then speak and the art of minstrelsy be frequent among all rank poet be call PRODUCT1 and trouv res finder court of love be hold where there be competition in poetry the prize be a golden violet and many of the brave warrior be also distinguish PRODUCT1 among -PRON- the eld son of PERSON1 there be much license of manner much turbulence and as the NORP1 hate angevin rule the PRODUCT1 never cease to stir up the son of PERSON1 against -PRON- PERSON1 powerful in fact as PERSON1 be -PRON- be -PRON- gathering so large a part of GPE1 under -PRON- rule which be in the end to build up the greatness of the NORP1 king what have hold -PRON- in check be the existence of the great fief or province each with -PRON- own line of duke or count and all practically independent of the king but now nearly all the province of southern and western GPE1 be gather into the hand of a single ruler and though -PRON- be a NORP1 in blood yet as -PRON- be king of GPE1 this ruler seem to -PRON- NORP1 subject no NORP1 but a foreigner -PRON- begin therefore to look to the NORP1 king to free -PRON- from a foreign ruler and the son of PERSON1 call PERSON1 be ready to take advantage of -PRON- disposition PERSON1 be a really able man make up by address for want of personal courage -PRON- set -PRON- to lower the power of the ORG1 of GPE1 and increase that of the ORG1 of GPE1 as a boy -PRON- have watch conference between -PRON- father and PERSON1 under the great elm of gisor on the border of GPE1 and see -PRON- father overreach -PRON- lay up a store of hatred to the rival king as soon as -PRON- have the power -PRON- cut down the elm which be so large that horseman could be shelter under -PRON- branch -PRON- support the son of PERSON1 in -PRON- rebellion and be always the bitter foe of the head of the family PERSON1 assume the cross in on the tiding of the loss of GPE1 and in joined PERSON1 at GPE1 where -PRON- winter and then sail for FAC1 after this city be take PERSON1 return to GPE1 where -PRON- continue to profit by the crime and dissension of the angevin and gain both as -PRON- enemy and as king of GPE1 when PERSON1 be successor PERSON1 the heir of the dukedom of GPE1 and claimant of both GPE1 and PERSON1 take advantage of the general indignation to hold a court of peer in which PERSON1 on -PRON- non appearance be adjudge to have forfeit -PRON- fief in the war which follow and end in PERSON1 not only gain the great PERSON1 dukedom which give -PRON- the command of GPE1 and of the mouth of the seine as well as ORG1 and poitou the country which hold the GPE1 in -PRON- power but establish the precedent that a crown vassal be amenable to justice and may be make to forfeit -PRON- land what -PRON- have win by the sword -PRON- hold by wisdom and good government see that the city be capable of be make to balance the power of the noble -PRON- grant -PRON- privilege which cause -PRON- to be esteem -PRON- good friend and -PRON- promote all improvement though once lay under an interdict by pope innocent iii for an unlawful marriage PERSON1 usually follow the policy which gain for the king of GPE1 the title of most NORP1 king the real meaning of this be that -PRON- should always support the pope against the emperor and in return be allow more than ordinary power over -PRON- clergy the great feudal vassal of eastern GPE1 with a strong instinct that -PRON- be -PRON- enemy make a league with the emperor PERSON1 and -PRON- uncle PERSON1 against PERSON1 attack -PRON- in the south and be repulse by PERSON1 be PERSON1 call the lion while the king -PRON- back by the burgher of -PRON- chief city gain at ORG1 over PERSON1 the ORDINAL1 real NORP1 victory in thus establish the power of the crown DATE1 FAC1 who have marry PERSON1 be niece PERSON1 of GPE1 be invite by the NORP1 baron to become -PRON- king on PERSON1 be refuse to be bind by the great charter and PERSON1 see -PRON- son actually in possession of GPE1 at the time of the death of the last of the son of -PRON- enemy PERSON1 on PERSON1 be death however the baron prefer -PRON- child to the NORP1 prince and fall away from GPE1 who be force to return to GPE1 the ORG1 the next great step in the building up of the NORP1 GPE1 be make by take advantage of a religious strife in the south the land near the LOC1 still have much of the old LOC1 and also of the old corruption and here arise a sect call the ORG1 who hold opinion other than those of the church on the origin of evil pope innocent iii after send some of the order of friar freshly establish by the NORP1 dominic to preach to -PRON- in vain declare -PRON- as great enemy of the faith as NORP1 and proclaim a crusade against -PRON- and -PRON- chief supporter PERSON1 GPE1 shrewd old king PERSON1 merely permit this crusade but the dislike of the north of GPE1 to the south make host of adventurer flock to the banner of -PRON- leader PERSON1 a PERSON1 baron devout and honourable but harsh and pitiless dreadful execution be do the whole country be lay waste and PERSON1 reduce to such distress that PERSON1 i king of GPE1 who be regard as the natural head of the southern race come to -PRON- aid but be defeat and slay at the battle of PERSON1 after this PERSON1 be force to submit but such hard term be force on -PRON- that -PRON- people revolt -PRON- country be grant to PERSON1 who lay siege to GPE1 and be kill before -PRON- could take the city the war be then carry on by FAC1 who have succeed -PRON- father as PERSON1 in though only to reign DATE1 as -PRON- die of a fever catch in a southern campaign in -PRON- widow PERSON1 make peace in the name of -PRON- PERSON1 ix and PERSON1 be force to give -PRON- only daughter in marriage to CARDINAL1 of -PRON- young son on -PRON- death the county of GPE1 lapse to the crown which thus become possessor of all southern GPE1 save PERSON1 which still remain to the NORP1 king but the whole of the district once people by the ORG1 have be so much waste as never to recover -PRON- prosperity and any cropping up of -PRON- opinion be guard against by the establishment of the inquisition which appoint NORP1 friar to inquire into and exterminate all that differ from the church at the same time the order of PERSON1 do much to instruct and quicken the conscience of the people and at the university especially that of GPE1 a great advance both in thought and learning be make PERSON1 be confessor PERSON1 found for the study of divinity the college which be know by -PRON- name and whose decision be afterwards receive as of ORG1 the ORG1 of GPE1 GPE1 have a wise ruler in PERSON1 and a still well CARDINAL1 in -PRON- PERSON1 ix who be well know as GPE1 and who be a really good and great man -PRON- be the ORDINAL1 to establish the ORG1 of GPE1 a court consist of the great feudal vassal lay and ecclesiastical who hold of the king direct and who have to try all cause -PRON- much dislike give such attendance and a certain number of man train to the law be add to -PRON- to guide the decision the ORG1 be thus only a court of justice and an office for register will and edict ORG1 be call the state general and consist of all estate of the realm but be only summon in time of emergency PERSON1 be the ORDINAL1 king to bring noble of the high rank to submit to the judgment of ORG1 when guilty of a crime enguerrand PERSON1 CARDINAL1 of the proud noble of GPE1 who have hang CARDINAL1 NORP1 youth for kill a rabbit be sentence to death the penalty be commute but the principle be establish GPE1 be uprightness and wisdom gain -PRON- honour and love everywhere and -PRON- be always remember as sit under the great oak at GPE1 do equal justice to rich and poor GPE1 be equally upright in -PRON- dealing with foreign power -PRON- would not take advantage of the weakness of PERSON1 of GPE1 to attack -PRON- land in PERSON1 though -PRON- maintain the right of GPE1 to GPE1 as have be forfeit by PERSON1 so much be -PRON- respect that -PRON- be call in to judge between PERSON1 and -PRON- baron respect the oath exact from the king by the mad ORG1 -PRON- decision in favour of PERSON1 be probably an honest CARDINAL1 but -PRON- be mislead by the very different relation of the NORP1 and NORP1 king to -PRON- noble who in GPE1 maintain lawlessness and violence while in GPE1 -PRON- be struggle for law and order throughout the struggle between the pope and the emperor PERSON1 would not be induce to assist in a persecution of the emperor which -PRON- consider unjust nor permit CARDINAL1 of -PRON- son to accept GPE1 and GPE1 when the pope declare that PERSON1 have forfeit -PRON- -PRON- could not however prevent -PRON- brother PERSON1 of GPE1 from accept -PRON- for PERSON1 have marry PERSON1 heiress of the imperial fief of PERSON1 and be thus independent of -PRON- brother GPE1 be able to establish a branch of the NORP1 royal family on the throne at GPE1 the reign of GPE1 be a time of much progress and improvement there be great scholar and thinker at all the university romance and poetry be flourish and influence people be habit so that courtesy i e the manner teach in castle court be soften the demeanour of knight and noble architecture be at -PRON- most beautiful period as be see above all in FAC1 at GPE1 this be build by PERSON1 to receive a gift of the NORP1 emperor namely a thorn which be believe to be from the crown of thorn -PRON- be CARDINAL1 of the most perfect building in existence crusade of PERSON1 during a severe illness make a vow to go on a crusade -PRON- ORDINAL1 fulfilment of this vow be make early in -PRON- reign in when -PRON- mother be still alive to undertake the PERSON1 -PRON- attempt be to attack the heart of the saracen power in GPE1 and -PRON- effect a landing and take the city of GPE1 there -PRON- leave -PRON- queen and advance on GPE1 but near PERSON1 -PRON- find -PRON- entangle in the canal of the LOC1 and with a great army of mamelukes in front a ORG1 be find and the NORP1 earl of GPE1 who have bring a troop to join the crusade advise that the ORDINAL1 to cross should wait and guard the passage of the next but the king be brother PERSON1 of ORG1 call this cowardice the earl be sting and declare -PRON- would be as forward among the foe as any NORP1 -PRON- both charge headlong be enclose by the enemy and slay and though the king at last put the mamelukes to flight -PRON- loss be dreadful the LOC1 rose and cut off -PRON- return -PRON- lose great part of -PRON- troop from sickness and be horribly harass by the mamelukes who throw among -PRON- host a strange burning missile call NORP1 fire and -PRON- be finally force to surrender -PRON- as a prisoner at PERSON1 with all -PRON- army -PRON- obtain -PRON- release by give up GPE1 and pay a heavy ransom after DATE1 in -PRON- attempt another crusade which be still more unfortunate for -PRON- land at GPE1 to wait for -PRON- brother to arrive from GPE1 apparently on some delusion of favourable disposition on the part of the PERSON1 sickness break out in the camp and the king -PRON- daughter and -PRON- ORDINAL1 son all die of fever and so fatal be the expedition that -PRON- son PERSON1 return to GPE1 escort CARDINAL1 coffin those of -PRON- father -PRON- brother -PRON- sister and -PRON- husband and -PRON- own wife and child PERSON1 the fair the reign of PERSON1 be very short the insolence and cruelty of the prove als in GPE1 have provoke the native to a massacre know as the NORP1 vesper and -PRON- then call in the king of GPE1 who finally obtain the island as a separate GPE1 from that on the NORP1 mainland where GPE1 and -PRON- descendant still reign while fight -PRON- uncle be battle on the GPE1 and besiege PERSON1 catch a fever and die on -PRON- way home in -PRON- successor PERSON1 call the fair be crafty cruel and greedy and make the ORG1 of GPE1 the instrument of -PRON- violence and exaction which -PRON- carry out in the name of the law to prevent ORG1 count of GPE1 from marry -PRON- daughter to the son of PERSON1 i of GPE1 -PRON- invite -PRON- and -PRON- father to -PRON- court and throw -PRON- both into prison while -PRON- offer -PRON- own daughter PERSON1 to PERSON1 of PERSON1 in -PRON- stead the NORP1 war prevent PERSON1 i from take up the cause of guy but the pope PERSON1 a man of a fierce temper though of a great age loudly call on PERSON1 to do justice to GPE1 and likewise blame in unmeasured term -PRON- exaction from the clergy -PRON- debasement of the coinage and -PRON- foul and vicious life furious abuse pass on both side PERSON1 avail -PRON- of a flaw in the pope be election to threaten -PRON- with deposition and in return be excommunicate -PRON- then send a NORP1 knight name PERSON1 with PERSON1 a turbulent NORP1 the hereditary enemy of PERSON1 and a band of savage mercenary soldier to anagni where the pope then be to force -PRON- to recall the sentence apparently intend -PRON- to act like the murderer of PERSON1 the old man be dignity however overawe -PRON- at the moment and -PRON- retire without lay hand on -PRON- but the shock -PRON- have undergo cause -PRON- death DATE1 -PRON- successor be poison almost immediately on -PRON- election be know to be adverse to ORG1 be equally balanced in the conclave but PERSON1 be friend advise -PRON- to buy over to -PRON- interest CARDINAL1 of -PRON- suppose foe whom -PRON- would then unite in choose PERSON1 of GPE1 be the man and in a secret interview promise PERSON1 to fulfil CARDINAL1 condition if -PRON- be make pope by -PRON- interest these be st the reconciliation of PERSON1 with the church nd that of -PRON- agent GPE1 a grant to the king of CARDINAL1 of all clerical property for DATE1 th the restoration of the PERSON1 family to GPE1 th the censure of PERSON1 be memory these CARDINAL1 be carry out by PERSON1 as -PRON- call -PRON- as soon as -PRON- be on the papal throne the ORDINAL1 remain a secret but be probably the destruction of the knights templars this order of military monk have be create for the defence of the crusade GPE1 of GPE1 and have acquire large possession in LOC1 now that -PRON- occupation in the LOC1 be go -PRON- be hate and dread by the king and PERSON1 be resolve on -PRON- wholesale destruction the papacy at ORG1 have never quit GPE1 but have go through the ceremony of -PRON- installation at PERSON1 and PERSON1 fear that in GPE1 -PRON- would avoid carry out the scheme for the ruin of the templars have -PRON- conduct to ORG1 a city of the empire which belong to the angevin king of GPE1 as count of PERSON1 and there for DATE1 the papal court remain as -PRON- be thus settle close to the NORP1 frontier the pope become almost vassal of GPE1 and this add greatly to the power and renown of the NORP1 king how real -PRON- hold on the papacy be be show in the ruin of the templars the order be now abandon by the pope and -PRON- knight be invite in large number to GPE1 under pretence of arrange a crusade have be thus entrap -PRON- be accuse of horrible and monstrous crime and torture elicit a few suppose confession -PRON- be then try by the inquisition and the great number be put to death by fire the grand master last of all while -PRON- land be seize by the king -PRON- seem to have be really a fierce arrogant and oppressive set of man or else there must have be some endeavour to save -PRON- belong as most of -PRON- do to noble NORP1 family the pest of GPE1 as PERSON1 call PERSON1 the fair be now the most formidable prince in LOC1 -PRON- contrive to annex to -PRON- dominion the city of PERSON1 hitherto an imperial city under -PRON- archbishop PERSON1 die in and -PRON- CARDINAL1 son LOC1 PERSON1 and PERSON1 be as cruel and harsh as -PRON- but without -PRON- talent and bring the crown and people to disgrace and misery each reign DATE1 and then die leave only daughter and the question arise whether the inheritance should go to female when LOC1 die in -PRON- brother PERSON1 after wait for the birth of a posthumous child who only live DATE1 take the crown and the ORG1 then declare that the law of the old salian franks have be against the inheritance of woman by this newly discover ORG1 the ORDINAL1 brother reign on PERSON1 be death but the GPE1 of PERSON1 have accrue to the family through -PRON- grandmother and not be subject to LAW1 go to the eld daughter of PERSON1 wife of the count of evreux ch DATE1 war war of PERSON1 by LAW1 as the lawyer call -PRON- the crown be give on the death of PERSON1 to PERSON1 of ORG1 to a brother of PERSON1 but -PRON- be claim by PERSON1 of GPE1 as son of the daughter of PERSON1 content -PRON- however with the mere assertion of -PRON- pretension until PERSON1 exasperate -PRON- by attack on the border of PERSON1 which the NORP1 king have long be covet to complete -PRON- possession of the south and by demand the surrender of PERSON1 of ORG1 who be disappoint in -PRON- claim to the county of ORG1 by the judgment of the ORG1 of GPE1 be practise by sorcery on the life of PERSON1 PERSON1 then declare war and -PRON- suppose right cause a century of warfare between GPE1 and GPE1 in which the break down trodden state of the NORP1 peasantry give GPE1 an immense advantage the knights and squires be fairly match but while the NORP1 yeoman be strong staunch and trustworthy the NORP1 be useless and only make a defeat bad by plunder the fall on each side alike the war begin in GPE1 where PERSON1 take the part of the count whose tyranny have cause -PRON- expulsion PERSON1 be call in to the aid of the citizen of GPE1 by -PRON- leader PERSON1 and gain a great victory over the NORP1 fleet at GPE1 but with no important result at the same time the CARDINAL1 king take opposite side in the war of the succession in GPE1 each defend the claim most inconsistent with -PRON- own pretension to the NORP1 PERSON1 uphold the male heir PERSON1 and ORG1 female representative the wife of PERSON1 and GPE1 further difficulty arise through PERSON1 of PERSON1 and count of evreux who be always on the watch to assert -PRON- claim to the NORP1 throne through -PRON- mother the daughter of LOC1 and be much hate and distrust by PERSON1 and -PRON- son PERSON1 of GPE1 fear the disaffection of the PERSON1 and ORG1 invite a number of -PRON- to a tournament at GPE1 and there have -PRON- put to death after a hasty form of trial thus drive -PRON- kindred to join -PRON- enemy CARDINAL1 of these offend PERSON1 of GPE1 invite PERSON1 to GPE1 where -PRON- land and have consume -PRON- supply be on -PRON- DATE1 to GPE1 when PERSON1 with the whole strength of the GPE1 endeavour to intercept -PRON- at ORG1 in GPE1 in PERSON1 be utterly incapable as a general -PRON- knight be wrong head and turbulent and absolutely cut down -PRON- own NORP1 hire archer for be in -PRON- way the defeat be total PERSON1 ride away to GPE1 and PERSON1 lay siege to ORG1 the place be so strong that -PRON- be force to blockade -PRON- and PERSON1 have time to gather another army to attempt -PRON- relief but the NORP1 army be so post that -PRON- could not attack -PRON- without great loss -PRON- retreat and the man of ORG1 surrender PERSON1 insist that CARDINAL1 burgher should bring -PRON- the key with rope round -PRON- neck to submit -PRON- to -PRON- CARDINAL1 offer -PRON- but -PRON- life be spare and -PRON- be honourably treat PERSON1 expel all the NORP1 and make ORG1 an NORP1 settlement a truce follow chiefly in consequence of the ravage of the black death which sweep off multitude throughout LOC1 a pestilence apparently breed by filth famine and all the misery of war and lawlessness but which spare no rank -PRON- have scarcely cease before PERSON1 die in -PRON- son PERSON1 be soon involve in a fresh war with GPE1 by the intrigue of PERSON1 bad and in advanced southwards to check the prince of GPE1 who have come out of PERSON1 on a plunder expedition the NORP1 be again totally route at GPE1 and the king -PRON- with -PRON- ORDINAL1 son PERSON1 be make prisoner and carry to GPE1 with most of the chief noble the jacquerie the call make on -PRON- vassal by these captive noble to supply -PRON- ransom bring the misery to a height the salt tax or ORG1 which be ORDINAL1 impose to meet the expense of the war be only pay by those who be neither clergy nor noble and the general saying be PERSON1 the nickname for the peasant have a broad back let -PRON- bear all the PERSON1 either by the king the feudal lord the clergy or the band of man at arm who rove through the country sell -PRON- to any prince who would employ -PRON- the wretched people be strip of everything and use to hide in hole and cave from ill usage or insult till -PRON- break out in a rebellion call the jacquerie and whenever -PRON- could seize a castle revenge -PRON- like the brute -PRON- have be make on those within -PRON- taxation be so levy by the king be officer as to be frightfully oppressive and corruption reign everywhere as the king be in prison and -PRON- heir PERSON1 have flee ignominiously from GPE1 the citizen of GPE1 hope to effect a reform and rise with -PRON- provost marshal PERSON1 at -PRON- head threaten PERSON1 and slew CARDINAL1 of -PRON- officer before -PRON- eye on -PRON- demand the state general be convoke and make wholesome regulation as to the manner of collect the taxis but no CARDINAL1 except perhaps PERSON1 have any real zeal or public spirit PERSON1 bad of PERSON1 who have pretend to espouse -PRON- cause betray -PRON- the king declare the decision of the states general null and void and the crafty management of -PRON- son prevent any union between the malcontent the gentry rally and put down the jacquerie with horrible cruelty and revenge the burgher of GPE1 find that PERSON1 bad only want to gain the throne and PERSON1 would have proclaim -PRON- but those who think -PRON- even worse than -PRON- cousin of PERSON1 admit the other PERSON1 by whom PERSON1 and -PRON- partisan be put to death the attempt at reform thus end in talk and murder and all fall back into the same state of misery and oppression the peace of GPE1 this PERSON1 eld son of PERSON1 obtain by purchase the imperial fief of PERSON1 of which the count have always be call PERSON1 a title thenceforth bear by the heir apparent of the GPE1 -PRON- father be captivity and the submission of GPE1 leave -PRON- master of the realm but -PRON- do little to defend -PRON- when PERSON1 again attack -PRON- and in -PRON- be force to bow to the term which the NORP1 king demand as the price of peace the peace of GPE1 permit PERSON1 to ransom -PRON- but resign to GPE1 the sovereignty over the duchy of GPE1 and leave ORG1 and ponthieu in the hand of PERSON1 die in before -PRON- ransom be pay and -PRON- son mount the throne as PERSON1 show -PRON- from this time a wary able man and do much to regain what have be lose by craftily watch -PRON- opportunity the war go on between the ally of each party though the NORP1 and NORP1 king profess to be at peace and at the battle of PERSON1 in PERSON1 bad be defeat and force to make peace with GPE1 on the other hand ORG1 in GPE1 lead by PERSON1 and the gallant breton PERSON1 be route DATE1 by the NORP1 party under sir PERSON1 be kill and ORG1 establish in the duchy DATE1 of war have create a dreadful class of man namely hire soldier of all nation who under some noted leader sell -PRON- service to whatever prince may need -PRON- under the name of free company and when unemployed live by plunder the peace have only let these wretch loose on the peasant some have seize castle whence -PRON- could plunder traveller other roam the country prey on the miserable peasant who fleece as -PRON- be by king baron and clergy be torture and murder by these ruffian so that many live in hole in the ground that -PRON- dwelling may not attract attention PERSON1 offer the king to relieve the country from these free company by lead -PRON- to assist the castilian against -PRON- tyrannical king PERSON1 the black prince who be then act as governor of GPE1 take however the part of PERSON1 and defeat du PERSON1 at the battle of navarete on the PERSON1 in renewal of the war this expedition ruin the prince be health and exhaust -PRON- ORG1 a hearth tax be lay on the inhabitant of GPE1 and -PRON- appeal against -PRON- to PERSON1 although by the peace of GPE1 -PRON- have give up all right to hear appeal as suzerain the treaty however be still not formally settle and on this ground PERSON1 receive -PRON- complaint the war thus begin again and the sword of the constable of GPE1 the high military dignity of the realm be give to du PERSON1 but only on condition that -PRON- would avoid pitch battle and merely harass the NORP1 and take -PRON- castle this policy be so strictly follow that the duke of GPE1 be allow to DATE1 from GPE1 to GPE1 without meet an enemy in the field and when king PERSON1 make -PRON- ORDINAL1 and last invasion nearly to the wall of GPE1 -PRON- be only turn back by famine and by a tremendous thunderstorm which make -PRON- believe that heaven be against -PRON- du PERSON1 die while besiege a castle and such be -PRON- fame that the NORP1 captain would place the key in no hand but that of -PRON- corpse the constable be sword be give to PERSON1 also a breton and call the butcher because -PRON- give no quarter to the NORP1 in revenge for the death of -PRON- brother the PERSON1 be almost to a man of ORG1 have be offend by the insolence and oppression of the NORP1 and PERSON1 after cling to ORG1 as long as possible be force to make -PRON- peace at length with PERSON1 have nearly regain all that have be lose when in -PRON- death leave the GPE1 to -PRON- son ORG1 of PERSON1 be a boy of DATE1 motherless and beset with ambitious uncle these uncle be PERSON1 of GPE1 to whom PERSON1 the last of the early angevin line in GPE1 bequeath -PRON- right PERSON1 of GPE1 a weak time server and PERSON1 the able and most honest of the CARDINAL1 -PRON- grandmother PERSON1 the wife of PERSON1 have be heiress of the duchy and county of PERSON1 and these now become -PRON- inheritance give -PRON- the rich part of GPE1 by still well fortune -PRON- have marry PERSON1 the only child of GPE1 count of PERSON1 contain the great cloth manufacture town of LOC1 GPE1 ypres etc all wealthy and independent and much inclined to close alliance with GPE1 whence -PRON- obtain -PRON- wool while -PRON- count be equally devoted to GPE1 just as count GPE1 ii have for -PRON- lawless rapacity be drive out of GPE1 by PERSON1 so -PRON- PERSON1 iii be expel by PERSON1 son to PERSON1 have be disgust by GPE1 be coarse violence and would not help -PRON- but after the old king be death ORG1 use -PRON- influence in the council to conduct the whole power of GPE1 to GPE1 where PERSON1 be defeat and tread to death in the battle of PERSON1 in on the count be death PERSON1 succeed -PRON- as count of GPE1 in right of -PRON- wife and thus be lay the foundation of the powerful and wealthy ORG1 of PERSON1 which for CARDINAL1 generation almost overshadow the crown of GPE1 insanity of PERSON1 the constable PERSON1 be much hate by the duke of GPE1 and an attack which be make on -PRON- in the street of GPE1 be clearly trace to GPE1 the young king who be much attach to PERSON1 set forth to exact punishment on -PRON- way a madman rush out of a forest and call out king -PRON- be betray PERSON1 be much frightened and further seem to have have a sunstroke for -PRON- at once become insane -PRON- recover for a time but at DATE1 while -PRON- and CARDINAL1 other be dance disguise as wild man -PRON- garment of pitch flax catch fire CARDINAL1 be burn and the shock bring back the king be madness -PRON- become subject to fit of insanity of long or short duration and in -PRON- interval -PRON- seem to have be almost imbecile no provision have then be make for the contingency of a mad king the condition of the country become bad than ever and power be grasp at by whoever could obtain -PRON- of the king be CARDINAL1 uncle the duke of GPE1 and -PRON- son be generally engross by a vain struggle to obtain GPE1 the duke of GPE1 be dull and weak and the chief struggle for influence be between ORG1 and -PRON- son PERSON1 on the CARDINAL1 hand and on the other the king be wife PERSON1 of GPE1 and -PRON- brother PERSON1 of GPE1 who be suspect of be -PRON- lover while the unhappy king and -PRON- little child be leave in a wretched state often scarcely provide with clothe or food NORP1 and armagnacs matter grow bad after the death of duke PERSON1 in and in just after a seeming reconciliation the duke of GPE1 be murder in the street of GPE1 by servant of PERSON1 of GPE1 have be a vain foolish man heedless of all save -PRON- own pleasure but -PRON- death increase the misery of GPE1 through the long and deadly struggle for vengeance that follow the king be helpless and the child of the duke of GPE1 be young but -PRON- cause be take up by a PERSON1 noble PERSON1 count of armagnac whose name the party take the duke of PERSON1 be always popular in GPE1 where the people lead by the guild of butcher be so devoted to -PRON- that -PRON- venture to have a sermon preach at the university justify the murder there be again a feeble attempt at reform make by the burgher but as before the more violent and lawless be guilty of such excess that the opposite party be call in to put -PRON- down the armagnacs be admit into GPE1 and take a terrible vengeance on the butcher and on all adherent of PERSON1 in the name of GPE1 the king be eld son a weak dissipate youth who be entirely lead by the count of armagnac invasion of PERSON1 all this time the war with GPE1 have smoulder on only break by brief truce and when GPE1 be in this wretched state PERSON1 renew the claim of PERSON1 and in land before PERSON1 after delay till -PRON- have take the city the PERSON1 call together the whole nobility of the GPE1 and advance against PERSON1 who like PERSON1 have be oblige to leave GPE1 and DATE1 towards ORG1 in search of supply the army meet at GPE1 where though the NORP1 greatly outnumber the NORP1 the skill of PERSON1 and the folly and confusion of the PERSON1 be army lead to a total defeat and the captivity of CARDINAL1 the chief man in GPE1 of ORG1 among -PRON- the young duke of GPE1 -PRON- be PERSON1 be policy to treat GPE1 not as a conquest but as an inheritance and -PRON- therefore refuse to let these captive be ransom till -PRON- should have reduce the country to obedience while -PRON- treat all the place that submit to -PRON- with great kindness the duke of PERSON1 hold aloof from the contest and the armagnacs who rule in GPE1 be too weak or too careless to send aid to GPE1 which be take by PERSON1 after a long siege GPE1 die in -PRON- next brother PERSON1 who be more inclined to PERSON1 do not survive -PRON- DATE1 and the ORDINAL1 brother PERSON1 a mere boy be in the hand of the armagnac in -PRON- reckless misuse of power provoke the citizen of GPE1 into let in the NORP1 when an unspeakably horrible massacre take place PERSON1 -PRON- be kill -PRON- naked corpse score with -PRON- ORG1 be drag about the street and man woman and even infant of -PRON- party be slaughter pitilessly ORG1 CARDINAL1 of -PRON- partisan carry off the PERSON1 but the queen weary of armagnac insolence have join ORG1 ORG1 meanwhile PERSON1 continue to advance and PERSON1 feel the need of join the whole strength of GPE1 against -PRON- and make overture to ORG1 either fear to be overshadow by -PRON- power or else in revenge for GPE1 and armagnac no sooner see that a reconciliation be likely to take place than -PRON- murder PERSON1 before the PERSON1 be eye at a conference on the bridge of PERSON1 be wind be say to be the hole which let the NORP1 into GPE1 -PRON- son PERSON1 the new duke of PERSON1 view the PERSON1 as guilty of -PRON- death go over with all -PRON- force to PERSON1 take with -PRON- the queen and the poor helpless king at the treaty of ORG1 in PERSON1 be declare regent and heir of the GPE1 at the same time as -PRON- receive the hand of PERSON1 daughter of PERSON1 this give -PRON- GPE1 and all the chief city in northern GPE1 but the armagnacs hold the south with GPE1 at -PRON- head PERSON1 be declare an outlaw by -PRON- father be court but -PRON- be in truth the leader of what have become the national and patriotic cause during this time after a long struggle and schism the pope again return to GPE1 the maid of GPE1 when PERSON1 in and the unhappy PERSON1 DATE1 the infant PERSON1 be proclaim king of GPE1 as well as of GPE1 at both GPE1 and GPE1 while PERSON1 be only proclaim at GPE1 and a few other place in the south PERSON1 be of a slow sluggish nature and the man around -PRON- be selfish and pleasure love intriguer who keep aloof all the bold spirit from -PRON- the brother of PERSON1 of GPE1 rule all the country north of the GPE1 with GPE1 as -PRON- head quarter for DATE1 little be do but in -PRON- cause GPE1 to be besiege the city hold out bravely all GPE1 look on anxiously and a young peasant girl name PERSON1 believe -PRON- call by voice from the saint to rescue the city and lead the king to -PRON- coronation at GPE1 with difficulty -PRON- obtain a hearing of the king and be allow to proceed to GPE1 lead the army with a consecrated sword which -PRON- never stain with blood -PRON- fill the NORP1 with confidence the NORP1 with fear as of a witch and thus -PRON- gain DATE1 wherever -PRON- appear GPE1 be save and -PRON- then conduct PERSON1 to GPE1 and stand beside -PRON- throne when -PRON- be crown then -PRON- say -PRON- work be do and would have return home but though the wretched king and -PRON- court never appreciate -PRON- -PRON- think -PRON- useful with the soldier and would not let -PRON- leave -PRON- -PRON- have lose -PRON- heart and hope and the man begin to be anger at -PRON- for put down all vice and foul language the captain be envious of -PRON- and at last when -PRON- have lead a PERSON1 out of the besiege town of ORG1 the gate be shut and -PRON- be make prisoner by a NORP1 PERSON1 of GPE1 the NORP1 hate -PRON- even more than the NORP1 the inquisitor be of -PRON- party and a court be hold at GPE1 which condemn -PRON- to die as a witch GPE1 consent but leave the city before the execution -PRON- own king make no effort to save -PRON- though DATE1 -PRON- cause enquiry to be make establish -PRON- innocence ennoble -PRON- family and free -PRON- village from taxation recovery of GPE1 but though PERSON1 be go -PRON- work last the constable PERSON1 of GPE1 the count of NORP1 and other brave leader continue to attack the NORP1 after DATE1 vengeance for -PRON- father be death the duke of PERSON1 make -PRON- peace with PERSON1 by a treaty at arras on condition of pay no more homage in GPE1 die soon after and there be nothing but dispute among ORG1 open -PRON- gate to the king and PERSON1 almost in spite of -PRON- be restore an able merchant name PERSON1 lend -PRON- money which equip -PRON- man for the recovery of GPE1 and -PRON- -PRON- wake into activity take GPE1 and the other city on LOC1 of GPE1 by these success PERSON1 have recover all save ORG1 that PERSON1 have take from GPE1 but -PRON- be now able to do more the CARDINAL1 province of the south which the NORP1 king have never be able to win be PERSON1 the duchy on LOC1 have be a part of PERSON1 be inheritance and pass through -PRON- to the NORP1 king but though -PRON- have lose all else the hatred of -PRON- inhabitant to the NORP1 enable -PRON- to retain this and PERSON1 have never yet pass under NORP1 rule -PRON- be wrest however from PERSON1 be descendant in this flood tide of conquest GPE1 hold out as long as -PRON- could but PERSON1 could send no aid and -PRON- be force to yield DATE1 brave old lord PERSON1 lead man to recover the duchy and be gladly welcome but -PRON- be slay in the battle of castillon fight like a lion -PRON- CARDINAL1 son fall beside -PRON- and -PRON- army be break GPE1 again surrender and the NORP1 king at last find -PRON- master of the great fief of LOC1 be at the close of DATE1 war the only possession leave to LOC1 of the channel the stand army as at DATE1 act in DATE1 war the great difficulty in time of peace be the presence of the band of free companion or mercenary soldier who when war and plunder fail -PRON- live by violence and robbery of the peasant PERSON1 who have awaken into vigour thereupon take into regular pay all who would submit to discipline and the rest be lead off on CARDINAL1 futile expedition into GPE1 and GPE1 and there leave to -PRON- fate the prince and noble be at ORDINAL1 so much disgusted at the regulation which bind the soldiery to respect the magistracy that -PRON- raise a rebellion which be foster by GPE1 who be ready to do anything that could annoy -PRON- father but -PRON- be soon detach from -PRON- the duke of PERSON1 would not assist -PRON- and the league fall to piece PERSON1 by thus retain company of hire troop in -PRON- pay lay the foundation of the ORDINAL1 stand army in LOC1 and enable the monarchy to tread down the feudal force of the noble -PRON- government be firm and wise and with -PRON- reign begin well time for GPE1 but -PRON- be long before -PRON- recover from the misery of the long strife the war have keep back much of progress there have be grievous havoc of building in the north and centre of GPE1 much lawlessness and cruelty prevail and yet there be a certain advance in learning and much love of romance and the theory of chivalry page of noble birth be breed up in castle to be ORDINAL1 squire and then knights there be immense formality and stateliness the order of precedence be TIME1 and pomp and display be wonderful strange alternation take place DATE1 the street of GPE1 would be a scene of horrible famine where hungry dog and even wolf put an end to the misery of starve homeless child of slaughter parent another the people would be gaze at royal banquet last a whole day with allegorical subtlety of jelly on the table and pageant come between the course where all the virtue harangue in turn or where knight deliver maiden from giant and salvage man in the south there be less misery and more progress PERSON1 be ORG1 at GPE1 be still a marvel of household architecture and PERSON1 of GPE1 and count of PERSON1 be an excellent painter on glass and also a poet ch the struggle with ORG1 all the trouble of GPE1 for DATE1 have go to increase the strength of the duke of PERSON1 the county and duchy of which ORG1 be the capital lie in the most fertile district of GPE1 and have as -PRON- have see be confer on PERSON1 the bold -PRON- marriage have give to -PRON- GPE1 with a gallant nobility and with the chief manufacturing city of LOC1 PERSON1 be son PERSON1 have marry a lady who ultimately bring into the family the great imperial county of GPE1 and GPE1 and -PRON- son duke PERSON1 the good by purchase or inheritance obtain possession of all the adjoining little fief form the country call the GPE1 some belong to the empire some to GPE1 PERSON1 have turn the scale in the struggle between GPE1 and GPE1 and as -PRON- reward have win the city on the LOC1 -PRON- have thus become the rich and most powerful prince in LOC1 and seem on the point of found a middle state lying between GPE1 and GPE1 -PRON- weak point be that the imperial fief in PERSON1 and PERSON1 lie between -PRON- dukedom of PERSON1 and -PRON- county in the GPE1 no NORP1 court equal in splendour that of PERSON1 the great city of GPE1 GPE1 and the rest though full of fierce and resolute man pay -PRON- due enough to make -PRON- the rich of prince and the NORP1 knight be among the bold in LOC1 all the art of life above all painting and domestic architecture nourish at GPE1 and nowhere be troop so well equipped burgher more prosperous learn more widespread than in -PRON- domain here too be the most ceremonious courtesy the most splendid banquet and the most wonderful display of jewel plate and cloth of gold PERSON1 a clever though a cold hearted indolent man let PERSON1 alone already see how the game would go for the future for when the PERSON1 have quarrel with the reign favourite and be kindly receive on -PRON- flight to PERSON1 the old king sneer say that the duke be foster the ORG1 who would steal -PRON- chicken PERSON1 be PERSON1 succeed -PRON- father PERSON1 in -PRON- be a man of great skill and craft with an iron will and subtle though pitiless nature who know in what the greatness of a king consist and work out -PRON- end mercilessly and unscrupulously the old feudal duke and count have all pass away except the duke of GPE1 but the duke of ORG1 and GPE1 hold princely appanage and there be a turbulent nobility who have grow up during the war foreign and civil and be encourage by the favouritism of PERSON1 all these feeling that GPE1 be -PRON- natural foe ORG1 against -PRON- in what be call the league of the public good with -PRON- own brother the duke of GPE1 and count PERSON1 who be know as PERSON1 bold the son of duke ORG1 at -PRON- head GPE1 be actually defeat by PERSON1 in the battle of GPE1 but -PRON- contrive so cleverly to break up the league by promise to each member and by sow dissension among -PRON- that -PRON- end by become more powerful than before PERSON1 bold on the death of PERSON1 the good in PERSON1 bold succeed to the duchy of PERSON1 -PRON- pursue more ardently the plan of form a GPE1 of PERSON1 and have even hope of be choose emperor ORDINAL1 however -PRON- have to consolidate -PRON- dominion by make -PRON- master of the country which part PERSON1 from the GPE1 with this view -PRON- obtain PERSON1 in pledge from -PRON- owner a needy son of ORG1 who be never likely to redeem -PRON- PERSON1 have be inherit by PERSON1 the wife of PERSON1 of GPE1 and titular king of GPE1 and have pass from -PRON- to -PRON- daughter who have marry the near heir in the male line the count of GPE1 but PERSON1 bold unjustly seize the dukedom drive out the lawful heir GPE1 son of PERSON1 meantime be on the watch for every error of PERSON1 and constantly sow danger in -PRON- path sometimes -PRON- mine explode too soon as when -PRON- have actually put -PRON- into PERSON1 be power by visit -PRON- at PERSON1 at the very moment when -PRON- emissary have encourage the city of PERSON1 to rise in revolt against -PRON- bishop an ally of the duke and -PRON- only buy -PRON- freedom by profuse promise and by aid PERSON1 in a most savage destruction of PERSON1 but after this -PRON- caution prevail -PRON- give secret support to the adherent of GPE1 and intrigue with the NORP1 who be often at issue with the NORP1 bailiff and soldiery in PERSON1 greedy reckless man from whom the man of PERSON1 revolt in favour of -PRON- former NORP1 lord meantime PERSON1 of PERSON1 be brother in law have plan with -PRON- an invasion of GPE1 and division of the GPE1 and in actually cross the sea with a splendid host but while PERSON1 be prevent from join -PRON- by the siege of GPE1 a city in alliance with PERSON1 of GPE1 meet PERSON1 on the bridge of pecquigny and by cajolery bribery and accusation of PERSON1 contrive to persuade -PRON- to carry home -PRON- army without strike a blow that meeting be a curious CARDINAL1 a wooden barrier like a wild beast be cage be erect in the middle of the bridge through which the CARDINAL1 king kiss CARDINAL1 another PERSON1 be the tall and handsome man present and splendidly attire GPE1 be small and mean look and clothe in an old blue suit with a hat decorate with little leaden image of the saint but -PRON- smooth tongue quite overcome the dull intellect of PERSON1 and in the mean time the NORP1 soldier be feast and allow -PRON- full swing the NORP1 be strictly watch to prevent all quarrel so skilfully do GPE1 manage that PERSON1 consent to make peace and return home the fall of PERSON1 bold PERSON1 have become entangle in many difficulty -PRON- be a harsh stern man much dislike and -PRON- governor in PERSON1 be fierce violent man who use every pretext for prey upon traveller the governor of PERSON1 have be put to death in a popular rise aid by ORG1 in and the man of PERSON1 -PRON- raise part of the sum for which the country have be pledge and revolt against ORG1 be incite by GPE1 to join -PRON- WORK_OF_ART1 make common cause with -PRON- in CARDINAL1 great battles granson and PERSON1 and all -PRON- chivalry be beat by the NORP1 pikeman but -PRON- push on the war PERSON1 the chief city of PERSON1 have rise against -PRON- and -PRON- besiege -PRON- on TIME1 lead the NORP1 to relieve the town by fall in TIME1 on the besieger camp there be a terrible fight the NORP1 be route and after long search the corpse of duke PERSON1 be find in a frozen pool strip plunder and cover with blood -PRON- be the last of the male line of PERSON1 and -PRON- great possession break up with -PRON- death -PRON- only child PERSON1 do not inherit the NORP1 dukedom nor the county though most of the fief in the low country which could descend to the female line be -PRON- undisputed portion GPE1 try by stir up -PRON- subject to force -PRON- into a marriage with -PRON- son PERSON1 but -PRON- throw -PRON- on the protection of ORG1 and marry PERSON1 son of the emperor PERSON1 carry -PRON- border land to swell the power of -PRON- family GPE1 be home government GPE1 be system of repression of the noble go on all this time -PRON- counsellor be of low birth PERSON1 -PRON- PERSON1 be the man -PRON- most trust -PRON- habit frugal -PRON- manner reserve and ironical -PRON- be dread hate and distrust and -PRON- become constantly more bitter suspicious and merciless those who fall under -PRON- displeasure be imprison in iron cage or put to death and the more turbulent family such as the ORG1 of armagnac be treat with frightful severity but -PRON- be not wanton violence -PRON- act on a regular system of depress the lawless nobility and increase ORG1 by bring the power of the city forward by trust for protection to the stand army chiefly of hire ORG1 and NORP1 and by save money by this mean -PRON- be able to purchase the county of LOC1 and GPE1 from the king of GPE1 thus make the GPE1 -PRON- frontier and on several occasion -PRON- make -PRON- ORG1 fight -PRON- battle instead of the sword of -PRON- knight -PRON- live in the castle of FAC1 guard by the utmost art of fortification and fill with hire NORP1 archer of -PRON- guard whom -PRON- prefer as defender to -PRON- own noble -PRON- be exceedingly unpopular with -PRON- noble but the statesman and historian PERSON1 who have go over to -PRON- from PERSON1 view -PRON- as the good and able of king -PRON- do much to promote trade and manufacture improve the city foster the university and be in truth the ORDINAL1 king since PERSON1 who have any real sense of statesmanship but though the burgher throve under -PRON- and the lawless noble be depress the state of the peasant be not improve feudal right press heavily on -PRON- and -PRON- be little well than savage grind down by PERSON1 impose by -PRON- ORG1 and GPE1 have add much to the NORP1 monarchy -PRON- have win back ORG1 -PRON- have seize the duchy and county of PERSON1 -PRON- have buy LOC1 -PRON- last acquisition be GPE1 the ORDINAL1 angevin family begin with GPE1 the son of PERSON1 have never succeed in gain a footing in GPE1 though -PRON- bear the royal title -PRON- hold however the imperial fief of PERSON1 and PERSON1 whose mother have be of this family obtain from -PRON- CARDINAL1 brother PERSON1 and PERSON1 that PERSON1 should be bequeath to -PRON- instead of pass to PERSON1 be grandson the duke of PERSON1 the king of GPE1 be thenceforth count of PERSON1 and though the county be not view as part of the GPE1 -PRON- be practically CARDINAL1 with -PRON- a yet great acquisition be make soon after GPE1 be death in the great NORP1 duchy of GPE1 fall to a female PERSON1 and the address of GPE1 be daughter the lady of PERSON1 who be regent of the realm prevail to secure the hand of the heiress for -PRON- brother PERSON1 thus the crown of GPE1 have by purchase conquest or inheritance obtain all the great feudal state that make up the country between the NORP1 channel and the GPE1 but each still remain a separate state with different law and custom and a separate ORG1 in each to register law and to act as a court of justice ch the NORP1 war campaign of PERSON1 from grasp at province after province on -PRON- own border however the NORP1 king be now to turn to wide dream of conquest abroad together with GPE1 have buy from king PERSON1 all the claim of the ORG1 of GPE1 among these be include a claim to GPE1 be son PERSON1 a vain and shallow lad be tempt by the possession of large treasure and a fine army to listen to the persuasion of an NORP1 intriguer PERSON1 of GPE1 and put forward these pretension thus begin a war which last nearly as long as DATE1 war with GPE1 but -PRON- be a war of aggression instead of a war of self defence PERSON1 cross the LOC1 in march the whole length of GPE1 without opposition and be crown at GPE1 while -PRON- royal family an illegitimate offshoot from the king of GPE1 flee into GPE1 and call on GPE1 for help but the insolent exaction of the NORP1 soldiery cause the people to rise against -PRON- and when PERSON1 return -PRON- be beset at ORG1 by a great league of NORP1 over whom -PRON- gain a complete victory small and puny though -PRON- be -PRON- fight like a lion and seem quite inspire by the ardour of combat the NORP1 fury la PERSON1 become a proverb among the NORP1 PERSON1 neglect however to send any supply or reinforcement to the garrison -PRON- have leave behind -PRON- in GPE1 and -PRON- all perish under want sickness and the sword of the NORP1 -PRON- be meditate another expedition when -PRON- strike -PRON- head against the top of a doorway and die in campaign of PERSON1 cousin PERSON1 marry -PRON- widow and thus prevent GPE1 from again part from the crown GPE1 not only succeed to the angevin right to GPE1 but through -PRON- grandmother -PRON- view -PRON- as heir of GPE1 -PRON- be PERSON1 wife to that duke of GPE1 who have be murder by PERSON1 -PRON- never advance far than to GPE1 whose surrender make -PRON- master of GPE1 which -PRON- hold for the great part of -PRON- reign but after a while the NORP1 king PERSON1 agree with -PRON- to throw over the cause of the unfortunate royal family of GPE1 and divide that GPE1 between -PRON- PERSON1 send a brilliant army to take possession of -PRON- share but the bound of each portion have not be define and the NORP1 and NORP1 troop begin a war even while -PRON- king be still treat with CARDINAL1 another the individual NORP1 knight do brilliant exploit for indeed -PRON- be the time of the chief blossom of fanciful chivalry a knight of PERSON1 name PERSON1 call the fearless and stainless knight and honour by friend and foe but the NORP1 be under PERSON1 call the great captain and after the battle of cerignola and the garigliano drive the NORP1 out of GPE1 though the war continue in GPE1 the holy league -PRON- be an age of league the NORP1 hate NORP1 and NORP1 both alike be continually form combination among -PRON- and with foreign power against whichever happen to be the strong the chief of these be call the holy league because -PRON- be form by pope PERSON1 who draw into -PRON- PERSON1 then head of the NORP1 empire PERSON1 of GPE1 and PERSON1 of GPE1 the NORP1 troop be attack in GPE1 and though -PRON- gain the battle of GPE1 in -PRON- be with the loss of -PRON- general PERSON1 of PERSON1 whose death serve as an excuse to PERSON1 of GPE1 for set up a claim to the GPE1 of PERSON1 -PRON- cunningly persuade PERSON1 to aid -PRON- in the attack by hold out the vain idea of go on to regain GPE1 and while CARDINAL1 troop of NORP1 be attack PERSON1 -PRON- land at ORG1 and take PERSON1 and PERSON1 the NORP1 force be at the same time be chase out of GPE1 however when PERSON1 have be take and the NORP1 finally drive out of GPE1 the pope and king who have gain -PRON- end leave PERSON1 to fight -PRON- own battle -PRON- thus be induce to make peace give -PRON- young sister PERSON1 as ORDINAL1 wife to GPE1 but that king over exert -PRON- at the banquet and die DATE1 after the marriage in during this reign the waste of blood and treasure on war of mere ambition be frightful and the country have be heavily tax but a brilliant soldiery have be train up and national vanity have much increase the king though without deserve much love be so kindly in manner that -PRON- be a favourite and be call the father of the people -PRON- ORDINAL1 wife PERSON1 be an excellent and high spirited woman who keep the court of GPE1 in a well state than ever before or since campaign of PERSON1 leave CARDINAL1 daughter the elder of whom PERSON1 carry GPE1 to -PRON- male heir PERSON1 of angoul ine PERSON1 have be much averse to the match but GPE1 say -PRON- keep -PRON- mouse for -PRON- own cat and give -PRON- daughter and -PRON- duchy to PERSON1 as soon as PERSON1 be dead PERSON1 be CARDINAL1 of the vain false and most dashing of NORP1 in fact -PRON- be an exaggeration in every way of the national character and thus become a national hero much overpraise -PRON- at once resolve to recover GPE1 and after cross the LOC1 encounter an army of NORP1 troop who have be hire to defend the NORP1 duchy on the field of PERSON1 have to fight a desperate battle with -PRON- after which -PRON- cause PERSON1 to dub -PRON- knight though NORP1 king be say to be bear knight in gain the victory over these mercenary who have be hitherto deem invincible -PRON- open for -PRON- a way into GPE1 and have all GPE1 at -PRON- foot the pope PERSON1 meet -PRON- at bologna and a concordat take place by which the NORP1 church become more entirely subject to the pope while in return all patronage be give up to the crown the effect be soon see in the increase corruption of the clergy and people PERSON1 bring home from this expedition much taste for NORP1 art and literature and all matter of elegance and ornament make great progress from this time the great NORP1 master work for -PRON- PERSON1 paint some of -PRON- most beautiful picture for -PRON- and PERSON1 come to -PRON- court and there die in -PRON- arm -PRON- palace especially that of PERSON1 be exceedingly beautiful in the new classic style call the renaissance great richness and splendour reign at court and set off -PRON- pretension to romance and chivalry learning and scholarship especially classical increase much and the king be sister PERSON1 of PERSON1 be an excellent and highly cultivate woman but even -PRON- writing prove that the whole tone of feeling be terribly coarse when not vicious PERSON1 the conquest of GPE1 make GPE1 the great power in ORG1 but -PRON- king be soon to find a mighty and active rival the old hatred between GPE1 and PERSON1 again awake PERSON1 the daughter of PERSON1 bold have marry PERSON1 of GPE1 and king of the NORP1 though never actually crown emperor -PRON- son PERSON1 marry PERSON1 the daughter of PERSON1 and heiress of GPE1 who lose -PRON- sense from grief on PERSON1 be untimely death and thus the direct heir to GPE1 and the GPE1 be PERSON1 -PRON- eld son on the death of PERSON1 in PERSON1 propose -PRON- to the elector as emperor but fail in spite of bribery PERSON1 be choose and from that time PERSON1 pursue -PRON- with unceasing hatred the claim to GPE1 and GPE1 be renew PERSON1 send troop to occupy GPE1 and be follow -PRON- -PRON- but the most powerful of all -PRON- noble the duke of bourbon constable of GPE1 have be alienate by an injustice perpetrate on -PRON- in favour of the king be mother and desert to the NORP1 offer to assist -PRON- and the NORP1 in divide GPE1 while -PRON- reserve for -PRON- PERSON1 -PRON- desertion hinder PERSON1 from send support to the troop in GPE1 who be force to PERSON1 be shoot in the spine while defend the rear guard and be leave to die under a tree the utmost honour be show -PRON- by the NORP1 but when bourbon come near -PRON- -PRON- bid -PRON- take pity not on CARDINAL1 who be die as a true soldier but on -PRON- as a traitor to king and country when the NORP1 in invade PERSON1 suffer a terrible defeat at FAC1 and be carry a prisoner to GPE1 where -PRON- remain for DATE1 and be only set free on make a treaty by which -PRON- be to give up all claim in GPE1 both to GPE1 and GPE1 also the county of PERSON1 and the suzerainty of those NORP1 county which have be fief of the NORP1 crown as well as to surrender -PRON- CARDINAL1 son as hostage for the performance of the condition war of PERSON1 and PERSON1 all the rest of the king be life be an attempt to elude or break these condition against which -PRON- have protest in -PRON- prison but when there be no NORP1 present to hear -PRON- do so the county of PERSON1 refuse to be transfer and the pope PERSON1 hate the NORP1 power in GPE1 contrive a fresh league against PERSON1 in which PERSON1 join but be justly reward by the miserable loss of another army -PRON- mother and PERSON1 be aunt meet at PERSON1 and conclude in what be call the lady peace which bear as hardly on GPE1 as the peace of GPE1 except that PERSON1 give up -PRON- claim to PERSON1 still PERSON1 be plan be not at an end -PRON- marry -PRON- ORDINAL1 son PERSON1 the only legitimate child of the great florentine ORG1 of ORG1 and try to induce PERSON1 to set up an NORP1 dukedom of GPE1 for the young pair but when the PERSON1 die and PERSON1 become heir of ORG1 would not give -PRON- any footing in PERSON1 never let any occasion pass of harass the emperor but be always defeat PERSON1 once actually invade PERSON1 but be force to retreat through the devastation of the country before -PRON- by PERSON1 afterwards constable of PERSON1 by loud complaint and by talk much of -PRON- honour contrived to make the world fancy -PRON- the injured man while -PRON- be really break oath in a shameless manner at last in the king and emperor meet at ORG1 and come to term PERSON1 marry as -PRON- ORDINAL1 wife PERSON1 be PERSON1 and in when PERSON1 be in haste to quell a revolt in the low country -PRON- ask a safe conduct through GPE1 and be splendidly entertain at GPE1 yet so low be the honour of the NORP1 that PERSON1 scarcely withstand the temptation of extort the duchy of GPE1 from -PRON- when in -PRON- power and give so many broad hint that PERSON1 be glad to be past the frontier the war be soon renew PERSON1 set up a claim to ORG1 as the key of GPE1 ORG1 -PRON- with the NORP1 and moors and slave take by -PRON- on the coast of GPE1 and GPE1 be actually bring into GPE1 nice be burn but the citadel hold out and as PERSON1 have ORG1 -PRON- with the emperor and have take PERSON1 make a final peace at ORG1 in -PRON- die DATE1 in PERSON1 -PRON- only survive son PERSON1 follow the same policy the rise of protestantism be now divide the empire in GPE1 and PERSON1 take advantage of the strife which break out between PERSON1 and the NORP1 prince to attack the emperor and make conquest across the NORP1 border -PRON- call -PRON- protector of the liberty of the NORP1 and league -PRON- with -PRON- seize PERSON1 which the duke of guise bravely defend when the emperor try to retake -PRON- this seizure of PERSON1 be the ORDINAL1 attempt of GPE1 to make conquest in GPE1 and the beginning of a contest between the NORP1 and NORP1 people which have go on to DATE1 after the siege a DATE1 truce be make during which PERSON1 resign -PRON- crown -PRON- brother have be already elect to the empire but -PRON- son PERSON1 become king of GPE1 and GPE1 and also inherit the low country the pope PERSON1 who be a neapolitan and hate the NORP1 rule incite PERSON1 a vain weak man to break the truce and send CARDINAL1 army to GPE1 under the duke of guise while another attack the frontier of ORG1 assist by the force of -PRON- wife PERSON1 meet this last attack with an army command by the duke of ORG1 -PRON- advance into GPE1 and besiege PERSON1 the NORP1 under the constable of PERSON1 come to relieve the city and be utterly defeat the constable -PRON- be make prisoner -PRON- nephew the admiral PERSON1 hold out PERSON1 to the last and thus give the country time to rally against the invader and guise be recall in haste from GPE1 -PRON- soon after surprised ORG1 which be thus restore to the NORP1 after have be hold by the NORP1 for DATE1 this be the only conquest the NORP1 retain when the final peace of PERSON1 be make in DATE1 for all else that have be take on either side be then restore ORG1 be give back to -PRON- duke together with the hand of PERSON1 be sister PERSON1 during a tournament hold in honour of the wedding PERSON1 be mortally injure by the splinter of a lance in and in the home trouble that follow all pretension to NORP1 power be drop by GPE1 after war which have last DATE1 ch the war of religion the bourbon and guises PERSON1 have leave CARDINAL1 son the eld of whom PERSON1 be DATE1 and the country be divide by CARDINAL1 great faction CARDINAL1 head by the guise family an offshoot of the ORG1 of PERSON1 the other by the bourbon who be descend in a direct male line from a young son of GPE1 be the next heir to the throne in case the ORG1 of PERSON1 should become extinct PERSON1 the head of the bourbon family be call king of PERSON1 because of -PRON- marriage with PERSON1 t the queen in -PRON- own right of this pyrenean GPE1 which be in fact entirely in the hand of the NORP1 so that -PRON- only actual possession consist of the little NORP1 county of foix and b PERSON1 -PRON- be dull and indolent but -PRON- wife be a woman of much ability and -PRON- brother PERSON1 of ORG1 be full of spirit and fire and little inclined to brook the ascendancy which the duke of guise and -PRON- brother enjoy at court partly in consequence of -PRON- exploit at ORG1 and partly from be uncle to the young queen PERSON1 of GPE1 wife of PERSON1 the bourbon likewise head the party among the noble who hope to profit by the king be youth to recover the privilege of which -PRON- have be gradually deprive while the ORG1 of guise be ready to maintain the power of the crown as long as that mean -PRON- own power the reformation the enmity of these CARDINAL1 party be much increase by the reaction against the prevalent doctrine and the corruption of the clergy this reaction have begin in the reign of PERSON1 when the WORK_OF_ART1 have be translate into NORP1 by CARDINAL1 student at ORG1 and the king be sister PERSON1 of PERSON1 have encourage the reformer PERSON1 have league with the NORP1 NORP1 because -PRON- be foe to the emperor while -PRON- persecute the like opinion at home to satisfy the pope PERSON1 a native of GPE1 the foremost NORP1 reformer be invite to the free city of GPE1 and there be make chief pastor while the scheme of theology call -PRON- institute become the text book of the reformed in GPE1 and GPE1 -PRON- doctrine be harsh and stern aiming at the utmost simplicity of worship and denounce the exist practice so fiercely that the people who hold -PRON- to have be wilfully lead astray by -PRON- clergy commit such violence in the church that the NORP1 loudly call for punishment on -PRON- the shameful life of many of the clergy and the wickedness of the court have cause a strong reaction against -PRON- and great number of both noble and burgher become NORP1 -PRON- term -PRON- NORP1 or reformer but -PRON- nickname be ORG1 probably from the NORP1 eidgenossen or oath comrades PERSON1 like -PRON- father protect NORP1 NORP1 and persecute NORP1 NORP1 but the lawyer of the ORG1 of GPE1 interpose declare that man ought not to be burn for heresy until a council of the church should have condemn -PRON- opinion and -PRON- be in the midst of this dispute that PERSON1 be slay the conspiracy of LOC1 the guise family be strong NORP1 the bourbon be the head of ORG1 chiefly from policy but admiral PERSON1 and -PRON- brother the PERSON1 d andelot be sincere and earnest reformer a ORDINAL1 party head by the old constable PERSON1 be NORP1 in faith but not unwilling to join with the ORG1 in pull down the guise and assert the power of the nobility a conspiracy for seize the person of the king and destroy the guise at the castle of LOC1 be detect in time to make -PRON- fruitless the CARDINAL1 bourbon prince keep in the background though ORG1 be universally know to have be the true head and mover in -PRON- and -PRON- be actually bring to trial the discovery only strengthen the hand of guise PERSON1 of PERSON1 even then however PERSON1 be die and -PRON- brother PERSON1 who succeed -PRON- in be but DATE1 the PERSON1 pass to -PRON- mother the florentine PERSON1 a wily cat like woman who have always hitherto be keep in the background and whose chief desire be to keep thing quiet by play off CARDINAL1 party against the other -PRON- at once release ORG1 and favour the bourbon and the ORG1 to keep down the guise even permit conference to see whether the NORP1 church could be reform so as to satisfy the NORP1 proposal be send by guise be brother the cardinal of PERSON1 to the council then sit at GPE1 for vernacular service the marriage of the clergy and other alteration which may win back the reformer but an attack by the follower of guise on a meeting of NORP1 at GPE1 of whose ringing of bell -PRON- mother have complain lead to the ORDINAL1 bloodshed and the outbreak of a civil war the religious war to trace each stage of the war would be impossible within these limit -PRON- be a war often lull for a short time and often break out again and in which the actor grow more and more cruel the reform influence be in the south the NORP1 in the LOC1 most of the provincial city at ORDINAL1 hold with the bourbon for the sake of civil and religious freedom though the guise family succeed to the popularity of the NORP1 duke in GPE1 still PERSON1 persuade PERSON1 of bourbon to return to court just as -PRON- wife PERSON1 of PERSON1 have become a staunch NORP1 and while dream of exchange -PRON- claim on PERSON1 for GPE1 -PRON- be kill on the NORP1 side while besiege GPE1 at the ORDINAL1 outbreak the ORG1 seem to have by far the great influence an endeavour be make to seize the king be person and this lead to a battle at dreux while -PRON- be doubtful PERSON1 actually declare -PRON- shall have to say -PRON- prayer in NORP1 guise however retrieve DATE1 and though PERSON1 be make prisoner on the CARDINAL1 side ORG1 be take on the other GPE1 be the GPE1 rally place and while besiege -PRON- guise -PRON- be assassinate -PRON- death be believe by -PRON- family to be due to the admiral PERSON1 the city of GPE1 fortify by PERSON1 of PERSON1 become the stronghold of the ORG1 leader after leader fall PERSON1 on the CARDINAL1 hand be kill at PERSON1 on the other be shoot in cold blood after the fight of PERSON1 a truce follow but be soon break again and in PERSON1 be the only man of age and stand at the head of ORG1 while the NORP1 have as leader PERSON1 of FAC1 be brother and PERSON1 of guise both young man of little more than twenty the ORG1 have be beat at all point but be still strong enough to have wring from -PRON- enemy permission to hold meeting for public worship within unwalled town and on the estate of such noble as hold with -PRON- PERSON1 be PERSON1 make use of the suspension of arm to try to detach the GPE1 leader by entangle -PRON- in the pleasure of the court and lower -PRON- sense of duty the court be studiously brilliant PERSON1 surround -PRON- with a bevy of lady call the queen mother be squadron whose amusement be find for the whole day the lady sit at -PRON- tapestry frame while NORP1 poetry and romance be read or love song sing by the gentleman -PRON- have garden game and hunt party with every opening for the lady to act as siren to any whom the queen wish to detach from the principle of honour and virtue and bind to -PRON- service ball pageant and theatrical follow in the evening and there be hardly a prince or noble in GPE1 who be not carry away by these seduction into dark habit of profligacy PERSON1 of PERSON1 dread -PRON- for -PRON- son PERSON1 whom -PRON- keep as long as possible under training in religion learning and hardy habit in the mountain of b arn and when PERSON1 try to draw -PRON- to court by propose a marriage between -PRON- and -PRON- young daughter PERSON1 leave -PRON- at home and go -PRON- to court PERSON1 try in vain to bend -PRON- will or discover -PRON- secret and -PRON- death early in while still at court be attribute to the queen mother massacre of PERSON1 be son PERSON1 be immediately summon to conclude the marriage and come attend by all the most distinguished ORG1 though the more wary of -PRON- remain at home and the baron of PERSON1 say if that wedding take place the favour will be crimson the duke of guise seem to have resolve on take this opportunity of revenge -PRON- for -PRON- father be murder but the queen mother be undecided until -PRON- find that -PRON- son PERSON1 who have be bid to cajole and talk over the GPE1 chief have be attract by -PRON- honesty and uprightness and be ready to throw -PRON- into -PRON- hand and escape from -PRON- an abortive attempt on guise be part to murder the admiral PERSON1 lead to all the ORG1 go about armed and make demonstration which alarm both the queen and the people of GPE1 guise and the duke of GPE1 be therefore allow to work -PRON- will and to rouse the bloodthirstiness of the GPE1 mob at TIME1 of the th of DATE1 PERSON1 be TIME1 the bell of the church of FAC1 begin to ring and the slaughter be begin by man distinguish by a white sleeve the king shelter -PRON- GPE1 surgeon and nurse in -PRON- room the young king of PERSON1 and prince of ORG1 be threaten into conform to the church but every other GPE1 who could be find be massacre from PERSON1 who be slay kneel in -PRON- bedroom by the follower of guise down to the poor and young and the street resound with the cry kill kill in every city where royal troop and guisard partisan have be live among ORG1 the same hideous work take place for DATE1 spare neither age nor sex how many CARDINAL1 die -PRON- be impossible to reckon but the work be so wholesale that none be leave except those in the southern city where the ORG1 have be too strong to be attack and in those castle where the seigneur be of the religion ORG1 think the destruction complete the court go in state to return thank for deliverance from a suppose plot while PERSON1 be body be hang on a gibbet the pope order public thanksgiving while PERSON1 put on mourning and the emperor PERSON1 alone among NORP1 prince show any horror or indignation but the heart of the unhappy young king be break by the guilt -PRON- have incur PERSON1 sink into a decline and die in find no comfort save in the surgeon and nurse -PRON- have save the league -PRON- brother PERSON1 who have be elect king of GPE1 throw up that crown in favour of that of GPE1 -PRON- be of a vain false weak character superstitiously devout and at the same time ferocious so as to alienate every CARDINAL1 all be ashamed of a man who dress in the extreme of foppery with a rosary of death be head at -PRON- girdle and pass from wild dissipation to abject penance -PRON- be call the GPE1 church warden and the queen be hairdresser for -PRON- pass from -PRON- toilette to the decoration of the wall of church with illumination cut out of old service book sometimes -PRON- go about surround with little dog sometimes flog -PRON- walk barefoot in a procession and -PRON- CARDINAL1 or favourite be the scandal of the country by -PRON- pride license and savage deed the war break out again and -PRON- only remain brother PERSON1 of PERSON1 on an equally hateful and contemptible being flee from court to ORG1 hope to force -PRON- brother into buy -PRON- submission but when the king of PERSON1 have follow -PRON- and begin the struggle in earnest -PRON- accept the duchy of GPE1 and return to -PRON- allegiance PERSON1 be invite by the insurgent NORP1 to become -PRON- chief and spend some time in GPE1 but return unsuccessful and die as the king be childless the next male heir be PERSON1 of PERSON1 who have flee from court soon after PERSON1 on return to the GPE1 faith and be reign in -PRON- CARDINAL1 county of b arn and foix the head of the ORG1 in the resolve never to permit a heretic to wear the NORP1 crown guise and -PRON- party form ORG1 to force PERSON1 to choose another successor GPE1 be devote to guise and the king find -PRON- almost a prisoner there leave the city but be again master by the duke at PERSON1 and could so PERSON1 -PRON- arrogance as to have recourse to assassination -PRON- cause -PRON- to be slay at the palace at PERSON1 in the fury of the league be so great that PERSON1 be drive to take refuge with the king of PERSON1 and -PRON- be together besiege GPE1 when PERSON1 be in -PRON- turn murder by a monk name PERSON1 in PERSON1 the leaguer proclaim as king an old uncle of the king of WORK_OF_ART1 rally round PERSON1 of PERSON1 who take the title of PERSON1 at GPE1 in GPE1 meet the force of leaguer and defeat -PRON- by -PRON- brilliant courage follow -PRON- white plume -PRON- last order to -PRON- troop become CARDINAL1 of the saying the NORP1 love to remember but -PRON- cause be still not win GPE1 hold out against -PRON- animate by almost fanatical fury and while -PRON- be besiege -PRON- GPE1 be invade from the GPE1 the old cardinal of bourbon be now dead and PERSON1 consider -PRON- daughter PERSON1 whose mother be the eld daughter of PERSON1 to be rightful queen of GPE1 -PRON- send therefore -PRON- able general the duke of PERSON1 to co operate with the leaguer and place -PRON- on the throne a war of strategy be carry on during which PERSON1 keep the enemy at bay but could do no more since the large number of -PRON- people though intend to have no king but -PRON- do not wish -PRON- to gain too easy a victory lest in that case -PRON- should remain a NORP1 however -PRON- be only wait to recant till -PRON- could do so with a good grace -PRON- really prefer NORP1 and have only be a political GPE1 and -PRON- good and most faithful adviser the baron of PERSON1 well know as duke of PERSON1 though a staunch NORP1 -PRON- recommend the change as the only mean of restore peace to the GPE1 there be little more resistance to PERSON1 after -PRON- have again be receive by the church in GPE1 weary of EVENT1 open -PRON- gate in and the inhabitant crowd round -PRON- with ecstasy so that -PRON- say poor people -PRON- be hungry for the sight of a king the leaguer make -PRON- peace and when PERSON1 of GPE1 again attack PERSON1 of guise be CARDINAL1 of the ORDINAL1 to hasten to the defence PERSON1 see that there be no further hope for -PRON- daughter and peace be make in the edict of GPE1 DATE1 in PERSON1 put forth what be call the edict of GPE1 because ORDINAL1 register in that ORG1 -PRON- secure to the ORG1 equal civil right with those of the NORP1 accept -PRON- marriage give -PRON- under restriction permission to meet for worship and for consultation and grant -PRON- city for the security of -PRON- right of which ORG1 be the chief the NORP1 have be nearly exterminate in the north but there be still a large number in the south of GPE1 and the burgher of the chief southern city be mostly GPE1 the war have be from the ORDINAL1 a very horrible CARDINAL1 there have be savage slaughter and still more savage reprisal on each side the young noble have be train into make a fashion of ferocity and practise graceful way of strike death blow whole district have be lay waste church and abbey destroy tomb rifle and the whole population accustom to every sort of horror and suffering while nobody but PERSON1 -PRON- and the duke of PERSON1 have any notion either of statesmanship or of religious toleration PERSON1 be plan just as the reign of PERSON1 have be a period of rest and recovery from EVENT1 so that of PERSON1 be CARDINAL1 of restoration from the ravage of DATE1 of intermittent civil war the king -PRON- not only have bright and engaging manner but be a man of large heart and mind and PERSON1 do much for the welfare of the country road canal bridge postal communication manufacture extend commerce all owe -PRON- promotion to -PRON- and bring prosperity to the burgher class and the king be especially endear to the peasantry by -PRON- say that -PRON- hope for the time when no cottage would be without a good fowl in -PRON- pot the great silk manufactory of southern GPE1 chiefly arise under -PRON- encouragement and there be prosperity of every kind the church -PRON- be in a far well state than before some of the good man of any time be then live in PERSON1 who do much to improve the training of the parochial clergy and who found the order of sister of charity who prevent the misery of the street of GPE1 from ever be so frightful as in DATE1 when desert child become the prey of wolf dog and pig the noble who have grow into insolence during the war either as favourite of PERSON1 or as zealous supporter of the GPE1 cause be subdue and tame the most note of these be the duke of PERSON1 the owner of the small principality of sedan who be reduce to obedience by the sight of PERSON1 be formidable train of artillery and the marshal duke of PERSON1 who think that PERSON1 have not sufficiently reward -PRON- service intrigue with GPE1 and ORG1 and be behead for -PRON- treason hatred to ORG1 in GPE1 and GPE1 be as keen as ever in GPE1 and in PERSON1 be prepare for another war on the plea of a dispute succession to the duchy of PERSON1 the old fanaticism still linger in GPE1 and PERSON1 have be advise to beware of pageant there but -PRON- be necessary that -PRON- ORDINAL1 wife PERSON1 should be crown before -PRON- go to the war as -PRON- be to be leave regent DATE1 after the coronation as PERSON1 be go to the arsenal to visit -PRON- old friend PERSON1 -PRON- be stab to the heart in -PRON- coach in the street of GPE1 by a fanatic name GPE1 the NORP1 call -PRON- FAC1 and -PRON- be CARDINAL1 of the most attractive and benevolent of man win the heart of all who approach -PRON- but the immorality of -PRON- life do much to confirm the already low standard that prevail among prince and noble in GPE1 the states general of PERSON1 be ORDINAL1 wife PERSON1 become regent for -PRON- PERSON1 xiii be only DATE1 and indeed -PRON- character be so weak that -PRON- whole reign be only CARDINAL1 long minority PERSON1 be entirely under the dominion of an NORP1 favourite name concini and -PRON- wife and -PRON- whole endeavour be to amass rich for -PRON- and keep the young king in helpless ignorance while -PRON- undo all that PERSON1 have effect and take bribe shamelessly the prince of ORG1 try to overthrow -PRON- and in hope of strengthen -PRON- in PERSON1 summon together the state general there come member for the noble for the clergy and for the ORDINAL1 estate i e the burgher and these be mostly lawyer and magistrate from the province be resolve to make -PRON- voice hear taxation be grow bad and bad not only be -PRON- confine to the burgher and peasant class exempt the clergy and the noble among which last be include -PRON- family to the remote generation but -PRON- have become the court custom to multiply office in order to pension the noble and keep -PRON- quiet and this together with the expense of the army make the weight of taxation ruinous moreover the presentation to the civil office hold by lawyer be make hereditary in -PRON- family on payment of a sum down and of fee at the death of each holder all these abuse be complain of and CARDINAL1 of the deputy even tell the nobility that if -PRON- do not learn to treat the despised class below -PRON- as young brother -PRON- would lay up a terrible store of retribution for -PRON- a petition to the king be draw up and be receive but never answer the door of ORG1 be close the member be tell -PRON- be by order of the king and the state general never meet again for DATE1 when the storm be just ready to fall the siege of GPE1 the rottenness of the state be chiefly owe to the nobility who as long as -PRON- be allow to grind down -PRON- peasant and shine at court have no sense of duty or public spirit and hate the burgher and lawyer far too much to make common cause with -PRON- against the constantly increase power of the throne -PRON- only intrigue and struggle for personal advantage and rivalry and never think of the good of the state -PRON- bitterly hate concini the marshal d ancre as -PRON- have be create but -PRON- remain in power till when CARDINAL1 of the king be gentleman PERSON1 plot with the king -PRON- and a few of -PRON- guard for -PRON- deliverance nothing could be easy than the execution the king order the captain of the guard to arrest concini and kill -PRON- if -PRON- resist and this be do concini be cut down on the step of the louvre and GPE1 exclaim at last i be a king but -PRON- be not in -PRON- to be a king and -PRON- never be CARDINAL1 all -PRON- life -PRON- only pass under the dominion of PERSON1 who be a high spirited young noble the ORG1 have be hold assembly which be consider more political than religious and -PRON- town of security be a grievance to royalty war break out again and GPE1 -PRON- go with PERSON1 to besiege montauban the place be take but disease break out in the army and PERSON1 die there be a fresh struggle for power between the queen mother and the prince of ORG1 ending in both be set aside by the queen be PERSON1 bishop of PERSON1 on and afterwards a cardinal the able statesman then in LOC1 who gain complete dominion over the king and country and rule -PRON- both with a rod of iron the ORG1 be gradually drive out of all -PRON- stronghold till only GPE1 remain to -PRON- this city be bravely and patiently defend by the magistrate and the duke of PERSON1 with hope of succour from GPE1 until these be disconcert by the murder of the duke of PERSON1 -PRON- be force to surrender after have hold out for DATE1 PERSON1 enter in triumph deprive the city of all -PRON- privilege and thus in conclude the war that have begin by the attack of the guisard on the congregation at GPE1 in the life and property of the ORG1 be still secure but all favour be close against -PRON- and every encouragement hold out to -PRON- to join the church many of the bad scandal have be remove and the clergy be much improve and from whatever motive -PRON- may be many of the more influential ORG1 begin to conform to the state religion ch the power of the crown ORG1 be administration cardinal PERSON1 be whole idea of statesmanship consist in make PERSON1 the great of prince at home and abroad to make anything great of PERSON1 who be feeble alike in mind and body be beyond any CARDINAL1 be power and ORG1 keep -PRON- in absolute subjection allow -PRON- a favourite with whom to hunt talk and amuse -PRON- but if the friend attempt to rouse the king to shake off the yoke crush -PRON- ruthlessly -PRON- be the crown rather than the king that the cardinal exalt put down whatever resist PERSON1 of GPE1 the king be only brother make a futile struggle for power and freedom of choice in marriage but be soon overcome -PRON- be spare as be the only heir to the GPE1 but the duke of PERSON1 who have be lead into -PRON- rebellion be bring to the block amid the pity and terror of all GPE1 whoever seem dangerous to the state or show any spirit of independence be mark by the cardinal and suffer a hopeless imprisonment if nothing bad but at the same time -PRON- government be intelligent and able and promote prosperity as far as be possible where there be such a crushing of individual spirit and enterprise ORG1 be plan in fact be to find a despotism though a wise and well order despotism at home while -PRON- make GPE1 great by conquest abroad and at this time the ambition of GPE1 find a favourable field in the state both of GPE1 and of GPE1 the war in GPE1 and GPE1 DATE1 war have be rage in GPE1 for DATE1 and GPE1 have take no part in -PRON- beyond encourage the NORP1 and NORP1 as the enemy of the emperor but the policy of ORG1 require that the disunion between -PRON- NORP1 and NORP1 state should be maintain and when thing begin to tend towards peace from mutual exhaustion the cardinal interfere and induce ORG1 to continue the war by give -PRON- money and reinforcement a war have already begin in GPE1 on behalf of the duke of nevers who have become heir to the duchy of GPE1 but whose family have live in GPE1 so long that the emperor and the king of GPE1 support a more distant claim of the duke of ORG1 to part of the duchy rather than admit a NORP1 prince into GPE1 ORG1 be quick to seize this pretext for attack GPE1 for GPE1 be now die into a weak power and -PRON- see in the war a means of acquire the GPE1 which belong to the NORP1 crown at ORDINAL1 nothing important be do but the NORP1 and NORP1 be wear out while CARDINAL1 young and able captain be grow up among the NORP1 the viscount of PERSON1 young son to the duke of PERSON1 and the duke of PERSON1 eld son of the prince of ORG1 and ORG1 be policy soon secure a brilliant career of success PERSON1 and ORG1 all fall into the hand of the NORP1 and from a chamber of sickness the cardinal direct the affair of CARDINAL1 army as well as make -PRON- fear and respect by the whole GPE1 cinq mars the last favourite -PRON- have give the king plot -PRON- overthrow with the help of the NORP1 but be detect and execute when the great minister be already at death be door ORG1 recommend an NORP1 priest PERSON1 whom -PRON- have train to work under -PRON- to carry on the government and die in the DATE1 of the king only survive -PRON- DATE1 die on the th of may the war be continue on the line ORG1 have lay down and DATE1 after the death of PERSON1 the army in the low country gain a splendid victory at GPE1 under the duke of PERSON1 entirely destroy the old NORP1 infantry the battle of GPE1 nordlingen and lens raise the fame of the NORP1 general to the high pitch and in reduce the emperor to make peace in the treaty of m nster GPE1 obtain as -PRON- spoil the CARDINAL1 bishoprics PERSON1 and FAC1 city in GPE1 and the sundgau with the savoyard town of GPE1 but the war with GPE1 continue till when PERSON1 engage to marry PERSON1 a daughter of the king of GPE1 the PERSON1 when an heir have long be despair of PERSON1 the wife of PERSON1 have become the mother of CARDINAL1 son the eld of whom PERSON1 be DATE1 at the time of -PRON- father be death the queen mother become regent and trust entirely to PERSON1 who have become a cardinal and pursue the policy of ORG1 but what have be endure from a man by birth a NORP1 noble be intolerable from a low bear NORP1 after the lion come the ORG1 be the saying and the ORG1 of GPE1 make a last stand by refuse to register the royal edict for fresh taxis be support both by the burgher of GPE1 and by a great number of the nobility who be personally jealous of PERSON1 this party be call the PERSON1 because in -PRON- discussion each man stand forth launch -PRON- speech and retreat just as the boy do with sling PERSON1 and stone in the street the struggle become serious but only a few of the lawyer in the ORG1 have any real principle or public spirit all the other actor cabal out of jealousy and party spirit make tool of the man of the gown whom -PRON- hate and despise though mostly far -PRON- superior in worth and intelligence PERSON1 hold fast by PERSON1 and be support by the duke of PERSON1 whom -PRON- father be death have make prince of ORG1 ORG1 be assistance enable -PRON- to blockade GPE1 and bring the ORG1 to term which conclude the ORDINAL1 act of the PERSON1 with the banishment of PERSON1 as a peace offering ORG1 however become so arrogant and overbearing that the queen cause -PRON- to be imprison whereupon -PRON- wife and -PRON- other friend begin a fresh war for -PRON- liberation and the queen be force to yield but -PRON- again show -PRON- so tyrannical that the queen and the ORG1 become reconcile and ORG1 to put -PRON- down give the command of the troop to PERSON1 again there be a battle at the gate of GPE1 in which all ORG1 be friend be wound and -PRON- -PRON- so entirely worst that -PRON- have to go into exile when -PRON- enter the NORP1 service while PERSON1 return to power at home the court of PERSON1 the court of GPE1 though never pure be much improve during the reign of PERSON1 and the PERSON1 of PERSON1 there be a spirit of romance and grace about -PRON- somewhat cumbrous and stately but outwardly pure and refined and quite a step out of the gross and open vice of the former reign the duchess PERSON1 a lady of great grace and wit make -PRON- ORG1 the centre of a brilliant society which set -PRON- to raise and refine the manner literature and language of the time no word that be consider vulgar or coarse be allow to pass muster and though in process of time this censorship become pedantic and petty there be no doubt that much be do to purify both the language and the tone of thought poem play epigram PERSON1 and even sermon be rehearse before the committee of taste in the h tel PERSON1 and a wonderful new stimulus be there give not only to ornamental but to solid literature many of the great man who make GPE1 illustrious be either end or begin -PRON- career at this time memoir writing specially flourish and the character of the man and woman of the court be know to -PRON- on all side cardinal PERSON1 and the duke of PERSON1 both deeply engaged in the PERSON1 have leave the CARDINAL1 memoir the other maxim of great power of irony PERSON1 CARDINAL1 of the queen be lady write a full history of the court PERSON1 CARDINAL1 of the great genius of all time be attach -PRON- to the jansenist this religious party so call from PERSON1 a NORP1 priest whose opinion be impute to -PRON- have spring up around the reform convent of port royal and number among -PRON- some of the able and good man of the time but the NORP1 consider -PRON- to hold false doctrine and there be a continual debate end at length in the persecution of the PERSON1 be provincial letter expose the jesuit system be among the able writing of the age philosophy poetry science history art be all make great progress though there be a stateliness and formality in all that be say and do redolent of the NORP1 queen be etiquette and the fastidious refinement of ORG1 of PERSON1 the attempt from the early time of the NORP1 monarchy have be to draw all government into the hand of the sovereign and the suppression of the PERSON1 complete the work PERSON1 though ill educate be a man of considerable ability much industry and great force of character arise from a profound belief that GPE1 be the ORDINAL1 country in the world and -PRON- the ORDINAL1 of NORP1 and -PRON- have a magnificent courtesy of demeanour which so impress all who come near -PRON- as to make -PRON- -PRON- willing slave there be enough in -PRON- to make CARDINAL1 king and CARDINAL1 respectable man besides be what PERSON1 say of -PRON- and when in the cardinal die the king show -PRON- fully equal to become -PRON- own prime minister the state be -PRON- -PRON- say and all centre upon -PRON- so that no room be leave for statesman the court be however in a most brilliant state there have be an unusual outburst of talent of every kind in the lull after the war of religion and in general thinker artist and man of literature GPE1 be unusually rich the king have a wonderful power of self assertion which attach -PRON- all to -PRON- almost as if -PRON- be a sort of divinity the stately elaborate NORP1 etiquette bring in by -PRON- mother PERSON1 become absolutely an engine of government PERSON1 have begin the evil custom of keep the noble quiet by give -PRON- situation at court with pension attach and these office be multiply to the most enormous and absurd degree so that every royal personage have some CARDINAL1 of personal attendant prince of the blood and noble of every degree be contented to hang about the court crowd into the most narrow lodging at GPE1 and throng -PRON- anteroom and to be order to remain in the country be a most severe punishment GPE1 under PERSON1 there be in fact nothing but the chase to occupy a gentleman on -PRON- own estate for -PRON- be allow no duty or responsibility each province have a governor or intendant a sort of viceroy and the administration of the city be manage chiefly on the part of the king even the mayor obtain -PRON- post by purchase the unhappy peasant have to pay in the ORDINAL1 place the taxis to government out of which be defray an intolerable number of pension many for useless office next the rent and due which support -PRON- lord be expenditure at court and thirdly the tithe and fee of the clergy besides which -PRON- be call off from the cultivation of -PRON- own field for a certain number of DATE1 to work at the road -PRON- horse may be use by royal messenger -PRON- lord be crop have to be get in by -PRON- labour gratis while -PRON- own be spoil and in short the only wonder be how -PRON- exist at all -PRON- hovel and -PRON- food be wretched and any attempt to amend -PRON- condition on the part of -PRON- lord would have be look on as betoken dangerous design and probably have land -PRON- in the GPE1 the peasant of GPE1 where the old constitution have be less entirely ruin and those of GPE1 be in a less oppressed condition and in the city trade flourish PERSON1 the comptroller general of the finance be so excellent a manager that the pressure of taxation be endurable in -PRON- time and -PRON- promote new manufacture such as glass at ORG1 at FAC1 at PERSON1 -PRON- also try to promote commerce and colonization and to create a ORG1 there be a great appearance of prosperity and in every department there be wonderful ability the reformation have lead to a considerable revival among the NORP1 NORP1 -PRON- the theological college establish in the last reign have much improve the tone of the clergy bossuet bishop of PERSON1 be CARDINAL1 of the most note preacher who ever exist and f n lon archbishop of PERSON1 CARDINAL1 of the good of man a reform of discipline begin in the convent of port royal end by attract and gather together some of the most excellent and able person in GPE1 among -PRON- PERSON1 a man of marvellous genius and depth of thought and PERSON1 the chief NORP1 dramatic poet -PRON- chief director PERSON1 of PERSON1 be however a pupil of PERSON1 a NORP1 ecclesiastic whose view on abstruse question of grace be condemn by the NORP1 and as the port royalist would not disown the doctrine attribute to -PRON- -PRON- be discourage and persecute throughout GPE1 be reign more because -PRON- be jealous of what would not bend to -PRON- will than for any real want of conformity PERSON1 be famous provincial letter be put forth during this controversy and in fact the literature of GPE1 reach -PRON- augustan age during this reign and the language acquire -PRON- standard perfection war in the low country PERSON1 the queen of PERSON1 be the child of the ORDINAL1 marriage of PERSON1 of GPE1 and on -PRON- father be death in GPE1 on pretext of an old law in GPE1 which give the daughter of a ORDINAL1 marriage the preference over the son of a ORDINAL1 claim the low country from the young PERSON1 of GPE1 -PRON- thus begin a war which be really a continuance of the old struggle between GPE1 and PERSON1 and of the endeavour of GPE1 to stretch -PRON- frontier to the LOC1 at ORDINAL1 GPE1 and GPE1 against -PRON- and oblige -PRON- to make the peace of ORG1 in but -PRON- then succeed in bribe PERSON1 to forsake the cause of the NORP1 and the war be renew in PERSON1 of GPE1 be most determined enemy through life keep up the spirit of the NORP1 and -PRON- obtain aid from GPE1 and GPE1 through DATE1 terrible war in which the great PERSON1 be kill at saltzbach in GPE1 at last from exhaustion all party be compel to conclude the peace of nimeguen in take advantage of undefined term in this GPE1 seize various city belong to NORP1 prince and likewise the free imperial city of GPE1 when all GPE1 be too much wear out by EVENT1 to offer resistance GPE1 be full of self glorification the king be view almost as a GPE1 and the splendour of -PRON- court and of -PRON- building especially the palace at GPE1 with -PRON- garden and fountain keep up the delusion of -PRON- greatness revocation of the edict of GPE1 in GPE1 suppose that the ORG1 have be so reduce in number that the edict of GPE1 could be repeal all freedom of worship be deny -PRON- -PRON- minister be banish but -PRON- flock be not allow to follow -PRON- if take while try to escape man be send to the galley woman to captivity and child to convent for education dragoon be quarter on family to torment -PRON- into go to mass a few make head in the wild moor of the cevenne under a brave youth name cavalier and other endure severe persecution in the south of GPE1 dragoon be quarter on -PRON- who make -PRON- -PRON- business to torment and insult -PRON- -PRON- marriage be declare invalid -PRON- child take from -PRON- to be educate in the NORP1 NORP1 faith a great number amount to at least succeed in escape chiefly to GPE1 and GPE1 whither -PRON- carry many of the manufacture that PERSON1 have take so much pain to establish many of those who settle in GPE1 be silk weaver and a large colony be thus establish at GPE1 which long keep up -PRON- NORP1 character the war of the palatinate this brutal act of tyranny be follow by a fresh attack on GPE1 on the plea of a suppose inheritance of -PRON- sister in law the duchess of GPE1 invade the palatinate on the LOC1 and carry on CARDINAL1 of the most ferocious war in history while -PRON- be at the same time support the cause of -PRON- cousin PERSON1 of GPE1 after -PRON- have flee and abdicate on the arrival of PERSON1 during this war however that generation of able man who have grow up with GPE1 begin to pass away and -PRON- success be not so uniform while PERSON1 be dead taxation begin to be more feel by the exhausted people and peace be make at PERSON1 in the war of the succession in GPE1 the last of the CARDINAL1 great war of GPE1 be reign be far more unfortunate PERSON1 of GPE1 die childless naming as -PRON- successor a NORP1 prince PERSON1 of GPE1 the ORDINAL1 son of the only son of PERSON1 be eld sister the queen of PERSON1 but the power of LOC1 at the peace of PERSON1 have agree that the crown of GPE1 should go to PERSON1 of GPE1 ORDINAL1 son of the emperor PERSON1 who be the descendant of young sister of the royal NORP1 line but do not excite the fear and jealousy of LOC1 as do a scion of the already overweening ORG1 of bourbon this lead to the war of the NORP1 succession GPE1 and GPE1 support PERSON1 and fight with GPE1 in GPE1 ORG1 and the low country in GPE1 be ultimately successful and -PRON- grandson PERSON1 retain the throne but the troop which -PRON- ally the elector of GPE1 introduce into GPE1 be totally overthrow at GPE1 by the NORP1 army under the duke of GPE1 and the NORP1 under prince PERSON1 a son of a young branch of the ORG1 of ORG1 have be breed up in GPE1 but have bitterly offend GPE1 by call -PRON- a stage king for show and a chess king for use have enter the emperor be service and be CARDINAL1 of -PRON- chief enemy -PRON- aid -PRON- cousin duke PERSON1 of ORG1 in repulse the NORP1 attack in DATE1 gain a great victory at GPE1 and advance into ORG1 be likewise in full career of victory in the low country and gain there the battle of LOC1 of ORG1 have outlive -PRON- good fortune -PRON- great general and statesman have pass away the country be exhaust famine be prey on the wretched peasantry supply could not be find and CARDINAL1 city after another of those GPE1 have seize be retake new victory at GPE1 and GPE1 be gain over the NORP1 army and though GPE1 be as resolute and undaunted as ever -PRON- affair be in a desperate state when -PRON- be save by a sudden change of policy on the part of queen PERSON1 of GPE1 who recall -PRON- army and leave -PRON- ally to continue the contest alone PERSON1 be not a match for GPE1 without GPE1 and the archduke PERSON1 have succeed -PRON- brother the emperor give up -PRON- pretension to the crown of GPE1 so that -PRON- become possible to conclude a general peace at GPE1 in by this time GPE1 be DATE1 and have suffer grievous family loss ORDINAL1 by the death of -PRON- only son and then of -PRON- eld grandson a young man of much promise of excellence who with -PRON- wife die of malignant measle probably from ignorant medical treatment since -PRON- infant whose illness be conceal by -PRON- nurse be the only CARDINAL1 of the family who survive the old king in spite of sorrow and reverse toil with indomitable energy to the end of -PRON- reign the long on record have last DATE1 when -PRON- die in -PRON- have raise the NORP1 crown to -PRON- great splendour but have sacrifice the country to -PRON- and -PRON- false notion of greatness the PERSON1 the crown now descend to PERSON1 a weakly child of DATE1 -PRON- great grandfather have try to provide for -PRON- good by leave the chief seat in ORG1 to -PRON- own illegitimate son the duke of GPE1 the most honest and conscientious man then in the family but though clever unwise and very unpopular -PRON- birth cause the appointment to be view as an outrage by the nobility and the king be will be set aside the ORDINAL1 prince of the blood royal PERSON1 of GPE1 the late king be nephew become sole regent a man of good ability but of easy indolent nature and who in the enforce idleness of -PRON- life have become dissipate and vicious beyond all imagination or description -PRON- be kindly and gracious and -PRON- mother say of -PRON- that -PRON- be like the prince in a fable whom all the fairy have endow with gift except CARDINAL1 malignant sprite who have prevent any favour being of use to -PRON- in the general exhaustion produce by the war of PERSON1 a PERSON1 name PERSON1 begin the great system of hollow speculation which have continue ever since to tempt people to -PRON- ruin -PRON- try raise sum of money on national credit and also devise a company who be to lend money to find a great settlement on the LOC1 the return from which be to be enormous every CARDINAL1 speculate in share and the wild excitement prevail law be ORG1 be mob by people seek interview with -PRON- and noble disguise -PRON- in livery to get access to -PRON- fortune be make DATE1 and lose the next and finally the whole plan prove to have be a mere baseless scheme ruin follow and the misery of the country increase the duke of GPE1 die suddenly in the king be now legally of age but -PRON- be dull and backward and little fit for government and the country be really rule by the duke of bourbon and after -PRON- by PERSON1 an aged statesman but fill with the same scheme of ambition as ORG1 or PERSON1 war of the NORP1 succession thus GPE1 plunge into new war PERSON1 marry the daughter of PERSON1 a NORP1 noble who after be raise to the throne be expel by NORP1 intrigue and violence GPE1 be oblige to take up arm on behalf of -PRON- father in law but be buy off by a gift from the emperor PERSON1 of the duchy of PERSON1 to PERSON1 to revert to -PRON- daughter after -PRON- death and thus become ORG1 to GPE1 belong to duke PERSON1 the husband of PERSON1 eld daughter to the emperor and PERSON1 receive instead the duchy of GPE1 while all the chief power in LOC1 agree to the so call pragmatic sanction by which PERSON1 decree that PERSON1 should inherit GPE1 and GPE1 and the other hereditary state on -PRON- father be death to the exclusion of the daughter of -PRON- eld brother PERSON1 when PERSON1 die however in a EVENT1 begin on this matter PERSON1 of GPE1 would neither allow PERSON1 be claim to the hereditary state nor join in elect -PRON- husband to the empire and GPE1 take part against -PRON- send marshal PERSON1 to support the elector of GPE1 who have be choose emperor PERSON1 of GPE1 hold with PERSON1 and gain a victory over the NORP1 at ORG1 in PERSON1 then join -PRON- army and the battle of fontenoy in be CARDINAL1 of the rare victory of GPE1 over GPE1 another victory follow at PERSON1 but elsewhere GPE1 have have heavy loss and in after the death of PERSON1 be make at ORG1 DATE1 dull and selfish by nature have be absolutely lead into vice by -PRON- courtier especially the duke of bourbon who fear -PRON- become active in public affair -PRON- have no sense of duty to -PRON- people and whereas -PRON- great grandfather have seek display and so call glory -PRON- care solely for pleasure and that of the gross and most sensual order so that -PRON- court be a hotbed of shameless vice all that could be wring from the impoverished country be lavish on the overgrown establishment of every member of the royal family in pension to noble and in shameful amusement of the king in another war break out in consequence of the hatred leave between GPE1 and GPE1 by the former struggle PERSON1 have by PERSON1 -PRON- ought to have disdain gain over GPE1 to take part with -PRON- and GPE1 be ORG1 with PERSON1 in this war GPE1 and GPE1 chiefly fight in -PRON- distant possession where the NORP1 be uniformly successful and after DATE1 another peace follow leave the boundary of the NORP1 state just where -PRON- be before after a frightful amount of bloodshed but GPE1 have have terrible loss -PRON- be drive from GPE1 and lose all -PRON- settlement in GPE1 and GPE1 GPE1 under PERSON1 meantime the gross vice and licentiousness of the king be beyond description and the nobility retain about the court by the system establish by PERSON1 be if not -PRON- equal in crime equally callous to the suffering cause by the reckless expensiveness of the court the whole cost of which be defray by the burgher and peasant no taxis be ask from clergy or noble and this latter term include all spring of a noble line to the utmost generation the owner of an estate have no mean of benefit -PRON- tenant even if -PRON- wish -PRON- for all matter even of local government depend on the crown all -PRON- could do be to draw -PRON- income from -PRON- and -PRON- be often force either by poverty or by -PRON- expensive life to strain to the utmost the old feudal system if -PRON- live at court -PRON- expense be heavy and only partly meet by -PRON- pension likewise raise from the taxis pay by the poor farmer if -PRON- live in the country -PRON- be a still great tyrant and be call by the people a LOC1 or kite no career be open to -PRON- young son except in the court the church or the army and here -PRON- monopolize the prize obtain all the rich diocese and abbey and all the promotion in the army the magistracy be almost all hereditary among lawyer who have buy -PRON- for -PRON- family from the crown and pay for the appointment of each son the official attach to each member of the royal family be almost incredible in number and all pay by the taxis the old ORG1 or salt tax have go on ever since EVENT1 and every member of a family have to pay -PRON- not accord to what -PRON- use but what -PRON- be suppose to need every pig be rate at what -PRON- ought to require for salt every cow sheep or hen have a toll to pay to king lord bishop sometimes also to priest and abbey the peasant be call off from -PRON- own work to give the due of labour to the road or to -PRON- lord -PRON- may not spread manure that could interfere with the game nor drive away the partridge that eat -PRON- corn so scanty be -PRON- crop that famine slay CARDINAL1 pass unnoticed and even if by any wonder prosperity smile on the peasant -PRON- durst not live in any kind of comfort lest the steward of -PRON- lord or of government should pounce on -PRON- wealth reaction meantime there be a strong feeling that change must come classical literature be study and NORP1 and NORP1 manner and institution be think ideal perfection there be great disgust at the fetter of a highly artificial life in which every CARDINAL1 be bind and at the institution which have be so misuse writer arise among whom PERSON1 and PERSON1 be the most eminent who aim at the overthrow of all the idea which have come to be thus abuse the CARDINAL1 by -PRON- caustic wit the other by -PRON- enthusiastic simplicity gain willing ear and the writer in a great encyclop dia then in course of publication contrive to attack most of the notion which have be hitherto take for grant and be closely connect with faith and with government the king -PRON- be dully aware that -PRON- be live on the crust of a volcano but -PRON- say -PRON- would last -PRON- time and so -PRON- do PERSON1 die of smallpox in leave -PRON- grandson to reap the harvest that generation have be sow ch the revolution attempt at reform -PRON- be evident that a change must be make PERSON1 -PRON- know -PRON- and slur over the word in -PRON- coronation oath that bind -PRON- to extirpate heresy but -PRON- be a slow dull man and affair have come to such a pass that a far able man than -PRON- could hardly have deal with the dead lock above without cause a frightful outbreak of the pen up masse below -PRON- queen PERSON1 be hate for be of NORP1 birth and though a spotless and noble woman -PRON- most trivial action give occasion to PERSON1 found on the crime of the last generation unfortunately the king though an honest and well intentioned man be totally unfit to guide a country through a dangerous crisis -PRON- courage be passive -PRON- manner be heavy dull and shy and though steadily industrious -PRON- be slow of comprehension and unready in action and reformation be the more difficult because to abolish the useless court office would have be utter starvation to many of -PRON- holder who have nothing but -PRON- pension to live upon yet there be a general passion for reform all rank alike look to some change to free -PRON- from the dead lock which make improvement impossible the government be bankrupt while the taxis be intolerable and DATE1 of the reign be spend in experiment necker a NORP1 banker be invite to take the charge of the finance and large loan be make to government for which -PRON- contrive to pay interest regularly some reduction be make in the expenditure but the king be old minister PERSON1 grow jealous of -PRON- popularity and obtain -PRON- dismissal the NORP1 take the part of the NORP1 colony in -PRON- revolt from GPE1 and the war thus occasion bring on an increase of the load of debt the general distress increase and -PRON- become necessary to devise some mode of taxing which may divide the PERSON1 between the whole nation instead of make the peasant pay all and the noble and clergy nothing GPE1 decide on call together the notable or high nobility but -PRON- be by no means dispose to tax -PRON- and only abuse -PRON- minister -PRON- then resolve on convoke the whole state general of the GPE1 which have never meet since the reign of PERSON1 the states general no CARDINAL1 exactly know the limit of the power of the state general when -PRON- meet in noble clergy and the deputy who represent the commonalty all form the assembly at GPE1 and though the king would have keep apart these last who be call the tier et t or ORDINAL1 estate -PRON- refuse to withdraw from FAC1 of GPE1 the count of PERSON1 the young son of a noble family who sit as a deputy declare that nothing short of bayonet should drive out those who sit by the will of the people and GPE1 yield thenceforth the vote of a noble a bishop or a deputy all count alike the party name of NORP1 for those who want to exalt the power of the people and of aristocrat for those who maintain the privilege of the noble come into use and the most extreme NORP1 be call jacobin from an old convent of NORP1 friar where -PRON- use to meet the mob of GPE1 always eager fickle and often blood thirsty be excited to the last degree by the debate and full of the remembrance of the insolence and cruelty of the noble sometimes rise and hunt down person whom -PRON- deem aristocrat hang -PRON- to the iron rod by which lamp be suspend over the street the king in alarm draw the army near and -PRON- be suppose that -PRON- be go to prevent all change by force of arm thereupon the citizen enrol -PRON- as a national guard wear cockade of red blue and white and command by ORG1 a noble of NORP1 opinion who have run away at CARDINAL1 to serve in EVENT1 on a report that the cannon of the GPE1 have be point upon GPE1 the mob rise in a frenzy rush upon -PRON- hang the guard and absolutely tear down the old castle to -PRON- foundation though -PRON- do not find a single prisoner in -PRON- this be a revolt say GPE1 when -PRON- hear of -PRON- sire -PRON- be a revolution be the answer LAW1 the mob have find out -PRON- power the fishwoman of the market always a peculiar and privileged class be frantically excite and be sure to be foremost in all the demonstration stir up by jacobin there be a great scarcity of provision in GPE1 and this together with the continual dread that reform would be check by violence madden the people on a report that the guard have show enthusiasm for the king the whole populace come pour out of GPE1 to GPE1 and after threaten the life of the queen bring the family back with -PRON- to GPE1 and keep -PRON- almost as prisoner while the assembly which follow -PRON- to GPE1 debate on LAW1 the noble be view as the bad enemy of the nation and all over the country there be rising of the peasant head by NORP1 from the town who sack -PRON- castle and often seize -PRON- person many flee to GPE1 and GPE1 and the dread that these would unite and return to bring back the old system continually increase the fury of the people the assembly now know as the constituent assembly sweep away all title and privilege and no CARDINAL1 be henceforth to bear any prefix to -PRON- name but citizen while at the same time the clergy be to renounce all the property of the church and to swear that -PRON- office and commission be derive from the will of the people alone and that -PRON- owe no obedience save to the state the estate thus yield up be suppose to be enough to supply all state expense without taxis but as -PRON- could not at once be turn into money promissory note or PERSON1 be issue but as coin be scarce these be not worth nearly -PRON- profess value and the general distress be thus much increase the other oath the great body of the clergy utterly refuse and -PRON- be therefore drive out of -PRON- benefice and become object of great suspicion to the NORP1 all the old boundary and other distinction between the province be destroy and GPE1 be divide into department each of which be to elect deputy in whose assembly all power be to be vest except that the king retain a right of veto i e of refuse -PRON- sanction to any measure -PRON- swear on the th of DATE1 to observe this new constitution the republic the constituent assembly now dissolve -PRON- and a fresh assembly call the legislative take -PRON- place for a time thing go on more peacefully distrust be however deeply sow the king be closely watch as an enemy and those of the noble who have emigrate begin to form army aid by the NORP1 on the frontier for -PRON- rescue this enrage the people who expect that -PRON- newly win liberty would be overthrow the ORDINAL1 time the king exercise -PRON- right of veto the mob rise in fury and though -PRON- then do no more than threaten on the advance of the emigrant army on the th of DATE1 a more terrible rise take place the NORP1 be sack the guard slaughter the unresisting king and -PRON- family depose and imprison in the tower of the temple in terror lest the noble in the prison should unite with the emigrant -PRON- be massacre by wholesale while with a vigour bear of the excitement the emigrant army be repulse and beat the monarchy come to an end and GPE1 become a republic in which the national convention which follow ORG1 be supreme the more moderate member of this be call PERSON1 from the LOC1 the estuary of the GPE1 from the neighbourhood of which many of -PRON- come -PRON- be able man scholar and philosopher full of scheme for revive classical time but wish to stop short of the plan of the jacobin of whom the chief be PERSON1 a lawyer from ORG1 fill with fanatical notion of the right of man -PRON- with a party of other violent NORP1 call the mountain of whom PERSON1 and PERSON1 be most note set to work to destroy all that interfere with -PRON- plan of general equality the guillotine a recently invent machine for beheading be set in all the chief market place and CARDINAL1 be put to death on the charge of conspire against the nation PERSON1 be execute early in and -PRON- be enough to have any sort of birthright to be think dangerous and put to death the reign of terror horror at the bloodshed perpetrate by the mountain lead a young girl name PERSON1 DATE1 to assassinate PERSON1 whom -PRON- suppose to be the chief cause of the cruelty that be take place but -PRON- death only add to the dread of reaction a committee of public safety be appoint by the convention and endeavour to sweep away every being who either seem adverse to equality or who may inherit any claim to rank the queen be put to death DATE1 after -PRON- husband and the PERSON1 who have begin to try to stem the tide of slaughter soon fall under the denunciation of the more violent to be accuse of conspire against the state be instantly fatal and no CARDINAL1 be life be safe PERSON1 be denounce by PERSON1 and perish and for DATE1 the reign of terror last the emigrant by form an army and advance on GPE1 assist by the force of GPE1 only make matter worse there be such a dread of the old oppression come back that the peasant be ready to fight to the death against the return of the noble the army where promotion use to go by rank instead of merit be so glad of the change that -PRON- be full of fresh spirit and repulse the army of NORP1 and emigrant all along the frontier the city of PERSON1 which have try to resist the change be take and frightfully use by collot d herbois a member of ORG1 the guillotine be too slow for -PRON- and -PRON- have the people mow down with grape shot declare that of this great city nothing should be leave but a monument inscribe PERSON1 resist ORG1 be no more in la vend e a district of GPE1 where the peasant be much attach to -PRON- clergy and noble -PRON- rise and gain such success that -PRON- dream for a little while of rescue and restore the little captive son of PERSON1 but -PRON- be defeat and put down by fire and sword and at GPE1 an immense number of execution take place chiefly by drown -PRON- be reckon that no less than person be guillotine in DATE1 between and besides those who die by other mean everything be change religion be to be do away with the church be close the ORDINAL1 instead of DATE1 appoint for rest death be an eternal sleep be inscribe on the school and reason represent by a classically dressed woman be enthrone in the cathedral of notre d -PRON- at the same time a new era be invent the nd of DATE1 have new name and the decimal measure of length weight and capacity which be base on the proportion of the LOC1 be plan all this time PERSON1 really seem to have think -PRON- the benefactor of the human race but at last the other member of the convention take courage to denounce -PRON- and -PRON- with CARDINAL1 more be arrest and send to the guillotine the bloodthirsty fever be over ORG1 be overthrow and people breathe again the directory the chief executive power be place in the hand of a directory consist of more moderate man and a time of much prosperity set in already in the new vigour bear of the strong emotion of the country the army win great victory not only repel the NORP1 and the emigrant but unite GPE1 to ORG1 a NORP1 officer who be call on to protect the directory from be again overawe by the mob become the lead spirit in GPE1 through -PRON- NORP1 victory -PRON- conquer GPE1 and GPE1 and force the emperor to let -PRON- become republic under NORP1 protection also to resign GPE1 to GPE1 by the treaty of ORG1 then make a descent on GPE1 hope to attack GPE1 from that side but -PRON- be foil by PERSON1 who destroy -PRON- fleet in the battle of the LOC1 and sir PERSON1 who hold out QUANTITY1 against -PRON- -PRON- hurry home to GPE1 on find that the directory have begin a fresh NORP1 war seize GPE1 and force -PRON- to give up -PRON- treasure and become a republic on -PRON- model and carry the pope off into captivity all the NORP1 power have ORG1 against -PRON- and GPE1 have be recover chiefly by NORP1 aid so that PERSON1 on the ground that a nation at war need a less cumbrous government than a directory contrive to get -PRON- choose ORDINAL1 consul with CARDINAL1 inferior in the consulate a great course of victory follow in GPE1 where PERSON1 command in person and in GPE1 under PERSON1 and GPE1 be force to make peace and GPE1 be the only country that still resist -PRON- till a general peace be make at GPE1 in but -PRON- only last for DATE1 for the NORP1 fail to perform the condition and begin the war afresh in the mean time PERSON1 have restore religion and order and so entirely master GPE1 that in -PRON- be able to form the republic into an empire and affect to be another PERSON1 -PRON- cause the pope to say mass at -PRON- coronation though -PRON- put the crown on -PRON- own head a concordat with the pope reinstate the clergy but alter the division of the diocese and put the bishop and priest in the pay of the state the empire ORG1 to this new NORP1 empire cause a fresh war with all LOC1 ORG1 however be defeat at GPE1 and PERSON1 the NORP1 be entirely crush at GPE1 and the NORP1 fight CARDINAL1 terrible but almost draw battle at PERSON1 and PERSON1 peace be then make with all CARDINAL1 at ORG1 in the term press exceedingly hard upon GPE1 scheme of invade GPE1 be entertain by the emperor but be disconcert by the destruction of the NORP1 and NORP1 fleet by PERSON1 at trafalgar GPE1 be then in alliance with GPE1 but ORG1 treacherously get the royal family into -PRON- hand seize -PRON- GPE1 make -PRON- brother PERSON1 -PRON- king but the NORP1 would not submit and call in the NORP1 to -PRON- aid the peninsular war result in a series of victory on the part of the NORP1 under GPE1 while GPE1 begin another war be again so crush that the emperor durst not refuse to give -PRON- daughter in marriage to ORG1 however in the conquest of GPE1 prove an exploit beyond ORG1 be power -PRON- reach GPE1 with -PRON- grand army but the city be burn down immediately after -PRON- arrival and -PRON- have no shelter or mean of support -PRON- be force to retreat through a fearful DATE1 without provision and harass by the cossacks who hang on the rear and cut off the straggler so that -PRON- whole splendid army have become a mere miserable break straggling remnant by the time the survivor reach the NORP1 frontier -PRON- -PRON- have hurry back to GPE1 as soon as -PRON- find -PRON- case hopeless to arrange -PRON- resistance to all LOC1 for every country rise against -PRON- on -PRON- ORDINAL1 disaster and DATE1 be spend in a series of desperate battle in GPE1 between -PRON- and the ORG1 powers PERSON1 and PERSON1 be doubtful but DATE1 battle of leipzic be a terrible defeat in DATE1 CARDINAL1 army those of GPE1 GPE1 GPE1 and GPE1 enter GPE1 at once and though ORG1 resist stand bravely and skilfully and gain single battle against GPE1 and GPE1 -PRON- could not stand against all LOC1 in DATE1 the ally enter GPE1 and -PRON- be force to abdicate be send under a strong guard to the little LOC1 isle of GPE1 -PRON- have drain GPE1 of man by -PRON- constant call for soldier who be draw by conscription from the whole country till there be not enough to do the work in the field and foreign prisoner have to be employ but -PRON- have confer on -PRON- CARDINAL1 great benefit in the great code of law call the code napol on which have ever since continue in force GPE1 under ORG1 the old law and custom vary in different province have be sweep away so that the field be clear and the system of government which ORG1 devise have remain practically unchanged from that time to this everything be make to depend upon the central government the minister of religion of justice of police of education etc have the regulation of all interior affair and appoint all who work under -PRON- so that nobody learn how to act alone and as the government have be in fact ever since dependent on the will of the people of GPE1 the whole country be helplessly in -PRON- hand the army as in almost all foreign nation be raise by conscription that be by draw lot among the young man liable to serve and who can only escape by pay a substitute to serve in -PRON- stead and this be generally the ORDINAL1 object of the saving of a family all feudal claim have be do away with and with -PRON- the right of primogeniture and indeed -PRON- be not possible for a testator to avoid leave -PRON- property to be share among -PRON- family though -PRON- can make some small difference in the amount each receive and thus estate be continually freshly divide and some portion become very small indeed NORP1 peasant be however most eager to own land and be usually very frugal sober and save and the country have go on increase in prosperity and comfort -PRON- be true that probably from the long habit of conceal any wealth -PRON- may possess the NORP1 farmer and peasantry care little for display or what -PRON- should call comfort and live rough hard work live even while well off and with large hoard of wealth but -PRON- condition have be wonderfully change for the well ever since the revolution all this have continue under the numerous change that have take place in the form of government ch GPE1 since the revolution the restoration the ally leave the people of GPE1 free to choose -PRON- government and -PRON- accept the old royal family who be on -PRON- border await a recall the son of PERSON1 have perish in the hand of -PRON- jailer and thus the king be next brother PERSON1 succeed to the throne bring back a large emigrant follow thing be not settle down when ORG1 in the spring of escape from ORG1 welcome -PRON- with delight and GPE1 be force to flee to GPE1 however the ally immediately rise in arm and the troop of GPE1 and GPE1 crush ORG1 entirely at GPE1 on the th of june -PRON- be send to ORG1 GPE1 in the LOC1 whence -PRON- could not again return to trouble the peace of LOC1 there -PRON- die in PERSON1 be restore and a charter be devise by which a limited monarchy be establish a king at the head and CARDINAL1 chamber CARDINAL1 of peer the other of deputy but with a very narrow franchise -PRON- do not however work amiss till after GPE1 be death in -PRON- brother PERSON1 try to fall back on the old system -PRON- check the freedom of the press and interfere with the freedom of election the consequence be a fresh revolution in DATE1 happily with little bloodshed but which force PERSON1 to go into exile with -PRON- grandchild PERSON1 whose father the duke of GPE1 have be assassinate in reign of PERSON1 the chamber of deputy offer the crown to PERSON1 of GPE1 -PRON- be descend from the regent -PRON- father have be CARDINAL1 of ORG1 in the revolution and when title be abolish have call -PRON- PERSON1 equality this have not save -PRON- head under the reign of terror and -PRON- son have be oblige to flee and lead a wander life at CARDINAL1 time gain -PRON- livelihood by teach mathematic at a school in GPE1 -PRON- have recover -PRON- family estate at the restoration and as the head of ORG1 be very popular -PRON- be elect king of the NORP1 not of GPE1 with a chamber of peer nominate for life only and another of deputy elect by voter whose qualification be MONEY1 or eight pound DATE1 -PRON- do -PRON- utmost to gain the good will of the people live a simple friendly family life and try to merit the term of the citizen king and in DATE1 of -PRON- reign -PRON- be successful the country be prosperous and a great colony be settle in GPE1 and endure a long and desperate war with the wild NORP1 tribe a colony be also establish in GPE1 in the LOC1 and attempt be carry out to compensate thus for the loss of colonial possession which GPE1 have sustain in war with GPE1 discontent however begin to arise on the CARDINAL1 hand from those who remember only the success of PERSON1 and not the misery -PRON- have cause and on the other from the work class who declare that the bourgeois or tradespeople have gain everything by the revolution of DATE1 but -PRON- -PRON- nothing PERSON1 do -PRON- good to gratify and amuse the people by send for the remain of ORG1 and give -PRON- a magnificent funeral and splendid monument among -PRON- old soldier the invalides but -PRON- popularity be wane in -PRON- eld son the duke of GPE1 a favourite with the people be kill by a fall from -PRON- carriage and this be another shock to -PRON- throne CARDINAL1 young grandson be leave and the king have also several son CARDINAL1 of whom the duke of PERSON1 -PRON- give in marriage to PERSON1 the sister and heiress presumptive to the queen of GPE1 though by treaty with the other NORP1 power -PRON- have be agree that -PRON- should not marry a NORP1 prince unless the queen have child of -PRON- own ambition for -PRON- family be a great offence to -PRON- subject and at the same time a nobleman the duke PERSON1 who have murder -PRON- wife commit suicide in prison to avoid public execution and the NORP1 declare whether justly or unjustly that this have be allow rather than let a noble die a felon be death the revolution of in spite of the increase prosperity of the country there be general disaffection there be CARDINAL1 party the orleanist who hold by PERSON1 and -PRON- minister PERSON1 and whose badge be the tricolour the NORP1 who retain -PRON- loyalty to the exile PERSON1 and whose symbol be the white bourbon flag the NORP1 and the NORP1 whose badge be the red cap and flag a demand for a franchise that should include the mass of the people be reject and the general displeasure pour -PRON- out in speech at political banquet an attempt to stop CARDINAL1 of these lead to an uproar ORG1 refuse to fire on the people and -PRON- fury rise unchecked so that the king think resistance vain sign an abdication and flee to GPE1 in DATE1 a provisional government be form and a new constitution be to be arrange but the GPE1 mob who find -PRON- condition unchanged and really want equality of wealth not of right make disturbance again and again and barricade the street till -PRON- be finally put down by general PERSON1 while the rest of GPE1 be entirely dependent on the will of the capital after DATE1 a republic be determine on which be to have a president at -PRON- head choose DATE1 by ORG1 nephew to PERSON1 be the ORDINAL1 president thus choose and after some struggle -PRON- not only master GPE1 but by the help of the army which be mostly PERSON1 -PRON- dismiss the chamber of deputy and imprison or exile all the opponent whom the troop have not put to death on the plea of an expect rise of the mob this be call a coup d tat and PERSON1 be then declare president for DATE1 the ORDINAL1 empire in DATE1 the president take the title of emperor call -PRON- PERSON1 as successor to the young son of PERSON1 -PRON- keep up a splendid and expensive court make GPE1 more than ever the toy shop of the world and do much to improve -PRON- by the widening of street and removal of old building treaty be make which much improve trade and the country advance in prosperity the rein of government be however tightly hold and nothing be so much avoid as the let man think or act for -PRON- while -PRON- eye be to be dazzle with splendour and victory in when GPE1 be attack GPE1 the emperor ORG1 with GPE1 in opposition and the CARDINAL1 army together besiege GPE1 and fight the battle of alma and inkermann take the city after DATE1 be siege and then make what be know as the treaty of GPE1 which guarantee the safety of GPE1 so long as the subject NORP1 nation be not misuse in PERSON1 join in an attack on the NORP1 power in GPE1 and together with PERSON1 of GPE1 and the NORP1 gain CARDINAL1 great victory at magenta and solferino but make peace as soon as -PRON- be convenient to -PRON- without regard to -PRON- promise to the king of GPE1 who be oblige to purchase -PRON- consent to become king of GPE1 by yield up to GPE1 -PRON- old inheritance of ORG1 and nice meantime discontent begin to spring up at home and the red NORP1 spirit be work on the huge fortune make by the successful only add to the sense of contrast secret society be at work and the emperor after DATE1 of success feel -PRON- popularity wane the franco NORP1 war in the NORP1 who have depose -PRON- queen PERSON1 ii make choice of a relation of the king of GPE1 as -PRON- king there have long be bitter jealousy between GPE1 and GPE1 and though the prince refuse the offer of GPE1 the NORP1 show such an overbearing spirit that a war break out the real desire of GPE1 be to obtain the much covet frontier of the LOC1 and the emperor heat -PRON- army with boastful proclamation which be but the prelude to direful defeat at ORG1 and forbach at sedan the emperor be force to surrender -PRON- as a prisoner and the tiding no soon arrive at GPE1 than the whole of the people turn -PRON- wrath on -PRON- and -PRON- family -PRON- wife PERSON1 have to flee a republic be declare and the city prepare to stand a siege the NORP1 advance and put down all resistance in other part of GPE1 great part of the army have be make prisoner and though there be much bravado there be little steadiness or courage leave among those who now take up arm GPE1 which be blockade after suffer much from famine surrender in DATE1 and peace be purchase in a treaty by which great part of PERSON1 and PERSON1 and the city of PERSON1 be give back to GPE1 the end '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466,
          "referenced_widgets": [
            "d4b5c156d444445fa505fdf771702829",
            "30761ddd7a814f40bb6ed831324298fb",
            "9a1bcc87de5846e7bab3b8760d2d618c",
            "44a89a41de3d40b184f42ab35eae8519",
            "2f04701b7c314d36832f3deedd4a4b09",
            "7c85275fcb9241c6b3e839c5d12ca0d8",
            "e155aec75ddc4e87a4904696a64f2062",
            "80f2959b267c4e4bbd627252e6f6d7f8"
          ]
        },
        "id": "SzottwAmclwD",
        "outputId": "2bfba162-9f2c-4776-e85c-7d0d6d62ee89"
      },
      "source": [
        "caps = set()\n",
        "for doc in tqdm(documents):\n",
        "    caps |= set(filter(lambda word: not word.islower(), doc.split()))\n",
        "caps"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4b5c156d444445fa505fdf771702829",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=11008.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'-PRON-',\n",
              " 'CARDINAL1',\n",
              " 'DATE1',\n",
              " 'EVENT1',\n",
              " 'FAC1',\n",
              " 'GPE1',\n",
              " 'LANGUAGE1',\n",
              " 'LAW1',\n",
              " 'LOC1',\n",
              " 'MONEY1',\n",
              " 'NORP1',\n",
              " 'ORDINAL1',\n",
              " 'ORG1',\n",
              " 'PERCENT1',\n",
              " 'PERSON1',\n",
              " 'PRODUCT1',\n",
              " 'QUANTITY1',\n",
              " 'TIME1',\n",
              " 'WORK_OF_ART1',\n",
              " '_',\n",
              " '¼',\n",
              " '½',\n",
              " '¾',\n",
              " '⅓'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkc-zAYfoMwH"
      },
      "source": [
        "Эти специальные токены надо обработать (заменить на представителя класса). Они --- результат `named entity recognition` в `spacy` (`en_core_web_lg`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUZBWSM4nxvk"
      },
      "source": [
        "# Автоенкодер"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHuBn7cXpyep"
      },
      "source": [
        "## Делаем подвыборку текстов для автоенкодера"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjT7QVzznX7K"
      },
      "source": [
        "N_DOCS = 10\n",
        "\n",
        "np.random.seed(123)\n",
        "docs_idx = np.random.choice(len(documents), size=N_DOCS, replace=False)\n",
        "docs_sample = [documents[idx] for idx in docs_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "at474GEpcdSu",
        "outputId": "70f7e782-36c7-4f85-b865-13eb6e3e9792"
      },
      "source": [
        "original_docs_idx = list(map(int, \"\"\"9913  3433  7283  1042  9241  9484  2665  8019   552  9165  8675  4150\n",
        "\t\t  4211  8970  7304  2306  9028  5470  8524   218  2128  8552  5475  6549\n",
        "\t\t  5705  2317  3382  6535  9103  4517   228  4225  2636  3825  3253  8663\n",
        "\t\t  6624  8224  8435  6793  3232  4536  4135  1258   767  8096  8713  1551\n",
        "\t\t  7252   555  5943  7170 10321  1049  2231  8369  9082 10179   119   592\n",
        "\t\t  3941  3173  8407  1215   329 10079  9397  2324  9947   229  3588  6680\n",
        "\t\t  2947  5063  2227  7003  3511  8811  1302  9687  7544  8493  5570  9609\n",
        "\t\t  1502  8646  6093  1919   200  9861  5664  6337  1613  3790  2862  9665\n",
        "\t\t  1033  4338   876   525\"\"\".split()))\n",
        "\n",
        "docs_sample = [documents[idx] for idx in original_docs_idx]\n",
        "print(original_docs_idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9913, 3433, 7283, 1042, 9241, 9484, 2665, 8019, 552, 9165, 8675, 4150, 4211, 8970, 7304, 2306, 9028, 5470, 8524, 218, 2128, 8552, 5475, 6549, 5705, 2317, 3382, 6535, 9103, 4517, 228, 4225, 2636, 3825, 3253, 8663, 6624, 8224, 8435, 6793, 3232, 4536, 4135, 1258, 767, 8096, 8713, 1551, 7252, 555, 5943, 7170, 10321, 1049, 2231, 8369, 9082, 10179, 119, 592, 3941, 3173, 8407, 1215, 329, 10079, 9397, 2324, 9947, 229, 3588, 6680, 2947, 5063, 2227, 7003, 3511, 8811, 1302, 9687, 7544, 8493, 5570, 9609, 1502, 8646, 6093, 1919, 200, 9861, 5664, 6337, 1613, 3790, 2862, 9665, 1033, 4338, 876, 525]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPh7lEm3jp1A",
        "outputId": "2a7c8559-64ac-4508-ca55-6c9297b6d972"
      },
      "source": [
        "print(docs_idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1333 9143 7900 7200 3099 5867 5417 9467 7609 9927]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAUhwwIzo0XU",
        "outputId": "5e9c4455-a842-40a5-ff8f-97a293cb6d82"
      },
      "source": [
        "print(\n",
        "    'Количество слов в подвыборке текстов:',\n",
        "    sum(map(lambda doc: doc.count(' '), docs_sample))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Количество слов в подвыборке текстов: 1923785\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ga12YPt4p6Vw"
      },
      "source": [
        "## Получаем вектора из BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HccUGiyWqAOU",
        "outputId": "40f26c22-737a-459d-d967-8307be724358"
      },
      "source": [
        "!pip3 install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "-OH28xvzpAep",
        "outputId": "600eed28-f9a3-44ea-cfe8-78f11639b90d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from scipy import spatial\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn.decomposition import PCA\n",
        "from tqdm import tqdm\n",
        "from transformers import (\n",
        "    BertModel, BertConfig, BertTokenizer, TrainingArguments, \n",
        "    Trainer, LineByLineTextDataset, DataCollatorForLanguageModeling,\n",
        "    DataCollatorWithPadding, DataCollator, BertForMaskedLM, BertForPreTraining\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-aae6a98af722>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m from transformers import (\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mBertModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLineByLineTextDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataCollatorForLanguageModeling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339,
          "referenced_widgets": [
            "d0e5aa7993d243fa908a604a90d8c4f6",
            "dbfd2029a2a541579837784f565244ea",
            "34d50ed2938346889be334cbbecd1a29",
            "f4bdadff7b534a48972f17c70c798adf",
            "4134cda753e240a29c25b87b3e57e27d",
            "5c8ae31f210d434080b4c8af54640f22",
            "e556e9a292d145cc8aaf39fb54e4f9a4",
            "e52ac86f7a6f41688c90a17dfbb84b43",
            "a3220976c6ad47dc9b5edd6a01828d22",
            "6f1c10c1f2d8480592b3b2f221996156",
            "e4ed6965b2fb480e8734a7ec65acc0f5",
            "6ad496b7b6464cc1ba0cb7cb8c35b560",
            "87b5f5bf532b42cfbd3523125dd8c4ec",
            "d7140c7f880346be9873a99dc16e0189",
            "7389db9f7abb4f2891be354a4cbea558",
            "fd2df6ff62034cbdb126be575bb98fdb",
            "e82a5590c5ac49c3a425cfc671106478",
            "7be986d13ddd48a7b965a707cf78a33c",
            "cc6e8847ca43485eb44a627e2985ca91",
            "3a59823dfc654cf29fecebaaa509449b",
            "1e05477a38dd4b1c84cb8fe61578da29",
            "a9a81a2717ed437db505388aea442fe3",
            "30d8cf457c674bbdbf4932954e78dbfc",
            "55423178eeb9446da3aad9524034c4a6",
            "a59e587ca1174d47b9011be0fb899aa9",
            "595b6fa954d348dbb18ce58b3af1be8c",
            "84da8a59f47740bb85787d834befc41c",
            "e6bec9e3cb2f4c8b807cb492515103fa",
            "c8501d12d75643b099f51c52a5819f45",
            "aee717de80684b5fbee68f9d1160e112",
            "6380f2ed225341aa9ec74dfa4f65fa88",
            "3832437630a74e8ba3d8ac85f6c169da",
            "042668cd13cd4dfc8d2fa8f483d941db",
            "70e86a1373604403a8df70ec0994dd4f",
            "3d805436b0da40ef9e1d04041cfdaadc",
            "a03901d001fb4df88ec42e185be93022",
            "73eadd9aefed40dcacbfef055c1802d0",
            "7e79d9d39244442f9d3c7403f4884a67",
            "c2b120ed8cbc40b18c9279f155fb15f1",
            "c19ce4b7750843db9679773bbaf4d195"
          ]
        },
        "id": "dM6QhTe7pHXP",
        "outputId": "82811856-ff2e-4276-df83-827a9450a434"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "config = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
        "model = BertModel.from_pretrained('bert-base-uncased', config=config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0e5aa7993d243fa908a604a90d8c4f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3220976c6ad47dc9b5edd6a01828d22",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e82a5590c5ac49c3a425cfc671106478",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a59e587ca1174d47b9011be0fb899aa9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "042668cd13cd4dfc8d2fa8f483d941db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXwDLN2GgIWI"
      },
      "source": [
        "tokenizer.save_pretrained('drive/MyDrive/Coursework2021/bert-base-uncased')\n",
        "model.save_pretrained('drive/MyDrive/Coursework2021/bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXGOBkybbToZ",
        "outputId": "b8a07bb5-9d37-4764-c036-618135aaaa65"
      },
      "source": [
        "text = 'perfect weather'\n",
        "def get_ids_tokens(text, tokenizer=tokenizer):\n",
        "    tokens = tokenizer.tokenize('[CLS] ' + text + ' [SEP]')\n",
        "    ids = tokenizer.encode(text)\n",
        "    return ids, tokens\n",
        "\n",
        "get_ids_tokens(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([101, 3819, 4633, 102], ['[CLS]', 'perfect', 'weather', '[SEP]'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sw2suP2ieTeM",
        "outputId": "f9677ec8-0d8b-47c2-80ac-421989e2dcd0"
      },
      "source": [
        "def get_embeddings_from_last(text, tokenizer=tokenizer, model=model):\n",
        "    \"\"\"Вытягивает векторные представления для слов в тексте как крайние hidden states\"\"\"\n",
        "    encoded_input = tokenizer(text, return_tensors='pt')\n",
        "    word_embeddings = model(**encoded_input)[2][-1][0].detach().numpy()[1:-1]\n",
        "    return word_embeddings\n",
        "\n",
        "get_embeddings_from_last('hello there guys')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.07356335, -0.2196039 ,  0.20317924, ..., -0.02935392,\n",
              "         0.1779071 ,  0.28512973],\n",
              "       [ 0.3164919 ,  0.40807694, -0.11108729, ..., -0.81638473,\n",
              "         0.227298  , -0.24182375],\n",
              "       [ 0.19511358, -0.22377829,  0.20091881, ...,  0.8444742 ,\n",
              "         0.8268071 , -0.28723466]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-RDp-mZbcfF",
        "outputId": "aae0943d-96c5-4f21-f4df-3865c2980396"
      },
      "source": [
        "def get_embeddings(text, tokenizer=tokenizer, model=model):\n",
        "    \"\"\"Вытягивает векторные представления для слов в тексте на основе 4 крайних hidden states\"\"\"\n",
        "    encoded_input = tokenizer(text, return_tensors='pt')\n",
        "    four_last_hidden = model(**encoded_input)[2][-4:]\n",
        "    four_last_hidden = np.asarray([hidden[0].detach().numpy() for hidden in four_last_hidden])\n",
        "    word_embeddings = four_last_hidden[:, 1:-1].sum(axis=0)\n",
        "    return word_embeddings\n",
        "\n",
        "get_embeddings('king queen man woman')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.8985329 ,  1.4828641 ,  3.7028997 , ..., -4.339518  ,\n",
              "        -0.66392106, -3.6544752 ],\n",
              "       [ 3.6270385 ,  2.4713006 ,  2.0691109 , ..., -4.5484433 ,\n",
              "        -1.1871322 , -2.1071362 ],\n",
              "       [-0.4785735 ,  3.6755993 ,  3.6122627 , ..., -3.2774358 ,\n",
              "         0.66562   , -3.3163805 ],\n",
              "       [-0.69320405, -0.6555717 , -1.8543705 , ..., -0.9665423 ,\n",
              "        -0.06166075, -1.6272787 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1hwQFU6qY1Z",
        "outputId": "3f2b89b0-bb5e-40eb-fef4-89928925a6d0"
      },
      "source": [
        "tokenizer(documents[2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (36598 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 10381, 1996, 2220, 2332, 1997, 14246, 2063, 2487, 1996, 2406, 1011, 4013, 2078, 1011, 2085, 2113, 2004, 14246, 2063, 2487, 2022, 1996, 12859, 1997, 2455, 3844, 1999, 2011, 8917, 2487, 1996, 3016, 1997, 20377, 3540, 2100, 1996, 14246, 2063, 2487, 1996, 8840, 2278, 2487, 1998, 1996, 8840, 2278, 2487, 2021, 2023, 2406, 2069, 5114, 1996, 2171, 1997, 14246, 2063, 2487, 2011, 3014, 1999, 3058, 2487, 1997, 2029, 1011, 4013, 2078, 1011, 2031, 2151, 4070, 1011, 4013, 2078, 1011, 2022, 2111, 2011, 1996, 4496, 2361, 2487, 1998, 1011, 4013, 2078, 1011, 2022, 2113, 2000, 1996, 4496, 2361, 2487, 2004, 2112, 1997, 1037, 2312, 2406, 2029, 4562, 1996, 2171, 1997, 2711, 2487, 2044, 2035, 1997, 1011, 4013, 2078, 1011, 3828, 8840, 2278, 2487, 2030, 2054, 1011, 4013, 2078, 1011, 2085, 2655, 14246, 2063, 2487, 2031, 2022, 16152, 1998, 7392, 2011, 1996, 4496, 2361, 2487, 1011, 4013, 2078, 1011, 2022, 24672, 2011, 5917, 1997, 1996, 2307, 4496, 2361, 2487, 2679, 1996, 2168, 2155, 2000, 2029, 4496, 2361, 2487, 7141, 1997, 2122, 5917, 1996, 25976, 7392, 1999, 1996, 2874, 2000, 1996, 2148, 1996, 4496, 2361, 2487, 1999, 1996, 8840, 2278, 2487, 2105, 1996, 8840, 2278, 2487, 2096, 1996, 3581, 2272, 2058, 1996, 2314, 1999, 1011, 4013, 2078, 1011, 4895, 21572, 26557, 3064, 8840, 2278, 2487, 3420, 1998, 2191, 1011, 4013, 2078, 1011, 3040, 1997, 1037, 2521, 2898, 3700, 3338, 2039, 2046, 7185, 2487, 2983, 2008, 1997, 1996, 2789, 21310, 1999, 2054, 2022, 2085, 14246, 2063, 2487, 1998, 2008, 1997, 1996, 2530, 21310, 3362, 2013, 1996, 8840, 2278, 2487, 2000, 8917, 2487, 4942, 20041, 2035, 1996, 2060, 4496, 2361, 2487, 25466, 1997, 2711, 2487, 2096, 1011, 4013, 2078, 1011, 11092, 1996, 4676, 1996, 2653, 1998, 2070, 1997, 1996, 10585, 1997, 1996, 23982, 4371, 4496, 2361, 2487, 2040, 2468, 1011, 4013, 2078, 1011, 3395, 2104, 1996, 2030, 18979, 2140, 2487, 4496, 2361, 2487, 5321, 1996, 3400, 2022, 20687, 1999, 1996, 8840, 2278, 2487, 2073, 1011, 4013, 2078, 1011, 2031, 2022, 2005, 1037, 2051, 2404, 2019, 2203, 2000, 2011, 2122, 4496, 2361, 2487, 5274, 1998, 1996, 2059, 4496, 2361, 2487, 2332, 2711, 2487, 2202, 1011, 4013, 2078, 1011, 2173, 2004, 3750, 2012, 1011, 4013, 2078, 1011, 2132, 2021, 1999, 1996, 2051, 1997, 1011, 4013, 2078, 1011, 7631, 1996, 2536, 2983, 1998, 3842, 1997, 2029, 1996, 3400, 2022, 17202, 2991, 4237, 2153, 2104, 2367, 12608, 1997, 1011, 4013, 2078, 1011, 7185, 2487, 1997, 2122, 2711, 2487, 13852, 2022, 2191, 2332, 1997, 1996, 2530, 21310, 1999, 2054, 2022, 2744, 1996, 4496, 2361, 2487, 2030, 2025, 2789, 14246, 2063, 2487, 2013, 2029, 1996, 2556, 14246, 2063, 2487, 2031, 3500, 2023, 14246, 2063, 2487, 1999, 2171, 3104, 2035, 1996, 2406, 8840, 2278, 2487, 1997, 8840, 2278, 2487, 2021, 8134, 1996, 4496, 2361, 2487, 2332, 2031, 2210, 2373, 2148, 1997, 1996, 14246, 2063, 2487, 1998, 8917, 2487, 2022, 2196, 2421, 1999, 1011, 4013, 2078, 1011, 1996, 8917, 2487, 1997, 14246, 2063, 2487, 1996, 2307, 5473, 2029, 2023, 4496, 2361, 2487, 14246, 2063, 2487, 2031, 2000, 3113, 2272, 2013, 1996, 4496, 2361, 2487, 2030, 2004, 1011, 4013, 2078, 1011, 2022, 2655, 1999, 14246, 2063, 2487, 1996, 4496, 2361, 2487, 2122, 10958, 3567, 3351, 1999, 14246, 2063, 2487, 2004, 1011, 4013, 2078, 1011, 10958, 3567, 3351, 1999, 14246, 2063, 2487, 1998, 1037, 2312, 2112, 1997, 1996, 2642, 3023, 2421, 1996, 2677, 1997, 1996, 16470, 2022, 2507, 2011, 2711, 2487, 13852, 2000, 2711, 2487, 2030, 4897, 2080, 7185, 2487, 1997, 1011, 4013, 2078, 1011, 3003, 3005, 2455, 2468, 2113, 2004, 1996, 2167, 2386, 2022, 2455, 2030, 14246, 2063, 2487, 2054, 2087, 4638, 1996, 10958, 3567, 3351, 1997, 2122, 11304, 2022, 1996, 5012, 1997, 14246, 2063, 2487, 1037, 2237, 2029, 3094, 1996, 2346, 2247, 8840, 2278, 2487, 1998, 1011, 4013, 2078, 1011, 2022, 1999, 6985, 1996, 2103, 1997, 14246, 2063, 2487, 2013, 1996, 4496, 2361, 2487, 2008, 1037, 6750, 2171, 2711, 2487, 1996, 2844, 5114, 1996, 3404, 1998, 12242, 1997, 1996, 21490, 4630, 1997, 1996, 4496, 2361, 2487, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 1998, 1011, 4013, 2078, 1011, 2155, 2468, 4175, 1045, 1041, 3648, 1998, 16167, 1997, 14246, 2063, 2487, 1998, 3804, 2030, 3003, 1997, 1996, 21310, 7185, 2487, 4245, 1997, 1011, 4013, 2078, 1011, 2022, 2428, 2307, 2158, 2711, 2487, 1996, 2844, 1051, 3527, 1998, 2711, 2487, 1998, 2043, 1996, 12608, 1997, 2711, 2487, 2031, 3280, 2041, 1037, 3804, 1997, 1996, 2711, 2487, 2022, 1999, 4410, 2332, 1997, 1996, 3581, 2035, 1996, 2044, 2332, 1997, 14246, 2063, 2487, 2091, 2000, 2711, 2487, 2022, 12608, 1997, 2711, 2487, 2011, 2023, 2689, 2174, 1011, 4013, 2078, 1011, 5114, 2210, 1999, 2613, 2373, 2005, 2295, 1011, 4013, 2078, 1011, 4366, 2000, 3627, 2058, 1996, 2878, 2406, 1997, 8917, 2487, 1011, 4013, 2078, 1011, 3691, 2022, 2210, 18235, 2094, 3828, 1999, 1996, 5884, 2029, 1011, 4013, 2078, 1011, 2031, 10295, 2004, 4175, 1997, 14246, 2063, 2487, 2421, 1996, 2103, 1997, 14246, 2063, 2487, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 1996, 12773, 2173, 1011, 4013, 2078, 1011, 2022, 6697, 2205, 1997, 1996, 2307, 6103, 2015, 1997, 2711, 2487, 1998, 2711, 2487, 1997, 7562, 1996, 3804, 1997, 14246, 2063, 2487, 1998, 1996, 4175, 1997, 14246, 2063, 2487, 2000, 1996, 8840, 2278, 2487, 1996, 4175, 1997, 14246, 2063, 2487, 2000, 1996, 2167, 1996, 4175, 1997, 12327, 2000, 1996, 8840, 2278, 2487, 1998, 1996, 3804, 1997, 14246, 2063, 2487, 2000, 1996, 2148, 3477, 1011, 4013, 2078, 1011, 14822, 2021, 2022, 1996, 2069, 5025, 7786, 1999, 1011, 4013, 2078, 1011, 2219, 5884, 1996, 14246, 2063, 2487, 1997, 2711, 2487, 1996, 2653, 1997, 2711, 2487, 2022, 14246, 2063, 2487, 2022, 12528, 4496, 2361, 2487, 1996, 14539, 2854, 1998, 4865, 2386, 2022, 3262, 26522, 4509, 1996, 7015, 2022, 2471, 4498, 2711, 2487, 2045, 2022, 2019, 4824, 2008, 1996, 2332, 2071, 2069, 2552, 2011, 1011, 4013, 2078, 1011, 9619, 1998, 2442, 2022, 5454, 2011, 1011, 4013, 2078, 1011, 2021, 3043, 2175, 2062, 2011, 2214, 7661, 1998, 1996, 2157, 1997, 1996, 2844, 2084, 2011, 2151, 2375, 1037, 16183, 2594, 2375, 2061, 2655, 2013, 1996, 2173, 2043, 3401, 1996, 21310, 2031, 2272, 2022, 6814, 2000, 4839, 2021, 2023, 2031, 2196, 2022, 2224, 2011, 1011, 4013, 2078, 1011, 3395, 3005, 2375, 3961, 2008, 1997, 1996, 2214, 4496, 2361, 2487, 3400, 2119, 1997, 2122, 2291, 1997, 2375, 2174, 2991, 2046, 4487, 13203, 2063, 1998, 2022, 5672, 2011, 12726, 2303, 1997, 7661, 2029, 6360, 4982, 2039, 1996, 10427, 1997, 1996, 2051, 2022, 17003, 2135, 12726, 1998, 27863, 1996, 3581, 2031, 2022, 1996, 9205, 1998, 2087, 4895, 28282, 3468, 1997, 2035, 1996, 4496, 2361, 2487, 3842, 1998, 2069, 12040, 1011, 4013, 2078, 1011, 2000, 1996, 3747, 1997, 4496, 2361, 2487, 1998, 10585, 2013, 1996, 4847, 2029, 14246, 2063, 2487, 18708, 2711, 2487, 2031, 3046, 2000, 3288, 1999, 8840, 2278, 2487, 2021, 1011, 4013, 2078, 1011, 2424, 1011, 4013, 2078, 1011, 16360, 3217, 6776, 1996, 2402, 3581, 1999, 1011, 4013, 2078, 1011, 2082, 2007, 2292, 1011, 4013, 2078, 1011, 2022, 7505, 15194, 2011, 1996, 4496, 2361, 2487, 3183, 1011, 4013, 2078, 1011, 4078, 18136, 2063, 1998, 1999, 1996, 8761, 2008, 3582, 1011, 4013, 2078, 1011, 2331, 3347, 25990, 6491, 3623, 2153, 1996, 10664, 2894, 2562, 2039, 2151, 19614, 1997, 3226, 2021, 2004, 1996, 8111, 1997, 1996, 4496, 2361, 2487, 2022, 15897, 3622, 2000, 1011, 4013, 2078, 1011, 2193, 2031, 2022, 6033, 1998, 2045, 2022, 2062, 18173, 1998, 23277, 29574, 2791, 2084, 2012, 2151, 2060, 2051, 1999, 1996, 11068, 1997, 14246, 2063, 2487, 2172, 2062, 1997, 1996, 2214, 8917, 2487, 5788, 2119, 2426, 1996, 2103, 1998, 1996, 11760, 1998, 1996, 4496, 2361, 2487, 4397, 7392, 1999, 1996, 2167, 2031, 3288, 2007, 1011, 4013, 2078, 1011, 1996, 6819, 3995, 3126, 1997, 1011, 4013, 2078, 1011, 2679, 1011, 4013, 2078, 1011, 2031, 2202, 2039, 2107, 2757, 2030, 3280, 3226, 2004, 1011, 4013, 2078, 1011, 2424, 1999, 14246, 2063, 2487, 1998, 2022, 4287, 1011, 4013, 2078, 1011, 2582, 2061, 2004, 1999, 2070, 3014, 2000, 8300, 2078, 1011, 4013, 2078, 1011, 20065, 2332, 1998, 1011, 4013, 2078, 1011, 2307, 24351, 2071, 3227, 3191, 1998, 4339, 1998, 3305, 1996, 4496, 2361, 2487, 1999, 2029, 2035, 2501, 2022, 2191, 2021, 2261, 3272, 1996, 11646, 2817, 2012, 2035, 2045, 2022, 2082, 1999, 10664, 1998, 2525, 2012, 14246, 2063, 2487, 1037, 2118, 2022, 4982, 2039, 2005, 1996, 2817, 1997, 8006, 8035, 2375, 4695, 1998, 2189, 1996, 2671, 2029, 2022, 2907, 2000, 2433, 1037, 2607, 1997, 2495, 1996, 3460, 1997, 2122, 2671, 8835, 1996, 6288, 1997, 2659, 3014, 2444, 11693, 1998, 5998, 2004, 2190, 1011, 4013, 2078, 1011, 2071, 1998, 10170, 2022, 7410, 2007, 11646, 2040, 3710, 2004, 1037, 4066, 1997, 2797, 14924, 2220, 2332, 1997, 8917, 2487, 4496, 1996, 2279, 7185, 2487, 2332, 2711, 2487, 2022, 2583, 2158, 1998, 1011, 4013, 2078, 1011, 2022, 2471, 13346, 2426, 1996, 9205, 7015, 1997, 1011, 4013, 2078, 1011, 2219, 5884, 1998, 1996, 2307, 4175, 1998, 3804, 2105, 1011, 4013, 2078, 1011, 3317, 2022, 3857, 1997, 4121, 3997, 1998, 3710, 2004, 9089, 1997, 20228, 20824, 2121, 2040, 8336, 2006, 21916, 1998, 2191, 2162, 2006, 2169, 2060, 24665, 2666, 6767, 27191, 21741, 7185, 2487, 2178, 2022, 20184, 2378, 2004, 1996, 14539, 2022, 2744, 2158, 2071, 3604, 7880, 1999, 3808, 1998, 7570, 18752, 2094, 10768, 21735, 1998, 14624, 3653, 3567, 4014, 1996, 2030, 18979, 2140, 2487, 7185, 2487, 2332, 2022, 2204, 1998, 25020, 2158, 2021, 2205, 5410, 2000, 3066, 2007, 1011, 4013, 2078, 1011, 21766, 26989, 2319, 7015, 2711, 2487, 2655, 1996, 25020, 2022, 5186, 26092, 2021, 5410, 1011, 4013, 2078, 1011, 2468, 7861, 12618, 4014, 2007, 1996, 4831, 2006, 4070, 1997, 2031, 5914, 2711, 2487, 1037, 3203, 4013, 3630, 17457, 2000, 2022, 2306, 1996, 3014, 1997, 16730, 23469, 2011, 1996, 2277, 1011, 4013, 2078, 1011, 2022, 4654, 9006, 23041, 24695, 2021, 2907, 2041, 6229, 2045, 2022, 1037, 2307, 3412, 4668, 3965, 2011, 1996, 6772, 2008, 1996, 2088, 2052, 2203, 1999, 1999, 2023, 17626, 2116, 2711, 2681, 1011, 4013, 2078, 1011, 2455, 2127, 3709, 1998, 1996, 9509, 2022, 1037, 6659, 15625, 3582, 2011, 1037, 20739, 9463, 5897, 1998, 1996, 14624, 1997, 14246, 2063, 2487, 2022, 2763, 16655, 26426, 3709, 1999, 2023, 5853, 2043, 1011, 4013, 2078, 1011, 2022, 6684, 2825, 2000, 3413, 9689, 2013, 7185, 2487, 2000, 2178, 1997, 1996, 7185, 2487, 2548, 2103, 14246, 2063, 2487, 1998, 2778, 11693, 6843, 21708, 1998, 1996, 2332, 2507, 2000, 1011, 4013, 2078, 1011, 2673, 1011, 4013, 2078, 1011, 2071, 3913, 1011, 4013, 2078, 1011, 2192, 2006, 1998, 2130, 16837, 2012, 1011, 4013, 2078, 1011, 8954, 2751, 2125, 1011, 4013, 2078, 1011, 4377, 2000, 1996, 2307, 14532, 1997, 1037, 2030, 18979, 2140, 2487, 2564, 1996, 17727, 11124, 3560, 2711, 2487, 1997, 2711, 2487, 2040, 2272, 2013, 1996, 2062, 20783, 1998, 13593, 2148, 6283, 1998, 4078, 18136, 2063, 1996, 5931, 2791, 1998, 2004, 3401, 4588, 2964, 1997, 1011, 4013, 2078, 1011, 3129, 1011, 4013, 2078, 1011, 2022, 1037, 9205, 1998, 13459, 2450, 1998, 3288, 2019, 5783, 1997, 18186, 2046, 1996, 2457, 1999, 2023, 5853, 1996, 2030, 18979, 2140, 2487, 6013, 1997, 14522, 2000, 1996, 2331, 2005, 28354, 2202, 2173, 1996, 6778, 2031, 2022, 1996, 3035, 2022, 18766, 2953, 2021, 2061, 2521, 2022, 1011, 4013, 2078, 1011, 2013, 12063, 1011, 4013, 2078, 1011, 2008, 1011, 4013, 2078, 1011, 4894, 2041, 7185, 2487, 1997, 1011, 4013, 2078, 1011, 3239, 2007, 1011, 4013, 2078, 1011, 3095, 2004, 1011, 4013, 2078, 1011, 2022, 2599, 2627, 1011, 4013, 2078, 1011, 2000, 1996, 12570, 2073, 1011, 4013, 2078, 1011, 2022, 3844, 1999, 1998, 6402, 2006, 2711, 2487, 2022, 2331, 2711, 2487, 2202, 2112, 2114, 1011, 4013, 2078, 1011, 2365, 2711, 2487, 1045, 2006, 6852, 1997, 1011, 4013, 2078, 1011, 2402, 2567, 2021, 2711, 2487, 3653, 3567, 4014, 2076, 1011, 4013, 2078, 1011, 5853, 1996, 11646, 9510, 1999, 4013, 25154, 2054, 2022, 2655, 1996, 18415, 1997, 2643, 2029, 27206, 2162, 1998, 2668, 14740, 2012, 3056, 2161, 1997, 3058, 2487, 1998, 2006, 3058, 2487, 1998, 2191, 2277, 1998, 23106, 2455, 2173, 1997, 9277, 1998, 8493, 2029, 2411, 5262, 4047, 1996, 2375, 3238, 2021, 2029, 2036, 3828, 1996, 5410, 1998, 6728, 20110, 1011, 4013, 2078, 1011, 2022, 2076, 2122, 5853, 2008, 1996, 25097, 2022, 4088, 1996, 2307, 5998, 2005, 15850, 2373, 1998, 4071, 2013, 1996, 3747, 1997, 1996, 3400, 2029, 2765, 1999, 1996, 3623, 4336, 1998, 2373, 1997, 1996, 11646, 1996, 3412, 10768, 2099, 6767, 3126, 2029, 2031, 4088, 2007, 3058, 2487, 2599, 2000, 1996, 3192, 1997, 2116, 6408, 1998, 2000, 2172, 2882, 2277, 4294, 1999, 1996, 5853, 1997, 8917, 2487, 1997, 14246, 2063, 2487, 6855, 14246, 2063, 2487, 1998, 2947, 2468, 2521, 2062, 3928, 2084, 1011, 4013, 2078, 1011, 10514, 6290, 8113, 2711, 2487, 1037, 5410, 2158, 1997, 13925, 10427, 2040, 4682, 2005, 3058, 2487, 1997, 1011, 4013, 2078, 1011, 2166, 2104, 6251, 1997, 4654, 9006, 23041, 21261, 2005, 2019, 4639, 10624, 2271, 3510, 2007, 2711, 2487, 11716, 1997, 14246, 2063, 2487, 1996, 2373, 1997, 1996, 2332, 1998, 1997, 1996, 2375, 2022, 2763, 2012, 1996, 2200, 2659, 1041, 10322, 2076, 1996, 2051, 1997, 2147, 1035, 1997, 1035, 2396, 2487, 2295, 2568, 1998, 5450, 2022, 2625, 2139, 15058, 2094, 2084, 1999, 3058, 2487, 1996, 2030, 18979, 2140, 2487, 16282, 14741, 2000, 1996, 4151, 2455, 2031, 2085, 2468, 7185, 2487, 2307, 2812, 2011, 2029, 1996, 2158, 1997, 1996, 8840, 2278, 2487, 6148, 14933, 2005, 1011, 4013, 2078, 1011, 8254, 14246, 2063, 2487, 2031, 2146, 2022, 2907, 2011, 1996, 4496, 2361, 2487, 2040, 2031, 7438, 1996, 21214, 2092, 2021, 2122, 2031, 2022, 16152, 2011, 1037, 9205, 10722, 29566, 2386, 5917, 2040, 6487, 1998, 6728, 20110, 1996, 21214, 2711, 2487, 1996, 24308, 2709, 2013, 1037, 14741, 13984, 2711, 2487, 2008, 1011, 4013, 2078, 1011, 2052, 2022, 2092, 2000, 16130, 2039, 8917, 2487, 2000, 3298, 2067, 1996, 4496, 2361, 2487, 2373, 1998, 8116, 14246, 2063, 2487, 1998, 8917, 2487, 11914, 2043, 2907, 1037, 2473, 2012, 14246, 2063, 2487, 1999, 14246, 2063, 2487, 9146, 2711, 2487, 2000, 6235, 1999, 10156, 2773, 1996, 14624, 1997, 21214, 1998, 1996, 11268, 5162, 3508, 1997, 1996, 4151, 2173, 5390, 3338, 2041, 2643, 2097, 1011, 4013, 2078, 1011, 1998, 20889, 16215, 17583, 2000, 4374, 2892, 2063, 3013, 2041, 1999, 8416, 2029, 2022, 3435, 2368, 2000, 1996, 3244, 1998, 16393, 1996, 4929, 2121, 2000, 1996, 4151, 2162, 2030, 16282, 2004, 1011, 4013, 2078, 1011, 2022, 2655, 2147, 1035, 1997, 1035, 2396, 2487, 2202, 2053, 3037, 1999, 1996, 3426, 2021, 1011, 4013, 2078, 1011, 2567, 2711, 2487, 4175, 1997, 8917, 2487, 4175, 1997, 2711, 2487, 1997, 14246, 2063, 2487, 1998, 2711, 2487, 14246, 2063, 2487, 3693, 1996, 5590, 2029, 2022, 2191, 2104, 2711, 2487, 1997, 2711, 2487, 3804, 1997, 2711, 2487, 2030, 2054, 1011, 4013, 2078, 1011, 2085, 2655, 1996, 14246, 2063, 2487, 1996, 16282, 6011, 3144, 14246, 2063, 2487, 2022, 5114, 1998, 1037, 14246, 2063, 2487, 1997, 12230, 2103, 1998, 3481, 2022, 2179, 1999, 14246, 2063, 2487, 1997, 2029, 2711, 2487, 2468, 1996, 2030, 18979, 2140, 2487, 2332, 1996, 2878, 1997, 1996, 8840, 2278, 2487, 2022, 6814, 2000, 2562, 2039, 1996, 4721, 1997, 1996, 4151, 2455, 2021, 1999, 2755, 2087, 1997, 2216, 2040, 2175, 2004, 4273, 21214, 2022, 2593, 4496, 2361, 2487, 4496, 2361, 2487, 2030, 4496, 2361, 2487, 1998, 1996, 2158, 1997, 1996, 8840, 2278, 2487, 2655, 2035, 11455, 3581, 7185, 2487, 2344, 1997, 8284, 2040, 2022, 2036, 7307, 2468, 1996, 4568, 8291, 1997, 1996, 14246, 2063, 2487, 1996, 7307, 1997, 2711, 2487, 2036, 2655, 2902, 3917, 2138, 1011, 4013, 2078, 1011, 2036, 7410, 21214, 1998, 7166, 1996, 5305, 1998, 1996, 7307, 27850, 2015, 2119, 2031, 5069, 1999, 2367, 2406, 1999, 8840, 2278, 2487, 2073, 3360, 2022, 3345, 2000, 1996, 3627, 1997, 1011, 4013, 2078, 1011, 2344, 1996, 2214, 7661, 1997, 26294, 21025, 4103, 1037, 2402, 6750, 2007, 1011, 4013, 2078, 1011, 4690, 2022, 4503, 2046, 1037, 2291, 2011, 2029, 1996, 2053, 6321, 4562, 2158, 2022, 3345, 2083, 1996, 4635, 1997, 3931, 1998, 21263, 2000, 2440, 5000, 9021, 1998, 2191, 2000, 2202, 19076, 2029, 14187, 1011, 4013, 2078, 1011, 2000, 17164, 7661, 2000, 5020, 2295, 4895, 3270, 9397, 6588, 2053, 4070, 2022, 2202, 1997, 1011, 4013, 2078, 1011, 14092, 2711, 2487, 1998, 2711, 2487, 2022, 2365, 2711, 2487, 2030, 1996, 6638, 2022, 1996, 2030, 18979, 2140, 2487, 2583, 2158, 3183, 1996, 2240, 1997, 2711, 2487, 2031, 3965, 2144, 1011, 4013, 2078, 1011, 4057, 1996, 6106, 1011, 4013, 2078, 1011, 2191, 1996, 2030, 18979, 2140, 2487, 3535, 2012, 13730, 1996, 7015, 6509, 2011, 10514, 4590, 2711, 2487, 1997, 2711, 2487, 1996, 2069, 6061, 1997, 2079, 2023, 2022, 2000, 6855, 1996, 4681, 1997, 7185, 2487, 2283, 1997, 7015, 2114, 2178, 1998, 2043, 2151, 12890, 5210, 17884, 15226, 2031, 2022, 10797, 14246, 2063, 2487, 2655, 2362, 1996, 7015, 3387, 1998, 11428, 1997, 1011, 4013, 2078, 1011, 5884, 1998, 6855, 1011, 4013, 2078, 1011, 9619, 1998, 5375, 1999, 2191, 2162, 2006, 1996, 5905, 2158, 1998, 16857, 1011, 4013, 2078, 1011, 3317, 2947, 1999, 2070, 3014, 2625, 2368, 1996, 3168, 1997, 14395, 17727, 19496, 3723, 2029, 2031, 3426, 2061, 2116, 4808, 1998, 2107, 9576, 18555, 2791, 1011, 4013, 2078, 1011, 2036, 9146, 1037, 2261, 1997, 1996, 2103, 2000, 5309, 1996, 2157, 1997, 2969, 2231, 1998, 4071, 2013, 1996, 5665, 8192, 1997, 1996, 4175, 2040, 2013, 1011, 4013, 2078, 1011, 6697, 2031, 2468, 1011, 4013, 2078, 1011, 26508, 2021, 1999, 2023, 1011, 4013, 2078, 1011, 4025, 2025, 2000, 2031, 2022, 2061, 2172, 5009, 2011, 2151, 4964, 6958, 2004, 2011, 1011, 4013, 2078, 1011, 2797, 3037, 1998, 3110, 2875, 1996, 3265, 2103, 2030, 2935, 1999, 3160, 2174, 8917, 2487, 2031, 4088, 2000, 2022, 4847, 2011, 2043, 2711, 2487, 3280, 2031, 2074, 3466, 1996, 3510, 1997, 1011, 4013, 2078, 1011, 2365, 2711, 2487, 2007, 2711, 2487, 1996, 20020, 1997, 1996, 3804, 1997, 14246, 2063, 2487, 2947, 3246, 2000, 2191, 1996, 4410, 2428, 2062, 3928, 2084, 1996, 2307, 3159, 2040, 12533, 1011, 4013, 2078, 1011, 14822, 2012, 2023, 2051, 2444, 1996, 2307, 2358, 2711, 2487, 11428, 1997, 2711, 2487, 2040, 2031, 1037, 6919, 3747, 2058, 2158, 2022, 2568, 1011, 4013, 2078, 1011, 2022, 1037, 2051, 1997, 2172, 2245, 1998, 12143, 1998, 2711, 2487, 2019, 2583, 3076, 1997, 8917, 2487, 2907, 1037, 6704, 2007, 2711, 2487, 1999, 2029, 1011, 4013, 2078, 1011, 2156, 1996, 2030, 18979, 2140, 2487, 5998, 2090, 24823, 1998, 3691, 2711, 2487, 27384, 1996, 2402, 2332, 2711, 2487, 2000, 2175, 2006, 1996, 2030, 18979, 2140, 2487, 16282, 2029, 2022, 16617, 2011, 1996, 3750, 1998, 1996, 2060, 3159, 1997, 8840, 2278, 2487, 2000, 15804, 1996, 12893, 1997, 14246, 2063, 2487, 2031, 2053, 8917, 2487, 2061, 1996, 2162, 2022, 2011, 2455, 2083, 1996, 17638, 2940, 1997, 8840, 2278, 2487, 3576, 2073, 1996, 2390, 2022, 2471, 6033, 2011, 1996, 2711, 2487, 2295, 14246, 2063, 2487, 2079, 3362, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 2022, 2007, 11855, 2486, 1011, 4013, 2078, 1011, 2071, 3466, 2498, 2011, 1011, 4013, 2078, 1011, 3049, 1998, 2711, 2487, 2040, 2031, 12673, 1011, 4013, 2078, 1011, 4025, 2000, 2031, 2022, 4498, 13593, 2011, 1996, 4763, 10427, 1997, 1996, 3581, 7392, 1999, 1996, 8840, 2278, 2487, 2574, 2044, 1011, 4013, 2078, 1011, 2709, 14246, 2063, 2487, 21969, 1011, 4013, 2078, 1011, 3510, 1998, 2711, 2487, 2468, 1996, 2564, 1997, 2711, 2487, 14246, 2063, 2487, 2040, 2574, 2044, 22490, 14246, 2063, 2487, 2004, 1011, 4013, 2078, 1011, 2711, 2487, 2004, 2092, 2004, 1996, 11068, 1997, 14246, 2063, 2487, 1998, 6655, 21709, 2232, 1011, 4013, 2078, 1011, 2030, 18979, 2140, 2487, 2365, 2000, 1996, 20020, 1997, 2711, 2487, 2022, 3510, 4025, 2000, 25672, 2035, 2008, 2711, 2487, 2031, 2079, 1999, 5333, 1996, 2548, 2373, 2005, 2711, 2487, 3294, 15849, 16102, 5004, 14246, 2063, 2487, 3005, 2069, 7692, 2022, 1999, 7408, 3468, 26911, 2000, 2202, 2112, 2114, 1011, 4013, 2078, 1011, 1999, 1011, 4013, 2078, 1011, 2116, 2155, 26260, 1996, 2878, 5853, 1997, 14246, 2063, 2487, 1996, 2402, 1996, 2516, 2008, 25276, 2000, 1011, 4013, 2078, 1011, 2006, 4070, 1997, 1011, 4013, 2078, 1011, 3722, 24282, 3267, 2022, 2069, 1037, 2501, 1997, 11251, 1998, 7071, 6229, 1011, 4013, 2078, 1011, 3280, 1999, 2054, 2166, 2175, 2006, 1999, 14246, 2063, 2487, 2175, 2006, 16552, 1999, 1996, 2148, 1996, 2455, 1997, 14246, 2063, 2487, 1998, 2711, 2487, 2031, 2196, 4530, 1996, 2214, 4556, 2293, 1997, 4623, 1998, 2396, 1037, 3730, 2433, 1997, 3714, 4496, 2361, 2487, 2022, 2059, 3713, 1998, 1996, 2396, 1997, 8117, 26877, 6508, 2022, 6976, 2426, 2035, 4635, 4802, 2022, 2655, 4031, 2487, 1998, 19817, 7140, 2615, 24501, 2424, 2121, 2457, 1997, 2293, 2022, 2907, 2073, 2045, 2022, 2971, 1999, 4623, 1996, 3396, 2022, 1037, 3585, 8766, 1998, 2116, 1997, 1996, 9191, 6750, 2022, 2036, 10782, 4031, 2487, 2426, 1011, 4013, 2078, 1011, 1996, 3449, 2094, 2365, 1997, 2711, 2487, 2045, 2022, 2172, 6105, 1997, 5450, 2172, 29083, 1998, 2004, 1996, 4496, 2361, 2487, 5223, 17076, 6777, 2378, 3627, 1996, 4031, 2487, 2196, 13236, 2000, 16130, 2039, 1996, 2365, 1997, 2711, 2487, 2114, 1011, 4013, 2078, 1011, 2711, 2487, 3928, 1999, 2755, 2004, 2711, 2487, 2022, 1011, 4013, 2078, 1011, 2022, 1011, 4013, 2078, 1011, 7215, 2061, 2312, 1037, 2112, 1997, 14246, 2063, 2487, 2104, 1011, 4013, 2078, 1011, 3627, 2029, 2022, 1999, 1996, 2203, 2000, 3857, 2039, 1996, 2307, 2791, 1997, 1996, 4496, 2361, 2487, 2332, 2054, 2031, 2907, 1011, 4013, 2078, 1011, 1999, 4638, 2022, 1996, 4598, 1997, 1996, 2307, 10882, 12879, 2030, 2874, 2169, 2007, 1011, 4013, 2078, 1011, 2219, 2240, 1997, 3804, 2030, 4175, 1998, 2035, 8134, 2981, 1997, 1996, 2332, 2021, 2085, 3053, 2035, 1996, 2874, 1997, 2670, 1998, 2530, 14246, 2063, 2487, 2022, 8587, 2046, 1996, 2192, 1997, 1037, 2309, 7786, 1998, 2295, 1011, 4013, 2078, 1011, 2022, 1037, 4496, 2361, 2487, 1999, 2668, 2664, 2004, 1011, 4013, 2078, 1011, 2022, 2332, 1997, 14246, 2063, 2487, 2023, 7786, 4025, 2000, 1011, 4013, 2078, 1011, 4496, 2361, 2487, 3395, 2053, 4496, 2361, 2487, 2021, 1037, 29524, 1011, 4013, 2078, 1011, 4088, 3568, 2000, 2298, 2000, 1996, 4496, 2361, 2487, 2332, 2000, 2489, 1011, 4013, 2078, 1011, 2013, 1037, 3097, 7786, 1998, 1996, 2365, 1997, 2711, 2487, 2655, 2711, 2487, 2022, 3201, 2000, 2202, 5056, 1997, 1011, 4013, 2078, 1011, 22137, 2711, 2487, 2022, 1037, 2428, 2583, 2158, 2191, 2039, 2011, 4769, 2005, 2215, 1997, 3167, 8424, 1011, 4013, 2078, 1011, 2275, 1011, 4013, 2078, 1011, 2000, 2896, 1996, 2373, 1997, 1996, 8917, 2487, 1997, 14246, 2063, 2487, 1998, 3623, 2008, 1997, 1996, 8917, 2487, 1997, 14246, 2063, 2487, 2004, 1037, 2879, 1011, 4013, 2078, 1011, 2031, 3422, 3034, 2090, 1011, 4013, 2078, 1011, 2269, 1998, 2711, 2487, 2104, 1996, 2307, 17709, 1997, 21025, 21748, 2006, 1996, 3675, 1997, 14246, 2063, 2487, 1998, 2156, 1011, 4013, 2078, 1011, 2269, 2058, 16416, 2818, 1011, 4013, 2078, 1011, 3913, 2039, 1037, 3573, 1997, 11150, 2000, 1996, 6538, 2332, 2004, 2574, 2004, 1011, 4013, 2078, 1011, 2031, 1996, 2373, 1011, 4013, 2078, 1011, 3013, 2091, 1996, 17709, 2029, 2022, 2061, 2312, 2008, 3586, 2386, 2071, 2022, 7713, 2104, 1011, 4013, 2078, 1011, 3589, 1011, 4013, 2078, 1011, 2490, 1996, 2365, 1997, 2711, 2487, 1999, 1011, 4013, 2078, 1011, 7417, 1998, 2022, 2467, 1996, 8618, 22277, 1997, 1996, 2132, 1997, 1996, 2155, 2711, 2487, 7868, 1996, 2892, 1999, 2006, 1996, 14841, 4667, 1997, 1996, 3279, 1997, 14246, 2063, 2487, 1998, 1999, 2587, 2711, 2487, 2012, 14246, 2063, 2487, 2073, 1011, 4013, 2078, 1011, 3467, 1998, 2059, 9498, 2005, 6904, 2278, 2487, 2044, 2023, 2103, 2022, 2202, 2711, 2487, 2709, 2000, 14246, 2063, 2487, 2073, 1011, 4013, 2078, 1011, 3613, 2000, 5618, 2011, 1996, 4126, 1998, 4487, 14416, 10992, 1997, 1996, 17076, 6777, 2378, 1998, 5114, 2119, 2004, 1011, 4013, 2078, 1011, 4099, 1998, 2004, 2332, 1997, 14246, 2063, 2487, 2043, 2711, 2487, 2022, 6332, 2711, 2487, 1996, 8215, 1997, 1996, 3804, 9527, 1997, 14246, 2063, 2487, 1998, 4366, 4630, 1997, 2119, 14246, 2063, 2487, 1998, 2711, 2487, 2202, 5056, 1997, 1996, 2236, 27427, 23773, 3370, 2000, 2907, 1037, 2457, 1997, 8152, 1999, 2029, 2711, 2487, 2006, 1011, 4013, 2078, 1011, 2512, 3311, 2022, 4748, 9103, 11818, 2000, 2031, 2005, 21156, 1011, 4013, 2078, 1011, 10882, 12879, 1999, 1996, 2162, 2029, 3582, 1998, 2203, 1999, 2711, 2487, 2025, 2069, 5114, 1996, 2307, 2711, 2487, 3804, 9527, 2029, 2507, 1011, 4013, 2078, 1011, 1996, 3094, 1997, 14246, 2063, 2487, 1998, 1997, 1996, 2677, 1997, 1996, 16470, 2004, 2092, 2004, 8917, 2487, 1998, 13433, 9956, 2226, 1996, 2406, 2029, 2907, 1996, 14246, 2063, 2487, 1999, 1011, 4013, 2078, 1011, 2373, 2021, 5323, 1996, 20056, 2008, 1037, 4410, 24351, 2022, 2572, 8189, 3468, 2000, 3425, 1998, 2089, 2022, 2191, 2000, 2005, 21156, 1011, 4013, 2078, 1011, 2455, 2054, 1011, 4013, 2078, 1011, 2031, 2663, 2011, 1996, 4690, 1011, 4013, 2078, 1011, 2907, 2011, 9866, 1998, 2204, 2231, 2156, 2008, 1996, 2103, 2022, 5214, 1997, 2022, 2191, 2000, 5703, 1996, 2373, 1997, 1996, 7015, 1011, 4013, 2078, 1011, 3946, 1011, 4013, 2078, 1011, 14293, 2029, 3426, 1011, 4013, 2078, 1011, 2000, 2022, 19593, 1011, 4013, 2078, 1011, 2204, 2767, 1998, 1011, 4013, 2078, 1011, 5326, 2035, 7620, 2295, 2320, 3913, 2104, 2019, 6970, 29201, 2011, 4831, 7036, 3523, 2005, 2019, 22300, 3510, 2711, 2487, 2788, 3582, 1996, 3343, 2029, 5114, 2005, 1996, 2332, 1997, 14246, 2063, 2487, 1996, 2516, 1997, 2087, 4496, 2361, 2487, 2332, 1996, 2613, 3574, 1997, 2023, 2022, 2008, 1011, 4013, 2078, 1011, 2323, 2467, 2490, 1996, 4831, 2114, 1996, 3750, 1998, 1999, 2709, 2022, 3499, 2062, 2084, 6623, 2373, 2058, 1011, 4013, 2078, 1011, 11646, 1996, 2307, 16708, 24351, 1997, 2789, 14246, 2063, 2487, 2007, 1037, 2844, 12753, 2008, 1011, 4013, 2078, 1011, 2022, 1011, 4013, 2078, 1011, 4099, 2191, 1037, 2223, 2007, 1996, 3750, 2711, 2487, 1998, 1011, 4013, 2078, 1011, 4470, 2711, 2487, 2114, 2711, 2487, 2886, 1011, 4013, 2078, 1011, 1999, 1996, 2148, 1998, 2022, 16360, 28426, 2063, 2011, 2711, 2487, 2022, 2711, 2487, 2655, 1996, 7006, 2096, 1996, 2332, 1011, 4013, 2078, 1011, 2067, 2011, 1996, 20934, 27172, 2121, 1997, 1011, 4013, 2078, 1011, 2708, 2103, 5114, 2012, 8917, 2487, 2058, 2711, 2487, 1996, 2030, 18979, 2140, 2487, 2613, 4496, 2361, 2487, 3377, 1999, 2947, 5323, 1996, 2373, 1997, 1996, 4410, 3058, 2487, 6904, 2278, 2487, 2040, 2031, 5914, 2711, 2487, 2022, 12286, 2711, 2487, 1997, 14246, 2063, 2487, 2022, 13260, 2011, 1996, 4496, 2361, 2487, 5797, 2000, 2468, 1011, 4013, 2078, 1011, 2332, 2006, 2711, 2487, 2022, 10214, 2000, 2022, 14187, 2011, 1996, 2307, 6111, 1998, 2711, 2487, 2156, 1011, 4013, 2078, 1011, 2365, 2941, 1999, 6664, 1997, 14246, 2063, 2487, 2012, 1996, 2051, 1997, 1996, 2331, 1997, 1996, 2197, 1997, 1996, 2365, 1997, 1011, 4013, 2078, 1011, 4099, 2711, 2487, 2006, 2711, 2487, 2022, 2331, 2174, 1996, 5797, 9544, 1011, 4013, 2078, 1011, 2775, 2000, 1996, 4496, 2361, 2487, 3159, 1998, 2991, 2185, 2013, 14246, 2063, 2487, 2040, 2022, 2486, 2000, 2709, 2000, 14246, 2063, 2487, 1996, 8917, 2487, 1996, 2279, 2307, 3357, 1999, 1996, 2311, 2039, 1997, 1996, 4496, 2361, 2487, 14246, 2063, 2487, 2022, 2191, 2011, 2202, 5056, 1997, 1037, 3412, 27865, 1999, 1996, 2148, 1996, 2455, 2379, 1996, 8840, 2278, 2487, 2145, 2031, 2172, 1997, 1996, 2214, 8840, 2278, 2487, 1998, 2036, 1997, 1996, 2214, 7897, 1998, 2182, 13368, 1037, 17831, 2655, 1996, 8917, 2487, 2040, 2907, 5448, 2060, 2084, 2216, 1997, 1996, 2277, 2006, 1996, 4761, 1997, 4763, 4831, 7036, 3523, 2044, 4604, 2070, 1997, 1996, 2344, 1997, 25287, 20229, 5323, 2011, 1996, 4496, 2361, 2487, 11282, 2000, 25250, 2000, 1011, 4013, 2078, 1011, 1999, 15784, 13520, 1011, 4013, 2078, 1011, 2004, 2307, 4099, 1997, 1996, 4752, 2004, 4496, 2361, 2487, 1998, 4013, 25154, 1037, 16282, 2114, 1011, 4013, 2078, 1011, 1998, 1011, 4013, 2078, 1011, 2708, 10129, 2711, 2487, 14246, 2063, 2487, 14021, 15603, 2094, 2214, 2332, 2711, 2487, 6414, 9146, 2023, 16282, 2021, 1996, 18959, 1997, 1996, 2167, 1997, 14246, 2063, 2487, 2000, 1996, 2148, 2191, 3677, 1997, 29506, 19311, 2000, 1996, 9484, 1997, 1011, 4013, 2078, 1011, 3003, 2711, 2487, 1037, 2711, 2487, 5797, 26092, 1998, 17164, 2021, 8401, 1998, 6770, 9463, 4757, 21794, 7781, 2022, 2079, 1996, 2878, 2406, 2022, 3913, 5949, 1998, 2711, 2487, 5547, 2000, 2107, 12893, 2008, 2711, 2487, 1045, 2332, 1997, 14246, 2063, 2487, 2040, 2022, 7634, 2004, 1996, 3019, 2132, 1997, 1996, 2670, 2679, 2272, 2000, 1011, 4013, 2078, 1011, 4681, 2021, 2022, 4154, 1998, 22889, 4710, 2012, 1996, 2645, 1997, 2711, 2487, 2044, 2023, 2711, 2487, 2022, 2486, 2000, 12040, 2021, 2107, 2524, 2744, 2022, 2486, 2006, 1011, 4013, 2078, 1011, 2008, 1011, 4013, 2078, 1011, 2111, 10073, 1011, 4013, 2078, 1011, 2406, 2022, 3946, 2000, 2711, 2487, 2040, 3913, 6859, 2000, 14246, 2063, 2487, 1998, 2022, 3102, 2077, 1011, 4013, 2078, 1011, 2071, 2202, 1996, 2103, 1996, 2162, 2022, 2059, 4287, 2006, 2011, 6904, 2278, 2487, 2040, 2031, 9510, 1011, 4013, 2078, 1011, 2269, 2004, 2711, 2487, 1999, 2295, 2069, 2000, 5853, 3058, 2487, 2004, 1011, 4013, 2078, 1011, 3280, 1997, 1037, 9016, 4608, 1999, 1037, 2670, 3049, 1999, 1011, 4013, 2078, 1011, 7794, 2711, 2487, 2191, 3521, 1999, 1996, 2171, 1997, 1011, 4013, 2078, 1011, 2711, 2487, 11814, 1998, 2711, 2487, 2022, 2486, 2000, 2507, 1011, 4013, 2078, 1011, 2069, 2684, 1999, 3510, 2000, 7185, 2487, 1997, 1011, 4013, 2078, 1011, 2402, 2365, 2006, 1011, 4013, 2078, 1011, 2331, 1996, 2221, 1997, 14246, 2063, 2487, 10876, 2063, 2000, 1996, 4410, 2029, 2947, 2468, 10295, 2953, 1997, 2035, 2670, 14246, 2063, 2487, 3828, 2711, 2487, 2029, 2145, 3961, 2000, 1996, 4496, 2361, 2487, 2332, 2021, 1996, 2878, 1997, 1996, 2212, 2320, 2111, 2011, 1996, 8917, 2487, 2031, 2022, 2061, 2172, 5949, 2004, 2196, 2000, 8980, 1011, 4013, 2078, 1011, 14165, 1998, 2151, 10416, 4691, 2039, 1997, 1011, 4013, 2078, 1011, 5448, 2022, 3457, 2114, 2011, 1996, 5069, 1997, 1996, 24638, 2029, 16823, 4496, 2361, 2487, 25287, 2000, 1999, 15549, 2890, 2046, 1998, 4654, 3334, 19269, 2035, 2008, 11234, 2013, 1996, 2277, 2012, 1996, 2168, 2051, 1996, 2344, 1997, 2711, 2487, 2079, 2172, 2000, 16021, 18300, 1998, 4248, 2368, 1996, 13454, 1997, 1996, 2111, 1998, 2012, 1996, 2118, 2926, 2008, 1997, 14246, 2063, 2487, 1037, 2307, 5083, 2119, 1999, 2245, 1998, 4083, 2022, 2191, 2711, 2487, 2022, 18766, 2953, 2711, 2487, 2179, 2005, 1996, 2817, 1997, 16968, 1996, 2267, 2029, 2022, 2113, 2011, 1011, 4013, 2078, 1011, 2171, 1998, 3005, 3247, 2022, 5728, 4374, 2004, 1997, 8917, 2487, 1996, 8917, 2487, 1997, 14246, 2063, 2487, 14246, 2063, 2487, 2031, 1037, 7968, 7786, 1999, 2711, 2487, 1998, 1037, 2145, 2092, 7185, 2487, 1999, 1011, 4013, 2078, 1011, 2711, 2487, 11814, 2040, 2022, 2092, 2113, 2004, 14246, 2063, 2487, 1998, 2040, 2022, 1037, 2428, 2204, 1998, 2307, 2158, 1011, 4013, 2078, 1011, 2022, 1996, 2030, 18979, 2140, 2487, 2000, 5323, 1996, 8917, 2487, 1997, 14246, 2063, 2487, 1037, 2457, 8676, 1997, 1996, 2307, 16708, 24351, 3913, 1998, 12301, 2040, 2907, 1997, 1996, 2332, 3622, 1998, 2040, 2031, 2000, 3046, 2035, 3426, 1011, 4013, 2078, 1011, 2172, 18959, 2507, 2107, 5270, 1998, 1037, 3056, 2193, 1997, 2158, 3345, 2000, 1996, 2375, 2022, 5587, 2000, 1011, 4013, 2078, 1011, 2000, 5009, 1996, 3247, 1996, 8917, 2487, 2022, 2947, 2069, 1037, 2457, 1997, 3425, 1998, 2019, 2436, 2005, 4236, 2097, 1998, 24754, 8917, 2487, 2022, 2655, 1996, 2110, 2236, 1998, 8676, 1997, 2035, 3776, 1997, 1996, 8391, 2021, 2022, 2069, 18654, 1999, 2051, 1997, 5057, 2711, 2487, 2022, 1996, 2030, 18979, 2140, 2487, 2332, 2000, 3288, 7015, 1997, 1996, 2152, 4635, 2000, 12040, 2000, 1996, 8689, 1997, 8917, 2487, 2043, 5905, 1997, 1037, 4126, 25540, 13094, 13033, 2711, 2487, 7185, 2487, 1997, 1996, 7098, 7015, 1997, 14246, 2063, 2487, 2040, 2031, 6865, 7185, 2487, 4496, 2361, 2487, 3360, 2005, 3102, 1037, 10442, 2022, 6251, 2000, 2331, 1996, 6531, 2022, 4012, 26746, 2021, 1996, 6958, 2022, 5323, 14246, 2063, 2487, 2022, 10051, 2791, 1998, 9866, 5114, 1011, 4013, 2078, 1011, 6225, 1998, 2293, 7249, 1998, 1011, 4013, 2078, 1011, 2022, 2467, 3342, 2004, 4133, 2104, 1996, 2307, 6116, 2012, 14246, 2063, 2487, 2079, 5020, 3425, 2000, 4138, 1998, 3532, 14246, 2063, 2487, 2022, 8053, 10051, 1999, 1011, 4013, 2078, 1011, 7149, 2007, 3097, 2373, 1011, 4013, 2078, 1011, 2052, 2025, 2202, 5056, 1997, 1996, 11251, 1997, 2711, 2487, 1997, 14246, 2063, 2487, 2000, 2886, 1011, 4013, 2078, 1011, 2455, 1999, 2711, 2487, 2295, 1011, 4013, 2078, 1011, 5441, 1996, 2157, 1997, 14246, 2063, 2487, 2000, 14246, 2063, 2487, 2004, 2031, 2022, 2005, 21156, 2011, 2711, 2487, 2061, 2172, 2022, 1011, 4013, 2078, 1011, 4847, 2008, 1011, 4013, 2078, 1011, 2022, 2655, 1999, 2000, 3648, 2090, 2711, 2487, 1998, 1011, 4013, 2078, 1011, 5797, 4847, 1996, 11292, 6635, 2013, 1996, 2332, 2011, 1996, 5506, 8917, 2487, 1011, 4013, 2078, 1011, 3247, 1999, 7927, 1997, 2711, 2487, 2022, 2763, 2019, 7481, 7185, 2487, 2021, 1011, 4013, 2078, 1011, 2022, 28616, 19738, 2094, 2011, 1996, 2200, 2367, 7189, 1997, 1996, 4496, 2361, 2487, 1998, 4496, 2361, 2487, 2332, 2000, 1011, 4013, 2078, 1011, 7015, 2040, 1999, 14246, 2063, 2487, 5441, 2375, 24913, 1998, 4808, 2096, 1999, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 2022, 5998, 2005, 2375, 1998, 2344, 2802, 1996, 5998, 2090, 1996, 4831, 1998, 1996, 3750, 2711, 2487, 2052, 2025, 2022, 19653, 2000, 6509, 1999, 1037, 14522, 1997, 1996, 3750, 2029, 1011, 4013, 2078, 1011, 5136, 4895, 29427, 4496, 9146, 7185, 2487, 1997, 1011, 4013, 2078, 1011, 2365, 2000, 5138, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 2043, 1996, 4831, 13520, 2008, 2711, 2487, 2031, 2005, 21156, 1011, 4013, 2078, 1011, 1011, 4013, 2078, 1011, 2071, 2025, 2174, 4652, 1011, 4013, 2078, 1011, 2567, 2711, 2487, 1997, 14246, 2063, 2487, 2013, 5138, 1011, 4013, 2078, 1011, 2005, 2711, 2487, 2031, 5914, 2711, 2487, 20020, 1997, 1996, 4461, 10882, 12879, 1997, 2711, 2487, 1998, 2022, 2947, 2981, 1997, 1011, 4013, 2078, 1011, 2567, 14246, 2063, 2487, 2022, 2583, 2000, 5323, 1037, 3589, 1997, 1996, 4496, 2361, 2487, 2548, 2155, 2006, 1996, 6106, 2012, 14246, 2063, 2487, 1996, 5853, 1997, 14246, 2063, 2487, 2022, 1037, 2051, 1997, 2172, 5082, 1998, 7620, 2045, 2022, 2307, 6288, 1998, 2228, 2121, 2012, 2035, 1996, 2118, 7472, 1998, 4623, 2022, 24299, 1998, 3747, 2111, 2022, 10427, 2061, 2008, 14571, 1045, 1041, 1996, 5450, 6570, 1999, 3317, 2457, 2022, 3730, 2368, 1996, 17183, 11219, 8162, 1997, 5000, 1998, 7015, 4294, 2022, 2012, 1011, 4013, 2078, 1011, 2087, 3376, 2558, 2004, 2022, 2156, 2682, 2035, 1999, 6904, 2278, 2487, 2012, 14246, 2063, 2487, 2023, 2022, 3857, 2011, 2711, 2487, 2000, 4374, 1037, 5592, 1997, 1996, 4496, 2361, 2487, 3750, 8419, 1037, 16337, 2029, 2022, 2903, 2000, 2022, 2013, 1996, 4410, 1997, 16337, 1011, 4013, 2078, 1011, 2022, 7185, 2487, 1997, 1996, 2087, 3819, 2311, 1999, 4598, 16282, 1997, 2711, 2487, 2076, 1037, 5729, 7355, 2191, 1037, 19076, 2000, 2175, 2006, 1037, 16282, 1011, 4013, 2078, 1011, 2030, 18979, 2140, 2487, 11865, 10270, 4014, 3672, 1997, 2023, 19076, 2022, 2191, 2220, 1999, 1011, 4013, 2078, 1011, 5853, 1999, 2043, 1011, 4013, 2078, 1011, 2388, 2022, 2145, 4142, 2000, 16617, 1996, 2711, 2487, 1011, 4013, 2078, 1011, 3535, 2022, 2000, 2886, 1996, 2540, 1997, 1996, 7354, 27524, 2373, 1999, 14246, 2063, 2487, 1998, 1011, 4013, 2078, 1011, 3466, 1037, 4899, 1998, 2202, 1996, 2103, 1997, 14246, 2063, 2487, 2045, 1011, 4013, 2078, 1011, 2681, 1011, 4013, 2078, 1011, 3035, 1998, 5083, 2006, 14246, 2063, 2487, 2021, 2379, 2711, 2487, 1011, 4013, 2078, 1011, 2424, 1011, 4013, 2078, 1011, 4372, 23395, 1999, 1996, 5033, 1997, 1996, 8840, 2278, 2487, 1998, 2007, 1037, 2307, 2390, 1997, 5003, 10199, 15851, 2015, 1999, 2392, 1037, 8917, 2487, 2022, 2424, 1998, 1996, 4496, 2361, 2487, 4656, 1997, 14246, 2063, 2487, 2040, 2031, 3288, 1037, 10123, 2000, 3693, 1996, 16282, 18012, 2008, 1996, 2030, 18979, 2140, 2487, 2000, 2892, 2323, 3524, 1998, 3457, 1996, 6019, 1997, 1996, 2279, 2021, 1996, 2332, 2022, 2567, 2711, 2487, 1997, 8917, 2487, 2655, 2023, 16592, 6610, 1996, 4656, 2022, 12072, 1998, 13520, 1011, 4013, 2078, 1011, 2052, 2022, 2004, 2830, 2426, 1996, 22277, 2004, 2151, 4496, 2361, 2487, 1011, 4013, 2078, 1011, 2119, 3715, 2132, 10052, 2022, 4372, 20464, 9232, 2011, 1996, 4099, 1998, 22889, 4710, 1998, 2295, 1996, 2332, 2012, 2197, 2404, 1996, 5003, 10199, 15851, 2015, 2000, 3462, 1011, 4013, 2078, 1011, 3279, 2022, 21794, 1996, 8840, 2278, 2487, 3123, 1998, 3013, 2125, 1011, 4013, 2078, 1011, 2709, 1011, 4013, 2078, 1011, 4558, 2307, 2112, 1997, 1011, 4013, 2078, 1011, 10123, 2013, 15556, 1998, 2022, 27762, 18820, 4757, 2011, 1996, 5003, 10199, 15851, 2015, 2040, 5466, 2426, 1011, 4013, 2078, 1011, 3677, 1037, 4326, 5255, 7421, 2655, 4496, 2361, 2487, 2543, 1998, 1011, 4013, 2078, 1011, 2022, 2633, 2486, 2000, 7806, 1011, 4013, 2078, 1011, 2004, 1037, 7267, 2012, 2711, 2487, 2007, 2035, 1011, 4013, 2078, 1011, 2390, 1011, 4013, 2078, 1011, 6855, 1011, 4013, 2078, 1011, 2713, 2011, 2507, 2039, 14246, 2063, 2487, 1998, 3477, 1037, 3082, 16540, 2044, 3058, 2487, 1999, 1011, 4013, 2078, 1011, 3535, 2178, 16282, 2029, 2022, 2145, 2062, 15140, 2005, 1011, 4013, 2078, 1011, 2455, 2012, 14246, 2063, 2487, 2000, 3524, 2005, 1011, 4013, 2078, 1011, 2567, 2000, 7180, 2013, 14246, 2063, 2487, 4593, 2006, 2070, 3972, 14499, 1997, 18731, 22137, 2006, 1996, 2112, 1997, 1996, 2711, 2487, 15556, 3338, 2041, 1999, 1996, 3409, 1998, 1996, 2332, 1011, 4013, 2078, 1011, 2684, 1998, 1011, 4013, 2078, 1011, 2030, 18979, 2140, 2487, 2365, 2035, 3280, 1997, 9016, 1998, 2061, 10611, 2022, 1996, 5590, 2008, 1011, 4013, 2078, 1011, 2365, 2711, 2487, 2709, 2000, 14246, 2063, 2487, 8620, 7185, 2487, 13123, 2216, 1997, 1011, 4013, 2078, 1011, 2269, 1011, 4013, 2078, 1011, 2567, 1011, 4013, 2078, 1011, 2905, 1998, 1011, 4013, 2078, 1011, 3129, 1998, 1011, 4013, 2078, 1011, 2219, 2564, 1998, 2775, 2711, 2487, 1996, 4189, 1996, 5853, 1997, 2711, 2487, 2022, 2200, 2460, 1996, 16021, 9890, 5897, 1998, 18186, 1997, 1996, 6011, 25520, 1999, 14246, 2063, 2487, 2031, 27895, 1996, 3128, 2000, 1037, 9288, 2113, 2004, 1996, 4496, 2361, 2487, 2310, 17668, 1998, 1011, 4013, 2078, 1011, 2059, 2655, 1999, 1996, 2332, 1997, 14246, 2063, 2487, 2040, 2633, 6855, 1996, 2479, 2004, 1037, 3584, 14246, 2063, 2487, 2013, 2008, 2006, 1996, 4496, 2361, 2487, 8240, 2073, 14246, 2063, 2487, 1998, 1011, 4013, 2078, 1011, 12608, 2145, 5853, 2096, 2954, 1011, 4013, 2078, 1011, 4470, 2022, 2645, 2006, 1996, 14246, 2063, 2487, 1998, 2022, 11741, 3351, 2711, 2487, 4608, 1037, 9016, 1998, 3280, 2006, 1011, 4013, 2078, 1011, 2126, 2188, 1999, 1011, 4013, 2078, 1011, 6332, 2711, 2487, 2655, 1996, 4189, 2022, 7477, 2100, 10311, 1998, 20505, 1998, 2191, 1996, 8917, 2487, 1997, 14246, 2063, 2487, 1996, 6602, 1997, 1011, 4013, 2078, 1011, 4808, 1998, 6635, 3258, 2029, 1011, 4013, 2078, 1011, 4287, 2041, 1999, 1996, 2171, 1997, 1996, 2375, 2000, 4652, 8917, 2487, 4175, 1997, 14246, 2063, 2487, 2013, 5914, 1011, 4013, 2078, 1011, 2684, 2000, 1996, 2365, 1997, 2711, 2487, 1045, 1997, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 13260, 1011, 4013, 2078, 1011, 1998, 1011, 4013, 2078, 1011, 2269, 2000, 1011, 4013, 2078, 1011, 2457, 1998, 5466, 1011, 4013, 2078, 1011, 2119, 2046, 3827, 2096, 1011, 4013, 2078, 1011, 3749, 1011, 4013, 2078, 1011, 2219, 2684, 2711, 2487, 2000, 2711, 2487, 1997, 2711, 2487, 1999, 1011, 4013, 2078, 1011, 26261, 4215, 1996, 4496, 2361, 2487, 2162, 4652, 2711, 2487, 1045, 2013, 2202, 2039, 1996, 3426, 1997, 3124, 2021, 1996, 4831, 2711, 2487, 1037, 2158, 1997, 1037, 9205, 12178, 2295, 1997, 1037, 2307, 2287, 9928, 2655, 2006, 2711, 2487, 2000, 2079, 3425, 2000, 14246, 2063, 2487, 1998, 10655, 7499, 1999, 4895, 4168, 3022, 12165, 2744, 1011, 4013, 2078, 1011, 6635, 3258, 2013, 1996, 11646, 1011, 4013, 2078, 1011, 2139, 15058, 3672, 1997, 1996, 25396, 1998, 1011, 4013, 2078, 1011, 12487, 1998, 13925, 2166, 9943, 6905, 3413, 2006, 2119, 2217, 2711, 2487, 24608, 1011, 4013, 2078, 1011, 1997, 1037, 28450, 1999, 1996, 4831, 2022, 2602, 2000, 15686, 1011, 4013, 2078, 1011, 2007, 19806, 1998, 1999, 2709, 2022, 4654, 9006, 23041, 24695, 1011, 4013, 2078, 1011, 2059, 4604, 1037, 4496, 2361, 2487, 5000, 2171, 2711, 2487, 2007, 2711, 2487, 1037, 22609, 4496, 2361, 2487, 1996, 14800, 4099, 1997, 2711, 2487, 1998, 1037, 2316, 1997, 9576, 22146, 5268, 2000, 9617, 29076, 2073, 1996, 4831, 2059, 2022, 2000, 2486, 1011, 4013, 2078, 1011, 2000, 9131, 1996, 6251, 4593, 13566, 1011, 4013, 2078, 1011, 2000, 2552, 2066, 1996, 13422, 1997, 2711, 2487, 1996, 2214, 2158, 2022, 13372, 2174, 2058, 10376, 2063, 1011, 4013, 2078, 1011, 2012, 1996, 2617, 1998, 1011, 4013, 2078, 1011, 11036, 2302, 3913, 2192, 2006, 1011, 4013, 2078, 1011, 2021, 1996, 5213, 1011, 4013, 2078, 1011, 2031, 13595, 3426, 1011, 4013, 2078, 1011, 2331, 3058, 2487, 1011, 4013, 2078, 1011, 6332, 2022, 9947, 2471, 3202, 2006, 1011, 4013, 2078, 1011, 2602, 2022, 2113, 2000, 2022, 15316, 2000, 8917, 2487, 2022, 8053, 12042, 1999, 1996, 9530, 23650, 2021, 2711, 2487, 2022, 2767, 18012, 1011, 4013, 2078, 1011, 2000, 4965, 2058, 2000, 1011, 4013, 2078, 1011, 3037, 7185, 2487, 1997, 1011, 4013, 2078, 1011, 6814, 22277, 3183, 1011, 4013, 2078, 1011, 2052, 2059, 15908, 1999, 5454, 2711, 2487, 1997, 14246, 2063, 2487, 2022, 1996, 2158, 1998, 1999, 1037, 3595, 4357, 4872, 2711, 2487, 2000, 11865, 10270, 4014, 7185, 2487, 4650, 2065, 1011, 4013, 2078, 1011, 2022, 2191, 4831, 2011, 1011, 4013, 2078, 1011, 3037, 2122, 2022, 2358, 1996, 16088, 1997, 2711, 2487, 2007, 1996, 2277, 1050, 2094, 2008, 1997, 1011, 4013, 2078, 1011, 4005, 14246, 2063, 2487, 1037, 3946, 2000, 1996, 2332, 1997, 7185, 2487, 1997, 2035, 23106, 3200, 2005, 3058, 2487, 16215, 1996, 6418, 1997, 1996, 2711, 2487, 2155, 2000, 14246, 2063, 2487, 16215, 1996, 8292, 3619, 5397, 1997, 2711, 2487, 2022, 3638, 2122, 7185, 2487, 2022, 4287, 2041, 2011, 2711, 2487, 2004, 1011, 4013, 2078, 1011, 2655, 1011, 4013, 2078, 1011, 2004, 2574, 2004, 1011, 4013, 2078, 1011, 2022, 2006, 1996, 12156, 6106, 1996, 2030, 18979, 2140, 2487, 3961, 1037, 3595, 2021, 2022, 2763, 1996, 6215, 1997, 1996, 7307, 27850, 2015, 2023, 2344, 1997, 2510, 8284, 2031, 2022, 3443, 2005, 1996, 4721, 1997, 1996, 16282, 14246, 2063, 2487, 1997, 14246, 2063, 2487, 1998, 2031, 9878, 2312, 6664, 1999, 8840, 2278, 2487, 2085, 2008, 1011, 4013, 2078, 1011, 6139, 1999, 1996, 8840, 2278, 2487, 2022, 2175, 1011, 4013, 2078, 1011, 2022, 5223, 1998, 14436, 2011, 1996, 2332, 1998, 2711, 2487, 2022, 10663, 2006, 1011, 4013, 2078, 1011, 17264, 6215, 1996, 25097, 2012, 8917, 2487, 2031, 2196, 8046, 14246, 2063, 2487, 2021, 2031, 2175, 2083, 1996, 5103, 1997, 1011, 4013, 2078, 1011, 8272, 2012, 2711, 2487, 1998, 2711, 2487, 3571, 2008, 1999, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 2052, 4468, 4287, 2041, 1996, 5679, 2005, 1996, 10083, 1997, 1996, 27850, 2015, 2031, 1011, 4013, 2078, 1011, 6204, 2000, 8917, 2487, 1037, 2103, 1997, 1996, 3400, 2029, 7141, 2000, 1996, 17076, 6777, 2378, 2332, 1997, 14246, 2063, 2487, 2004, 4175, 1997, 2711, 2487, 1998, 2045, 2005, 3058, 2487, 1996, 12156, 2457, 3961, 2004, 1011, 4013, 2078, 1011, 2022, 2947, 7392, 2485, 2000, 1996, 4496, 2361, 2487, 8880, 1996, 4831, 2468, 2471, 24351, 1997, 14246, 2063, 2487, 1998, 2023, 5587, 6551, 2000, 1996, 2373, 1998, 17738, 7962, 1997, 1996, 4496, 2361, 2487, 2332, 2129, 2613, 1011, 4013, 2078, 1011, 2907, 2006, 1996, 25097, 2022, 2022, 2265, 1999, 1996, 10083, 1997, 1996, 27850, 2015, 1996, 2344, 2022, 2085, 10824, 2011, 1996, 4831, 1998, 1011, 4013, 2078, 1011, 5000, 2022, 13260, 1999, 2312, 2193, 2000, 14246, 2063, 2487, 2104, 3653, 6528, 3401, 1997, 13621, 1037, 16282, 2031, 2022, 2947, 4372, 6494, 2361, 1011, 4013, 2078, 1011, 2022, 26960, 1997, 9202, 1998, 21668, 4126, 1998, 8639, 12005, 26243, 1037, 2261, 6814, 12633, 1011, 4013, 2078, 1011, 2022, 2059, 3046, 2011, 1996, 24638, 1998, 1996, 2307, 2193, 2022, 2404, 2000, 2331, 2011, 2543, 1996, 2882, 3040, 2197, 1997, 2035, 2096, 1011, 4013, 2078, 1011, 2455, 2022, 15126, 2011, 1996, 2332, 1011, 4013, 2078, 1011, 4025, 2000, 2031, 2022, 2428, 1037, 9205, 15818, 1998, 28558, 2275, 1997, 2158, 2030, 2842, 2045, 2442, 2031, 2022, 2070, 26911, 2000, 3828, 1011, 4013, 2078, 1011, 7141, 2004, 2087, 1997, 1011, 4013, 2078, 1011, 2079, 2000, 7015, 4496, 2361, 2487, 2155, 1996, 20739, 1997, 14246, 2063, 2487, 2004, 2711, 2487, 2655, 2711, 2487, 1996, 4189, 2022, 2085, 1996, 2087, 18085, 3159, 1999, 8840, 2278, 2487, 1011, 4013, 2078, 1011, 9530, 18886, 3726, 2000, 17827, 2000, 1011, 4013, 2078, 1011, 13738, 1996, 2103, 1997, 2711, 2487, 2718, 5886, 3406, 2019, 4461, 2103, 2104, 1011, 4013, 2078, 1011, 6507, 2711, 2487, 3280, 1999, 1998, 1011, 4013, 2078, 1011, 7185, 2487, 2365, 8840, 2278, 2487, 2711, 2487, 1998, 2711, 2487, 2022, 2004, 10311, 1998, 8401, 2004, 1011, 4013, 2078, 1011, 2021, 2302, 1011, 4013, 2078, 1011, 5848, 1998, 3288, 1996, 4410, 1998, 2111, 2000, 29591, 1998, 14624, 2169, 5853, 3058, 2487, 1998, 2059, 3280, 2681, 2069, 2684, 1998, 1996, 3160, 13368, 3251, 1996, 12839, 2323, 2175, 2000, 2931, 2043, 8840, 2278, 2487, 3280, 1999, 1011, 4013, 2078, 1011, 2567, 2711, 2487, 2044, 3524, 2005, 1996, 4182, 1997, 1037, 19086, 2775, 2040, 2069, 2444, 3058, 2487, 2202, 1996, 4410, 1998, 1996, 8917, 2487, 2059, 13520, 2008, 1996, 2375, 1997, 1996, 2214, 16183, 2937, 21310, 2031, 2022, 2114, 1996, 12839, 1997, 2450, 2011, 2023, 4397, 7523, 8917, 2487, 1996, 2030, 18979, 2140, 2487, 2567, 5853, 2006, 2711, 2487, 2022, 2331, 2021, 1996, 14246, 2063, 2487, 1997, 2711, 2487, 2031, 16222, 6820, 2063, 2000, 1996, 2155, 2083, 1011, 4013, 2078, 1011, 7133, 1998, 2025, 2022, 3395, 2000, 2375, 2487, 2175, 2000, 1996, 3449, 2094, 2684, 1997, 2711, 2487, 2564, 1997, 1996, 4175, 1997, 23408, 2890, 5602, 10381, 3058, 2487, 2162, 2162, 1997, 2711, 2487, 2011, 2375, 2487, 2004, 1996, 5160, 2655, 1011, 4013, 2078, 1011, 1996, 4410, 2022, 2507, 2006, 1996, 2331, 1997, 2711, 2487, 2000, 2711, 2487, 1997, 8917, 2487, 2000, 1037, 2567, 1997, 2711, 2487, 2021, 1011, 4013, 2078, 1011, 2022, 4366, 2011, 2711, 2487, 1997, 14246, 2063, 2487, 2004, 2365, 1997, 1996, 2684, 1997, 2711, 2487, 4180, 1011, 4013, 2078, 1011, 2174, 2007, 1996, 8210, 23617, 1997, 1011, 4013, 2078, 1011, 3653, 29048, 2127, 2711, 2487, 4654, 3022, 4842, 3686, 1011, 4013, 2078, 1011, 2011, 2886, 2006, 1996, 3675, 1997, 2711, 2487, 2029, 1996, 4496, 2361, 2487, 2332, 2031, 2146, 2022, 11821, 2102, 2000, 3143, 1011, 4013, 2078, 1011, 6664, 1997, 1996, 2148, 1998, 2011, 5157, 1996, 7806, 1997, 2711, 2487, 1997, 8917, 2487, 2040, 2022, 4487, 3736, 9397, 25785, 1999, 1011, 4013, 2078, 1011, 4366, 2000, 1996, 2221, 1997, 8917, 2487, 2011, 1996, 8689, 1997, 1996, 8917, 2487, 1997, 14246, 2063, 2487, 2022, 10975, 18908, 5562, 2011, 2061, 19170, 2854, 2006, 1996, 2166, 1997, 2711, 2487, 2711, 2487, 2059, 13520, 2162, 1998, 1011, 4013, 2078, 1011, 6814, 2157, 3426, 1037, 2301, 1997, 8309, 2090, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 1999, 2029, 1996, 3338, 2091, 19817, 7716, 4181, 2110, 1997, 1996, 4496, 2361, 2487, 14539, 2854, 2507, 14246, 2063, 2487, 2019, 14269, 5056, 1996, 7307, 1998, 21263, 2015, 2022, 7199, 2674, 2021, 2096, 1996, 4496, 2361, 2487, 6300, 20778, 2022, 2844, 26355, 1998, 3404, 13966, 1996, 4496, 2361, 2487, 2022, 11809, 1998, 2069, 2191, 1037, 4154, 2919, 2011, 20228, 20824, 1996, 2991, 2006, 2169, 2217, 11455, 1996, 2162, 4088, 1999, 14246, 2063, 2487, 2073, 2711, 2487, 2202, 1996, 2112, 1997, 1996, 4175, 3005, 5939, 5521, 4890, 2031, 3426, 1011, 4013, 2078, 1011, 18272, 2711, 2487, 2022, 2655, 1999, 2000, 1996, 4681, 1997, 1996, 6926, 1997, 14246, 2063, 2487, 2011, 1011, 4013, 2078, 1011, 3003, 2711, 2487, 1998, 5114, 1037, 2307, 3377, 2058, 1996, 4496, 2361, 2487, 4170, 2012, 14246, 2063, 2487, 2021, 2007, 2053, 2590, 2765, 2012, 1996, 2168, 2051, 1996, 7185, 2487, 2332, 2202, 4500, 2217, 1999, 1996, 2162, 1997, 1996, 8338, 1999, 14246, 2063, 2487, 2169, 6985, 1996, 4366, 2087, 20316, 2007, 1011, 4013, 2078, 1011, 2219, 3653, 29048, 2000, 1996, 4496, 2361, 2487, 2711, 2487, 27329, 1996, 3287, 8215, 2711, 2487, 1998, 8917, 2487, 2931, 4387, 1996, 2564, 1997, 2711, 2487, 1998, 14246, 2063, 2487, 2582, 7669, 13368, 2083, 2711, 2487, 1997, 2711, 2487, 1998, 4175, 1997, 23408, 2890, 5602, 2040, 2022, 2467, 2006, 1996, 3422, 2000, 20865, 1011, 4013, 2078, 1011, 4366, 2000, 1996, 4496, 2361, 2487, 6106, 2083, 1011, 4013, 2078, 1011, 2388, 1996, 2684, 1997, 8840, 2278, 2487, 1998, 2022, 2172, 5223, 1998, 29245, 2011, 2711, 2487, 1998, 1011, 4013, 2078, 1011, 2365, 2711, 2487, 1997, 14246, 2063, 2487, 3571, 1996, 4487, 3736, 16020, 7542, 1997, 1996, 2711, 2487, 1998, 8917, 2487, 13260, 1037, 2193, 1997, 1011, 4013, 2078, 1011, 2000, 1037, 2977, 2012, 14246, 2063, 2487, 1998, 2045, 2031, 1011, 4013, 2078, 1011, 2404, 2000, 2331, 2044, 1037, 27151, 2433, 1997, 3979, 2947, 3298, 1011, 4013, 2078, 1011, 2785, 5596, 2000, 3693, 1011, 4013, 2078, 1011, 4099, 7185, 2487, 1997, 2122, 2125, 10497, 2711, 2487, 1997, 14246, 2063, 2487, 13260, 2711, 2487, 2000, 14246, 2063, 2487, 2073, 1011, 4013, 2078, 1011, 2455, 1998, 2031, 16678, 1011, 4013, 2078, 1011, 4425, 2022, 2006, 1011, 4013, 2078, 1011, 3058, 2487, 2000, 14246, 2063, 2487, 2043, 2711, 2487, 2007, 1996, 2878, 3997, 1997, 1996, 14246, 2063, 2487, 26911, 2000, 19115, 1011, 4013, 2078, 1011, 2012, 8917, 2487, 1999, 14246, 2063, 2487, 1999, 2711, 2487, 2022, 12580, 19907, 2004, 1037, 2236, 1011, 4013, 2078, 1011, 5000, 2022, 3308, 2132, 1998, 22609, 1998, 7078, 3013, 2091, 1011, 4013, 2078, 1011, 2219, 4496, 2361, 2487, 10887, 11024, 2005, 2022, 1999, 1011, 4013, 2078, 1011, 2126, 1996, 4154, 2022, 2561, 2711, 2487, 4536, 2185, 2000, 14246, 2063, 2487, 1998, 2711, 2487, 3913, 6859, 2000, 8917, 2487, 1996, 2173, 2022, 2061, 2844, 2008, 1011, 4013, 2078, 1011, 2022, 2486, 2000, 15823, 1011, 4013, 2078, 1011, 1998, 2711, 2487, 2031, 2051, 2000, 8587, 2178, 2390, 2000, 3535, 1011, 4013, 2078, 1011, 4335, 2021, 1996, 4496, 2361, 2487, 2390, 2022, 2061, 2695, 2008, 1011, 4013, 2078, 1011, 2071, 2025, 2886, 1011, 4013, 2078, 1011, 2302, 2307, 3279, 1011, 4013, 2078, 1011, 7822, 1998, 1996, 2158, 1997, 8917, 2487, 7806, 2711, 2487, 18292, 2008, 7185, 2487, 20934, 27172, 2121, 2323, 3288, 1011, 4013, 2078, 1011, 1996, 3145, 2007, 8164, 2461, 1011, 4013, 2078, 1011, 3300, 2000, 12040, 1011, 4013, 2078, 1011, 2000, 1011, 4013, 2078, 1011, 7185, 2487, 3749, 1011, 4013, 2078, 1011, 2021, 1011, 4013, 2078, 1011, 2166, 2022, 8622, 1998, 1011, 4013, 2078, 1011, 2022, 6225, 8231, 7438, 2711, 2487, 4654, 11880, 2035, 1996, 4496, 2361, 2487, 1998, 2191, 8917, 2487, 2019, 4496, 2361, 2487, 4093, 1037, 18415, 3582, 15897, 1999, 9509, 1997, 1996, 10958, 3567, 3351, 1997, 1996, 2304, 2331, 2029, 11740, 2125, 20889, 2802, 8840, 2278, 2487, 1037, 20739, 9463, 5897, 4593, 8843, 2011, 10882, 24658, 15625, 1998, 2035, 1996, 14624, 1997, 2162, 1998, 2375, 24913, 2021, 2029, 8622, 2053, 4635, 1011, 4013, 2078, 1011, 2031, 20071, 13236, 2077, 2711, 2487, 3280, 1999, 1011, 4013, 2078, 1011, 2365, 2711, 2487, 2022, 2574, 9125, 1999, 1037, 4840, 2162, 2007, 14246, 2063, 2487, 2011, 1996, 20014, 27611, 1997, 2711, 2487, 2919, 1998, 1999, 3935, 22161, 2015, 2000, 4638, 1996, 3159, 1997, 14246, 2063, 2487, 2040, 2031, 2272, 2041, 1997, 2711, 2487, 2006, 1037, 20228, 20824, 5590, 1996, 4496, 2361, 2487, 2022, 2153, 6135, 2799, 2012, 14246, 2063, 2487, 1998, 1996, 2332, 1011, 4013, 2078, 1011, 2007, 1011, 4013, 2078, 1011, 2030, 18979, 2140, 2487, 2365, 2711, 2487, 2022, 2191, 7267, 1998, 4287, 2000, 14246, 2063, 2487, 2007, 2087, 1997, 1996, 2708, 7015, 1996, 14855, 2278, 4226, 7373, 1996, 2655, 2191, 2006, 1011, 4013, 2078, 1011, 24351, 2011, 2122, 12481, 7015, 2000, 4425, 1011, 4013, 2078, 1011, 16540, 3288, 1996, 14624, 2000, 1037, 4578, 1996, 5474, 4171, 2030, 8917, 2487, 2029, 2022, 2030, 18979, 2140, 2487, 17607, 2000, 3113, 1996, 10961, 1997, 1996, 2162, 2022, 2069, 3477, 2011, 2216, 2040, 2022, 4445, 11646, 4496, 7015, 1998, 1996, 2236, 3038, 2022, 2711, 2487, 1996, 8367, 2005, 1996, 14539, 2031, 1037, 5041, 2067, 2292, 1011, 4013, 2078, 1011, 4562, 2035, 1996, 2711, 2487, 2593, 2011, 1996, 2332, 1996, 16708, 2935, 1996, 11646, 2030, 1996, 2316, 1997, 2158, 2012, 2849, 2040, 20996, 3726, 2083, 1996, 2406, 5271, 1011, 4013, 2078, 1011, 2000, 2151, 3159, 2040, 2052, 12666, 1011, 4013, 2078, 1011, 1996, 23277, 29574, 2111, 2022, 6167, 1997, 2673, 1998, 2224, 2000, 5342, 1999, 4920, 1998, 5430, 2013, 5665, 8192, 2030, 15301, 6229, 1011, 4013, 2078, 1011, 3338, 2041, 1999, 1037, 7417, 2655, 1996, 14855, 2278, 4226, 7373, 1998, 7188, 1011, 4013, 2078, 1011, 2071, 15126, 1037, 3317, 7195, 1011, 4013, 2078, 1011, 2066, 1996, 26128, 1011, 4013, 2078, 1011, 2031, 2022, 2191, 2006, 2216, 2306, 1011, 4013, 2078, 1011, 14952, 2022, 2061, 12767, 2011, 1996, 2332, 2022, 2961, 2004, 2000, 2022, 25966, 7699, 28558, 1998, 7897, 5853, 7249, 2004, 1996, 2332, 2022, 1999, 3827, 1998, 1011, 4013, 2078, 1011, 8215, 2711, 2487, 2031, 10574, 16270, 20936, 27678, 27191, 2013, 14246, 2063, 2487, 1996, 6926, 1997, 14246, 2063, 2487, 3246, 2000, 3466, 1037, 5290, 1998, 4125, 2007, 1011, 4013, 2078, 1011, 18523, 8610, 2711, 2487, 2012, 1011, 4013, 2078, 1011, 2132, 15686, 2711, 2487, 1998, 22889, 7974, 7185, 2487, 1997, 1011, 4013, 2078, 1011, 2961, 2077, 1011, 4013, 2078, 1011, 3239, 2006, 1011, 4013, 2078, 1011, 5157, 1996, 2110, 2236, 2022, 9530, 6767, 3489, 1998, 2191, 2878, 14045, 7816, 2004, 2000, 1996, 5450, 1997, 8145, 1996, 25964, 2021, 2053, 7185, 2487, 3272, 3383, 2711, 2487, 2031, 2151, 2613, 27838, 2389, 2030, 2270, 4382, 2711, 2487, 2919, 1997, 2711, 2487, 2040, 2031, 9811, 2000, 9686, 6873, 8557, 1011, 4013, 2078, 1011, 3426, 20895, 1011, 4013, 2078, 1011, 1996, 2332, 13520, 1996, 3247, 1997, 1996, 2163, 2236, 19701, 1998, 11675, 1998, 1996, 7477, 2100, 2968, 1997, 1011, 4013, 2078, 1011, 2365, 4652, 2151, 2586, 2090, 1996, 15451, 8663, 6528, 2102, 1996, 20262, 8320, 1998, 2404, 2091, 1996, 14855, 2278, 4226, 7373, 2007, 9202, 18186, 1998, 7195, 1996, 20934, 27172, 2121, 1997, 14246, 2063, 2487, 2424, 2008, 2711, 2487, 2919, 2069, 2215, 2000, 5114, 1996, 6106, 1998, 2711, 2487, 2052, 2031, 4013, 25154, 1011, 4013, 2078, 1011, 2021, 2216, 2040, 2228, 1011, 4013, 2078, 1011, 2130, 4788, 2084, 1011, 4013, 2078, 1011, 5542, 1997, 2711, 2487, 6449, 1996, 2060, 2711, 2487, 2011, 3183, 2711, 2487, 1998, 1011, 4013, 2078, 1011, 14254, 2022, 2404, 2000, 2331, 1996, 3535, 2012, 5290, 2947, 2203, 1999, 2831, 1998, 4028, 1998, 2035, 2991, 2067, 2046, 1996, 2168, 2110, 1997, 14624, 1998, 20489, 1996, 3521, 1997, 14246, 2063, 2487, 2023, 2711, 2487, 3449, 2094, 2365, 1997, 2711, 2487, 6855, 2011, 5309, 1996, 4461, 10882, 12879, 1997, 2711, 2487, 1997, 2029, 1996, 4175, 2031, 2467, 2022, 2655, 2711, 2487, 1037, 2516, 23166, 15628, 4562, 2011, 1996, 8215, 6835, 1997, 1996, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 2269, 2022, 16187, 1998, 1996, 12339, 1997, 14246, 2063, 2487, 2681, 1011, 4013, 2078, 1011, 3040, 1997, 1996, 8391, 2021, 1011, 4013, 2078, 1011, 2079, 2210, 2000, 6985, 1011, 4013, 2078, 1011, 2043, 2711, 2487, 2153, 2886, 1011, 4013, 2078, 1011, 1998, 1999, 1011, 4013, 2078, 1011, 2022, 2486, 2000, 6812, 2000, 1996, 2744, 2029, 1996, 4496, 2361, 2487, 2332, 5157, 2004, 1996, 3976, 1997, 3521, 1996, 3521, 1997, 14246, 2063, 2487, 9146, 2711, 2487, 2000, 16540, 1011, 4013, 2078, 1011, 2021, 12897, 2000, 14246, 2063, 2487, 1996, 12601, 2058, 1996, 11068, 1997, 14246, 2063, 2487, 1998, 2681, 8917, 2487, 1998, 21179, 4048, 13765, 1999, 1996, 2192, 1997, 2711, 2487, 3280, 1999, 2077, 1011, 4013, 2078, 1011, 16540, 2022, 3477, 1998, 1011, 4013, 2078, 1011, 2365, 4057, 1996, 6106, 2004, 2711, 2487, 2265, 1011, 4013, 2078, 1011, 2013, 2023, 2051, 1037, 15705, 2583, 2158, 1998, 2079, 2172, 2000, 12452, 2054, 2031, 2022, 4558, 2011, 7477, 6588, 3422, 1011, 4013, 2078, 1011, 4495, 1996, 2162, 2175, 2006, 2090, 1996, 9698, 1997, 2169, 2283, 2295, 1996, 4496, 2361, 2487, 1998, 4496, 2361, 2487, 2332, 11268, 7971, 2000, 2022, 2012, 3521, 1998, 2012, 1996, 2645, 1997, 2711, 2487, 1999, 2711, 2487, 2919, 2022, 4154, 1998, 2486, 2000, 2191, 3521, 2007, 14246, 2063, 2487, 2006, 1996, 2060, 2192, 8917, 2487, 1999, 14246, 2063, 2487, 2599, 2011, 2711, 2487, 1998, 1996, 26984, 16659, 2711, 2487, 2022, 2799, 3058, 2487, 2011, 1996, 4496, 2361, 2487, 2283, 2104, 2909, 2711, 2487, 2022, 3102, 1998, 8917, 2487, 5323, 1999, 1996, 11068, 3058, 2487, 1997, 2162, 2031, 3443, 1037, 21794, 2465, 1997, 2158, 8419, 10887, 5268, 1997, 2035, 3842, 2040, 2104, 2070, 3264, 3003, 5271, 1011, 4013, 2078, 1011, 2326, 2000, 3649, 3159, 2089, 2342, 1011, 4013, 2078, 1011, 2104, 1996, 2171, 1997, 2489, 2194, 1998, 2043, 18787, 2444, 2011, 20228, 20824, 1996, 3521, 2031, 2069, 2292, 2122, 23277, 3388, 2818, 6065, 2006, 1996, 14539, 2070, 2031, 15126, 3317, 2043, 3401, 1011, 4013, 2078, 1011, 2071, 20228, 20824, 21916, 2060, 25728, 1996, 2406, 8336, 2006, 1996, 13736, 14539, 2040, 10574, 3401, 2004, 1011, 4013, 2078, 1011, 2022, 2011, 2332, 5797, 1998, 11646, 2022, 8639, 1998, 4028, 2011, 2122, 21766, 26989, 2319, 2061, 2008, 2116, 2444, 1999, 4920, 1999, 1996, 2598, 2008, 1011, 4013, 2078, 1011, 13160, 2089, 2025, 9958, 3086, 2711, 2487, 3749, 1996, 2332, 2000, 15804, 1996, 2406, 2013, 2122, 2489, 2194, 2011, 2599, 1011, 4013, 2078, 1011, 2000, 6509, 1996, 3459, 27861, 2078, 2114, 1011, 4013, 2078, 1011, 5939, 5521, 20913, 2332, 2711, 2487, 1996, 2304, 3159, 2040, 2022, 2059, 2552, 2004, 3099, 1997, 14246, 2063, 2487, 2202, 2174, 1996, 2112, 1997, 2711, 2487, 1998, 4154, 4241, 2711, 2487, 2012, 1996, 2645, 1997, 6583, 10755, 12870, 2006, 1996, 2711, 2487, 1999, 14524, 1997, 1996, 2162, 2023, 5590, 10083, 1996, 3159, 2022, 2740, 1998, 15095, 1011, 4013, 2078, 1011, 8917, 2487, 1037, 22885, 4171, 2022, 3913, 2006, 1996, 21490, 4630, 1997, 14246, 2063, 2487, 1998, 1011, 4013, 2078, 1011, 5574, 2114, 1011, 4013, 2078, 1011, 2000, 2711, 2487, 2348, 2011, 1996, 3521, 1997, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 2031, 2507, 2039, 2035, 2157, 2000, 2963, 5574, 2004, 10514, 6290, 8113, 1996, 5036, 2174, 2022, 2145, 2025, 6246, 7392, 1998, 2006, 2023, 2598, 2711, 2487, 4374, 1011, 4013, 2078, 1011, 12087, 1996, 2162, 2947, 4088, 2153, 1998, 1996, 4690, 1997, 1996, 12294, 1997, 14246, 2063, 2487, 1996, 2152, 2510, 13372, 1997, 1996, 8391, 2022, 2507, 2000, 4241, 2711, 2487, 2021, 2069, 2006, 4650, 2008, 1011, 4013, 2078, 1011, 2052, 4468, 6510, 2645, 1998, 6414, 18820, 4757, 1996, 4496, 2361, 2487, 1998, 2202, 1011, 4013, 2078, 1011, 3317, 2023, 3343, 2022, 2061, 9975, 3582, 2008, 1996, 3804, 1997, 14246, 2063, 2487, 2022, 3499, 2000, 3058, 2487, 2013, 14246, 2063, 2487, 2000, 14246, 2063, 2487, 2302, 3113, 2019, 4099, 1999, 1996, 2492, 1998, 2043, 2332, 2711, 2487, 2191, 1011, 4013, 2078, 1011, 2030, 18979, 2140, 2487, 1998, 2197, 5274, 3053, 2000, 1996, 2813, 1997, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 2022, 2069, 2735, 2067, 2011, 15625, 1998, 2011, 1037, 14388, 8505, 19718, 2029, 2191, 1011, 4013, 2078, 1011, 2903, 2008, 6014, 2022, 2114, 1011, 4013, 2078, 1011, 4241, 2711, 2487, 3280, 2096, 2022, 11741, 3351, 1037, 3317, 1998, 2107, 2022, 1011, 4013, 2078, 1011, 4476, 2008, 1996, 4496, 2361, 2487, 2952, 2052, 2173, 1996, 3145, 1999, 2053, 2192, 2021, 2008, 1997, 1011, 4013, 2078, 1011, 11547, 1996, 12294, 2022, 4690, 2022, 2507, 2000, 2711, 2487, 2036, 1037, 16659, 1998, 2655, 1996, 14998, 2138, 1011, 4013, 2078, 1011, 2507, 2053, 4284, 2000, 1996, 4496, 2361, 2487, 1999, 7195, 2005, 1996, 2331, 1997, 1011, 4013, 2078, 1011, 2567, 1996, 2711, 2487, 2022, 2471, 2000, 1037, 2158, 1997, 8917, 2487, 2031, 2022, 2125, 10497, 2011, 1996, 16021, 9890, 5897, 1998, 20489, 1997, 1996, 4496, 2361, 2487, 1998, 2711, 2487, 2044, 18856, 2075, 2000, 8917, 2487, 2004, 2146, 2004, 2825, 2022, 2486, 2000, 2191, 1011, 4013, 2078, 1011, 3521, 2012, 3091, 2007, 2711, 2487, 2031, 3053, 12452, 2035, 2008, 2031, 2022, 4558, 2043, 1999, 1011, 4013, 2078, 1011, 2331, 2681, 1996, 14246, 2063, 2487, 2000, 1011, 4013, 2078, 1011, 2365, 8917, 2487, 1997, 2711, 2487, 2022, 1037, 2879, 1997, 3058, 2487, 2388, 3238, 1998, 2022, 13462, 2007, 12479, 4470, 2122, 4470, 2022, 2711, 2487, 1997, 14246, 2063, 2487, 2000, 3183, 2711, 2487, 1996, 2197, 1997, 1996, 2220, 17076, 6777, 2378, 2240, 1999, 14246, 2063, 2487, 2022, 4226, 8988, 1011, 4013, 2078, 1011, 2157, 2711, 2487, 1997, 14246, 2063, 2487, 1037, 5410, 2051, 8241, 1998, 2711, 2487, 1996, 2583, 1998, 2087, 7481, 1997, 1996, 7185, 2487, 1011, 4013, 2078, 1011, 7133, 2711, 2487, 1996, 2564, 1997, 2711, 2487, 2031, 2022, 20020, 1997, 1996, 11068, 1998, 2221, 1997, 2711, 2487, 1998, 2122, 2085, 2468, 1011, 4013, 2078, 1011, 12839, 2507, 1011, 4013, 2078, 1011, 1996, 4138, 2112, 1997, 14246, 2063, 2487, 2011, 2145, 2092, 7280, 1011, 4013, 2078, 1011, 2031, 5914, 2711, 2487, 1996, 2069, 2775, 1997, 14246, 2063, 2487, 4175, 1997, 2711, 2487, 5383, 1996, 2307, 8416, 9922, 2237, 1997, 8840, 2278, 2487, 14246, 2063, 2487, 1061, 28994, 4385, 2035, 7272, 1998, 2981, 1998, 2172, 13050, 2000, 2485, 4707, 2007, 14246, 2063, 2487, 2043, 3401, 1011, 4013, 2078, 1011, 6855, 1011, 4013, 2078, 1011, 12121, 2096, 1011, 4013, 2078, 1011, 4175, 2022, 8053, 7422, 2000, 14246, 2063, 2487, 2074, 2004, 4175, 14246, 2063, 2487, 2462, 2031, 2005, 1011, 4013, 2078, 1011, 2375, 3238, 9680, 6305, 3012, 2022, 3298, 2041, 1997, 14246, 2063, 2487, 2011, 2711, 2487, 2061, 1011, 4013, 2078, 1011, 2711, 2487, 3523, 2022, 4654, 11880, 2011, 2711, 2487, 2365, 2000, 2711, 2487, 2031, 2022, 12721, 2011, 14246, 2063, 2487, 2022, 20392, 4808, 1998, 2052, 2025, 2393, 1011, 4013, 2078, 1011, 2021, 2044, 1996, 2214, 2332, 2022, 2331, 8917, 2487, 2224, 1011, 4013, 2078, 1011, 3747, 1999, 1996, 2473, 2000, 6204, 1996, 2878, 2373, 1997, 14246, 2063, 2487, 2000, 14246, 2063, 2487, 2073, 2711, 2487, 2022, 4154, 1998, 29449, 2000, 2331, 1999, 1996, 2645, 1997, 2711, 2487, 1999, 2006, 1996, 4175, 2022, 2331, 2711, 2487, 9510, 1011, 4013, 2078, 1011, 2004, 4175, 1997, 14246, 2063, 2487, 1999, 2157, 1997, 1011, 4013, 2078, 1011, 2564, 1998, 2947, 2022, 3913, 1996, 3192, 1997, 1996, 3928, 1998, 7272, 8917, 2487, 1997, 2711, 2487, 2029, 2005, 7185, 2487, 4245, 2471, 15849, 16102, 5004, 1996, 4410, 1997, 14246, 2063, 2487, 19272, 1997, 2711, 2487, 1996, 12294, 2711, 2487, 2022, 2172, 5223, 2011, 1996, 3804, 1997, 14246, 2063, 2487, 1998, 2019, 2886, 2029, 2022, 2191, 2006, 1011, 4013, 2078, 1011, 1999, 1996, 2395, 1997, 14246, 2063, 2487, 2022, 4415, 7637, 2000, 14246, 2063, 2487, 1996, 2402, 2332, 2040, 2022, 2172, 22476, 2000, 2711, 2487, 2275, 5743, 2000, 6635, 7750, 2006, 1011, 4013, 2078, 1011, 2126, 1037, 28441, 5481, 2041, 1997, 1037, 3224, 1998, 2655, 2041, 2332, 1011, 4013, 2078, 1011, 2022, 20895, 2711, 2487, 2022, 2172, 10363, 1998, 2582, 4025, 2000, 2031, 2031, 1037, 19352, 13181, 3489, 2005, 1011, 4013, 2078, 1011, 2012, 2320, 2468, 9577, 1011, 4013, 2078, 1011, 8980, 2005, 1037, 2051, 2021, 2012, 3058, 2487, 2096, 1011, 4013, 2078, 1011, 1998, 7185, 2487, 2060, 2022, 3153, 14249, 2004, 3748, 2158, 1011, 4013, 2078, 1011, 19002, 1997, 6510, 13109, 8528, 4608, 2543, 7185, 2487, 2022, 6402, 1998, 1996, 5213, 3288, 2067, 1996, 2332, 2022, 12013, 1011, 4013, 2078, 1011, 2468, 3395, 2000, 4906, 1997, 19272, 1997, 2146, 2030, 2460, 9367, 1998, 1999, 1011, 4013, 2078, 1011, 13483, 1011, 4013, 2078, 1011, 4025, 2000, 2031, 2022, 2471, 10047, 4783, 6895, 2571, 2053, 9347, 2031, 2059, 2022, 2191, 2005, 1996, 9530, 3436, 11916, 1997, 1037, 5506, 2332, 1996, 4650, 1997, 1996, 2406, 2468, 2919, 2084, 2412, 1998, 2373, 2022, 10616, 2012, 2011, 9444, 2071, 6855, 1011, 4013, 2078, 1011, 1997, 1996, 2332, 2022, 7185, 2487, 4470, 1996, 3804, 1997, 14246, 2063, 2487, 1998, 1011, 4013, 2078, 1011, 2365, 2022, 3227, 25540, 25725, 2011, 1037, 15784, 5998, 2000, 6855, 14246, 2063, 2487, 1996, 3804, 1997, 14246, 2063, 2487, 2022, 10634, 1998, 5410, 1998, 1996, 2708, 5998, 2005, 3747, 2022, 2090, 8917, 2487, 1998, 1011, 4013, 2078, 1011, 2365, 2711, 2487, 2006, 1996, 7185, 2487, 2192, 1998, 2006, 1996, 2060, 1996, 2332, 2022, 2564, 2711, 2487, 1997, 14246, 2063, 2487, 1998, 1011, 4013, 2078, 1011, 2567, 2711, 2487, 1997, 14246, 2063, 2487, 2040, 2022, 8343, 1997, 2022, 1011, 4013, 2078, 1011, 7089, 2096, 1996, 12511, 2332, 1998, 1011, 4013, 2078, 1011, 2210, 2775, 2022, 2681, 1999, 1037, 23277, 29574, 2110, 2411, 20071, 3073, 2007, 8416, 2063, 2030, 2833, 4496, 2361, 2487, 1998, 2849, 8490, 18357, 2015, 3043, 4982, 2919, 2044, 1996, 2331, 1997, 3804, 2711, 2487, 1999, 1998, 1999, 2074, 2044, 1037, 16064, 16088, 1996, 3804, 1997, 14246, 2063, 2487, 2022, 4028, 1999, 1996, 2395, 1997, 14246, 2063, 2487, 2011, 7947, 1997, 2711, 2487, 1997, 14246, 2063, 2487, 2031, 2022, 1037, 15784, 13219, 2158, 18235, 27822, 2015, 1997, 2035, 3828, 1011, 4013, 2078, 1011, 2219, 5165, 2021, 1011, 4013, 2078, 1011, 2331, 3623, 1996, 14624, 1997, 14246, 2063, 2487, 2083, 1996, 2146, 1998, 9252, 5998, 2005, 14096, 2008, 3582, 1996, 2332, 2022, 13346, 1998, 1996, 2775, 1997, 1996, 3804, 1997, 14246, 2063, 2487, 2022, 2402, 2021, 1011, 4013, 2078, 1011, 3426, 2022, 2202, 2039, 2011, 1037, 2711, 2487, 7015, 2711, 2487, 4175, 1997, 2849, 8490, 18357, 3005, 2171, 1996, 2283, 2202, 1996, 3804, 1997, 2711, 2487, 2022, 2467, 2759, 1999, 14246, 2063, 2487, 2073, 1996, 2111, 2599, 2011, 1996, 9054, 1997, 14998, 2022, 2061, 7422, 2000, 1011, 4013, 2078, 1011, 2008, 1011, 4013, 2078, 1011, 6957, 2000, 2031, 1037, 18408, 25250, 2012, 1996, 2118, 16114, 1996, 4028, 2045, 2022, 2153, 1037, 7408, 3468, 3535, 2012, 5290, 2191, 2011, 1996, 20934, 27172, 2121, 2021, 2004, 2077, 1996, 2062, 6355, 1998, 2375, 3238, 2022, 5905, 1997, 2107, 9987, 2008, 1996, 4500, 2283, 2022, 2655, 1999, 2000, 2404, 1011, 4013, 2078, 1011, 2091, 1996, 2849, 8490, 18357, 2015, 2022, 6449, 2046, 14246, 2063, 2487, 1998, 2202, 1037, 6659, 14096, 2006, 1996, 14998, 1998, 2006, 2035, 25276, 3372, 1997, 2711, 2487, 1999, 1996, 2171, 1997, 14246, 2063, 2487, 1996, 2332, 2022, 3449, 2094, 2365, 1037, 5410, 4487, 18719, 17585, 3360, 2040, 2022, 4498, 2599, 2011, 1996, 4175, 1997, 2849, 8490, 18357, 5274, 1997, 2711, 2487, 2035, 2023, 2051, 1996, 2162, 2007, 14246, 2063, 2487, 2031, 15488, 7140, 16502, 2006, 2069, 3338, 2011, 4766, 18415, 1998, 2043, 14246, 2063, 2487, 2022, 1999, 2023, 23277, 29574, 2110, 2711, 2487, 20687, 1996, 4366, 1997, 2711, 2487, 1998, 1999, 2455, 2077, 2711, 2487, 2044, 8536, 6229, 1011, 4013, 2078, 1011, 2031, 2202, 1996, 2103, 1996, 2711, 2487, 2655, 2362, 1996, 2878, 11760, 1997, 1996, 14246, 2063, 2487, 1998, 5083, 2114, 2711, 2487, 2040, 2066, 2711, 2487, 2031, 2022, 27885, 3669, 3351, 2000, 2681, 14246, 2063, 2487, 1998, 3058, 2487, 2875, 8917, 2487, 1999, 3945, 1997, 4425, 1996, 2390, 3113, 2012, 14246, 2063, 2487, 2073, 2295, 1996, 4496, 2361, 2487, 6551, 2041, 19172, 5677, 1996, 4496, 2361, 2487, 1996, 8066, 1997, 2711, 2487, 1998, 1996, 26272, 1998, 6724, 1997, 1996, 2711, 2487, 2022, 2390, 2599, 2000, 1037, 2561, 4154, 1998, 1996, 16187, 1997, 7185, 2487, 1996, 2708, 2158, 1999, 14246, 2063, 2487, 1997, 8917, 2487, 2426, 1011, 4013, 2078, 1011, 1996, 2402, 3804, 1997, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 2022, 2711, 2487, 2022, 3343, 2000, 7438, 14246, 2063, 2487, 2025, 2004, 1037, 9187, 2021, 2004, 2019, 12839, 1998, 1011, 4013, 2078, 1011, 3568, 10214, 2000, 2292, 2122, 12481, 2022, 16540, 6229, 1011, 4013, 2078, 1011, 2323, 2031, 5547, 1996, 2406, 2000, 22645, 2096, 1011, 4013, 2078, 1011, 7438, 2035, 1996, 2173, 2008, 12040, 2000, 1011, 4013, 2078, 1011, 2007, 2307, 16056, 1996, 3804, 1997, 2711, 2487, 2907, 2632, 21511, 2013, 1996, 5049, 1998, 1996, 2849, 8490, 18357, 2015, 2040, 3627, 1999, 14246, 2063, 2487, 2022, 2205, 5410, 2030, 2205, 23358, 2000, 4604, 4681, 2000, 14246, 2063, 2487, 2029, 2022, 2202, 2011, 2711, 2487, 2044, 1037, 2146, 6859, 14246, 2063, 2487, 3280, 1999, 1011, 4013, 2078, 1011, 2279, 2567, 2711, 2487, 2040, 2022, 2062, 13050, 2000, 2711, 2487, 2079, 2025, 5788, 1011, 4013, 2078, 1011, 3058, 2487, 1998, 1996, 2030, 18979, 2140, 2487, 2567, 2711, 2487, 1037, 8210, 2879, 2022, 1999, 1996, 2192, 1997, 1996, 2849, 8490, 18357, 1999, 1011, 4013, 2078, 1011, 18555, 28616, 8557, 1997, 2373, 27895, 1996, 6926, 1997, 14246, 2063, 2487, 2046, 2292, 1999, 1996, 4496, 2361, 2487, 2043, 2019, 4895, 13102, 25508, 8231, 9202, 9288, 2202, 2173, 2711, 2487, 1011, 4013, 2078, 1011, 2022, 3102, 1011, 4013, 2078, 1011, 6248, 11547, 3556, 2007, 1011, 4013, 2078, 1011, 8917, 2487, 2022, 8011, 2055, 1996, 2395, 1998, 2158, 2450, 1998, 2130, 10527, 1997, 1011, 4013, 2078, 1011, 2283, 2022, 14574, 6770, 9463, 4757, 2135, 8917, 2487, 7185, 2487, 1997, 1011, 4013, 2078, 1011, 14254, 4287, 2125, 1996, 2711, 2487, 2021, 1996, 3035, 16040, 1997, 2849, 8490, 18357, 16021, 9890, 5897, 2031, 3693, 8917, 2487, 8917, 2487, 5564, 2711, 2487, 3613, 2000, 5083, 1998, 2711, 2487, 2514, 1996, 2342, 1997, 3693, 1996, 2878, 3997, 1997, 14246, 2063, 2487, 2114, 1011, 4013, 2078, 1011, 1998, 2191, 25052, 2000, 8917, 2487, 2593, 3571, 2000, 2022, 15849, 16102, 5004, 2011, 1011, 4013, 2078, 1011, 2373, 2030, 2842, 1999, 7195, 2005, 14246, 2063, 2487, 1998, 2849, 8490, 18357, 2053, 10076, 2156, 2008, 1037, 16088, 2022, 3497, 2000, 2202, 2173, 2084, 1011, 4013, 2078, 1011, 4028, 2711, 2487, 2077, 1996, 2711, 2487, 2022, 3239, 2012, 1037, 3034, 2006, 1996, 2958, 1997, 2711, 2487, 2022, 3612, 2022, 2360, 2000, 2022, 1996, 4920, 2029, 2292, 1996, 4496, 2361, 2487, 2046, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 2365, 2711, 2487, 1996, 2047, 3804, 1997, 2711, 2487, 3193, 1996, 2711, 2487, 2004, 5905, 1997, 1011, 4013, 2078, 1011, 2331, 2175, 2058, 2007, 2035, 1011, 4013, 2078, 1011, 2486, 2000, 2711, 2487, 2202, 2007, 1011, 4013, 2078, 1011, 1996, 3035, 1998, 1996, 3532, 13346, 2332, 2012, 1996, 5036, 1997, 8917, 2487, 1999, 2711, 2487, 2022, 13520, 11315, 1998, 8215, 1997, 1996, 14246, 2063, 2487, 2012, 1996, 2168, 2051, 2004, 1011, 4013, 2078, 1011, 4374, 1996, 2192, 1997, 2711, 2487, 2684, 1997, 2711, 2487, 2023, 2507, 1011, 4013, 2078, 1011, 14246, 2063, 2487, 1998, 2035, 1996, 2708, 2103, 1999, 2642, 14246, 2063, 2487, 2021, 1996, 2849, 8490, 18357, 2015, 2907, 1996, 2148, 2007, 14246, 2063, 2487, 2012, 1011, 4013, 2078, 1011, 2132, 2711, 2487, 2022, 13520, 2019, 19104, 2011, 1011, 4013, 2078, 1011, 2269, 2022, 2457, 2021, 1011, 4013, 2078, 1011, 2022, 1999, 3606, 1996, 3003, 1997, 2054, 2031, 2468, 1996, 2120, 1998, 14314, 3426, 2076, 2023, 2051, 2044, 1037, 2146, 5998, 1998, 8040, 23108, 1996, 4831, 2153, 2709, 2000, 14246, 2063, 2487, 1996, 10850, 1997, 14246, 2063, 2487, 2043, 2711, 2487, 1999, 1998, 1996, 12511, 2711, 2487, 3058, 2487, 1996, 10527, 2711, 2487, 2022, 4013, 25154, 2332, 1997, 14246, 2063, 2487, 2004, 2092, 2004, 1997, 14246, 2063, 2487, 2012, 2119, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 2096, 2711, 2487, 2022, 2069, 4013, 25154, 2012, 14246, 2063, 2487, 1998, 1037, 2261, 2060, 2173, 1999, 1996, 2148, 2711, 2487, 2022, 1997, 1037, 4030, 23667, 17701, 2232, 3267, 1998, 1996, 2158, 2105, 1011, 4013, 2078, 1011, 2022, 14337, 1998, 5165, 2293, 20014, 27611, 2099, 2040, 2562, 2632, 21511, 2035, 1996, 7782, 4382, 2013, 1011, 4013, 2078, 1011, 1996, 2567, 1997, 2711, 2487, 1997, 14246, 2063, 2487, 3627, 2035, 1996, 2406, 2167, 1997, 1996, 14246, 2063, 2487, 2007, 14246, 2063, 2487, 2004, 1011, 4013, 2078, 1011, 2132, 4284, 2005, 3058, 2487, 2210, 2022, 2079, 2021, 1999, 1011, 4013, 2078, 1011, 3426, 14246, 2063, 2487, 2000, 2022, 2022, 11741, 3351, 1996, 2103, 2907, 2041, 9191, 2135, 2035, 14246, 2063, 2487, 2298, 2006, 23403, 1998, 1037, 2402, 14539, 2611, 2171, 2711, 2487, 2903, 1011, 4013, 2078, 1011, 2655, 2011, 2376, 2013, 1996, 3002, 2000, 5343, 1996, 2103, 1998, 2599, 1996, 2332, 2000, 1011, 4013, 2078, 1011, 12773, 2012, 14246, 2063, 2487, 2007, 7669, 1011, 4013, 2078, 1011, 6855, 1037, 4994, 1997, 1996, 2332, 1998, 2022, 3499, 2000, 10838, 2000, 14246, 2063, 2487, 2599, 1996, 2390, 2007, 1037, 12299, 4690, 2029, 1011, 4013, 2078, 1011, 2196, 21101, 2007, 2668, 1011, 4013, 2078, 1011, 6039, 1996, 4496, 2361, 2487, 2007, 7023, 1996, 4496, 2361, 2487, 2007, 3571, 2004, 1997, 1037, 6965, 1998, 2947, 1011, 4013, 2078, 1011, 5114, 3058, 2487, 11210, 1011, 4013, 2078, 1011, 3711, 14246, 2063, 2487, 2022, 3828, 1998, 1011, 4013, 2078, 1011, 2059, 6204, 2711, 2487, 2000, 14246, 2063, 2487, 1998, 3233, 3875, 1011, 4013, 2078, 1011, 6106, 2043, 1011, 4013, 2078, 1011, 2022, 4410, 2059, 1011, 4013, 2078, 1011, 2360, 1011, 4013, 2078, 1011, 2147, 2022, 2079, 1998, 2052, 2031, 2709, 2188, 2021, 2295, 1996, 23277, 29574, 2332, 1998, 1011, 4013, 2078, 1011, 2457, 2196, 9120, 1011, 4013, 2078, 1011, 1011, 4013, 2078, 1011, 2228, 1011, 4013, 2078, 1011, 6179, 2007, 1996, 5268, 1998, 2052, 2025, 2292, 1011, 4013, 2078, 1011, 2681, 1011, 4013, 2078, 1011, 1011, 4013, 2078, 1011, 2031, 4558, 1011, 4013, 2078, 1011, 2540, 1998, 3246, 1998, 1996, 2158, 4088, 2000, 2022, 4963, 2012, 1011, 4013, 2078, 1011, 2005, 2404, 2091, 2035, 3580, 1998, 12487, 2653, 1996, 2952, 2022, 4372, 24918, 1997, 1011, 4013, 2078, 1011, 1998, 2012, 2197, 2043, 1011, 4013, 2078, 1011, 2031, 2599, 1037, 2711, 2487, 2041, 1997, 1996, 2022, 11741, 3351, 2237, 1997, 8917, 2487, 1996, 4796, 2022, 3844, 1998, 1011, 4013, 2078, 1011, 2022, 2191, 7267, 2011, 1037, 4496, 2361, 2487, 2711, 2487, 1997, 14246, 2063, 2487, 1996, 4496, 2361, 2487, 5223, 1011, 4013, 2078, 1011, 2130, 2062, 2084, 1996, 4496, 2361, 2487, 1996, 1999, 15549, 28307, 2022, 1997, 1011, 4013, 2078, 1011, 2283, 1998, 1037, 2457, 2022, 2907, 2012, 14246, 2063, 2487, 2029, 28887, 1011, 4013, 2078, 1011, 2000, 3280, 2004, 1037, 6965, 14246, 2063, 2487, 9619, 2021, 2681, 1996, 2103, 2077, 1996, 7781, 1011, 4013, 2078, 1011, 2219, 2332, 2191, 2053, 3947, 2000, 3828, 1011, 4013, 2078, 1011, 2295, 3058, 2487, 1011, 4013, 2078, 1011, 3426, 4372, 15549, 2854, 2000, 2022, 2191, 5323, 1011, 4013, 2078, 1011, 12660, 4372, 25083, 2571, 1011, 4013, 2078, 1011, 2155, 1998, 2489, 1011, 4013, 2078, 1011, 2352, 2013, 14952, 7233, 1997, 14246, 2063, 2487, 2021, 2295, 2711, 2487, 2022, 2175, 1011, 4013, 2078, 1011, 2147, 2197, 1996, 12294, 2711, 2487, 1997, 14246, 2063, 2487, 1996, 4175, 1997, 4496, 2361, 2487, 1998, 2060, 9191, 3003, 3613, 2000, 2886, 1996, 4496, 2361, 2487, 2044, 3058, 2487, 14096, 2005, 1011, 4013, 2078, 1011, 2269, 2022, 2331, 1996, 3804, 1997, 2711, 2487, 2191, 1011, 4013, 2078, 1011, 3521, 2007, 2711, 2487, 2011, 1037, 5036, 2012, 12098, 8180, 2006, 4650, 1997, 3477, 2053, 2062, 14822, 1999, 14246, 2063, 2487, 3280, 2574, 2044, 1998, 2045, 2022, 2498, 2021, 7593, 2426, 8917, 2487, 2330, 1011, 4013, 2078, 1011, 4796, 2000, 1996, 2332, 1998, 2711, 2487, 2471, 1999, 8741, 1997, 1011, 4013, 2078, 1011, 2022, 9239, 2019, 2583, 6432, 2171, 2711, 2487, 18496, 1011, 4013, 2078, 1011, 2769, 2029, 1041, 15549, 2361, 1011, 4013, 2078, 1011, 2158, 2005, 1996, 7233, 1997, 14246, 2063, 2487, 1998, 1011, 4013, 2078, 1011, 1011, 4013, 2078, 1011, 5256, 2046, 4023, 2202, 14246, 2063, 2487, 1998, 1996, 2060, 2103, 2006, 8840, 2278, 2487, 1997, 14246, 2063, 2487, 2011, 2122, 3112, 2711, 2487, 2031, 8980, 2035, 3828, 8917, 2487, 2008, 2711, 2487, 2031, 2202, 2013, 14246, 2063, 2487, 2021, 1011, 4013, 2078, 1011, 2022, 2085, 2583, 2000, 2079, 2062, 1996, 7185, 2487, 2874, 1997, 1996, 2148, 2029, 1996, 4496, 2361, 2487, 2332, 2031, 2196, 2022, 2583, 2000, 2663, 2022, 2711, 2487, 1996, 11068, 2006, 8840, 2278, 2487, 2031, 2022, 1037, 2112, 1997, 2711, 2487, 2022, 12839, 1998, 3413, 2083, 1011, 4013, 2078, 1011, 2000, 1996, 4496, 2361, 2487, 2332, 2021, 2295, 1011, 4013, 2078, 1011, 2031, 4558, 2035, 2842, 1996, 11150, 1997, 1011, 4013, 2078, 1011, 21490, 4630, 2000, 1996, 4496, 2361, 2487, 9585, 1011, 4013, 2078, 1011, 2000, 9279, 2023, 1998, 2711, 2487, 2031, 2196, 2664, 3413, 2104, 4496, 2361, 2487, 3627, 1011, 4013, 2078, 1011, 2022, 23277, 4355, 2174, 2013, 2711, 2487, 2022, 12608, 1999, 2023, 7186, 10401, 1997, 9187, 14246, 2063, 2487, 2907, 2041, 2004, 2146, 2004, 1011, 4013, 2078, 1011, 2071, 2021, 2711, 2487, 2071, 4604, 2053, 4681, 1998, 1011, 4013, 2078, 1011, 2022, 2486, 2000, 10750, 3058, 2487, 9191, 2214, 2935, 2711, 2487, 2599, 2158, 2000, 8980, 1996, 11068, 1998, 2022, 24986, 6160, 2021, 1011, 4013, 2078, 1011, 2022, 22889, 4710, 1999, 1996, 2645, 1997, 19371, 2078, 2954, 2066, 1037, 7006, 1011, 4013, 2078, 1011, 7185, 2487, 2365, 2991, 3875, 1011, 4013, 2078, 1011, 1998, 1011, 4013, 2078, 1011, 2390, 2022, 3338, 14246, 2063, 2487, 2153, 7806, 1998, 1996, 4496, 2361, 2487, 2332, 2012, 2197, 2424, 1011, 4013, 2078, 1011, 3040, 1997, 1996, 2307, 10882, 12879, 1997, 8840, 2278, 2487, 2022, 2012, 1996, 2485, 1997, 3058, 2487, 2162, 1996, 2069, 6664, 2681, 2000, 8840, 2278, 2487, 1997, 1996, 3149, 1996, 3233, 2390, 2004, 2012, 3058, 2487, 2552, 1999, 3058, 2487, 2162, 1996, 2307, 7669, 1999, 2051, 1997, 3521, 2022, 1996, 3739, 1997, 1996, 2316, 1997, 2489, 7452, 2030, 22146, 5268, 2040, 2043, 2162, 1998, 20228, 20824, 8246, 1011, 4013, 2078, 1011, 2444, 2011, 4808, 1998, 13742, 1997, 1996, 14539, 2711, 2487, 2040, 2031, 8300, 2078, 2046, 6819, 3995, 3126, 2045, 6279, 2239, 2202, 2046, 3180, 3477, 2035, 2040, 2052, 12040, 2000, 9009, 1998, 1996, 2717, 2022, 2599, 2125, 2006, 7185, 2487, 24495, 5590, 2046, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 1998, 2045, 2681, 2000, 1011, 4013, 2078, 1011, 6580, 1996, 3159, 1998, 7015, 2022, 2012, 2030, 18979, 2140, 2487, 2061, 2172, 17733, 2012, 1996, 7816, 2029, 14187, 1996, 5268, 2100, 2000, 4847, 1996, 23848, 2923, 22648, 2100, 2008, 1011, 4013, 2078, 1011, 5333, 1037, 7417, 2029, 2022, 6469, 2011, 14246, 2063, 2487, 2040, 2022, 3201, 2000, 2079, 2505, 2008, 2071, 5754, 6977, 1011, 4013, 2078, 1011, 2269, 2021, 1011, 4013, 2078, 1011, 2022, 2574, 20010, 6776, 2013, 1011, 4013, 2078, 1011, 1996, 3804, 1997, 2711, 2487, 2052, 2025, 6509, 1011, 4013, 2078, 1011, 1998, 1996, 2223, 2991, 2000, 3538, 2711, 2487, 2011, 2947, 9279, 2194, 1997, 10887, 10123, 1999, 1011, 4013, 2078, 1011, 3477, 3913, 1996, 3192, 1997, 1996, 2030, 18979, 2140, 2487, 3233, 2390, 1999, 8840, 2278, 2487, 1998, 9585, 1996, 12078, 2000, 29449, 2091, 1996, 16708, 2486, 1997, 1996, 7015, 1011, 4013, 2078, 1011, 2231, 2022, 3813, 1998, 7968, 1998, 2007, 1011, 4013, 2078, 1011, 5853, 4088, 2092, 2051, 2005, 14246, 2063, 2487, 2021, 1011, 4013, 2078, 1011, 2022, 2146, 2077, 1011, 4013, 2078, 1011, 8980, 2013, 1996, 14624, 1997, 1996, 2146, 27865, 1996, 2162, 2031, 2562, 2067, 2172, 1997, 5082, 2045, 2031, 2022, 24665, 2666, 6767, 2271, 22156, 1997, 2311, 1999, 1996, 2167, 1998, 2803, 1997, 14246, 2063, 2487, 2172, 2375, 24913, 1998, 18186, 3653, 3567, 4014, 1998, 2664, 2045, 2022, 1037, 3056, 5083, 1999, 4083, 1998, 2172, 2293, 1997, 7472, 1998, 1996, 3399, 1997, 9610, 10175, 2854, 3931, 1997, 7015, 4182, 2022, 8843, 2039, 1999, 3317, 2000, 2022, 2030, 18979, 2140, 2487, 21263, 1998, 2059, 7307, 2045, 2022, 14269, 5337, 3012, 1998, 2110, 20942, 1996, 2344, 1997, 23359, 2022, 2051, 2487, 1998, 13433, 8737, 1998, 4653, 2022, 6919, 4326, 11477, 9323, 2202, 2173, 3058, 2487, 1996, 2395, 1997, 14246, 2063, 2487, 2052, 2022, 1037, 3496, 1997, 9202, 15625, 2073, 7501, 3899, 1998, 2130, 4702, 2404, 2019, 2203, 2000, 1996, 14624, 1997, 2732, 3726, 11573, 2775, 1997, 14574, 6687, 2178, 1996, 2111, 2052, 2022, 3657, 2012, 2548, 19032, 2197, 1037, 2878, 2154, 2007, 2035, 27203, 11259, 3723, 1997, 20919, 2006, 1996, 2795, 1998, 12438, 2272, 2090, 1996, 2607, 2073, 2035, 1996, 11870, 18820, 3070, 5657, 1999, 2735, 2030, 2073, 5000, 8116, 10494, 2013, 5016, 1998, 18340, 2158, 1999, 1996, 2148, 2045, 2022, 2625, 14624, 1998, 2062, 5082, 2711, 2487, 2022, 8917, 2487, 2012, 14246, 2063, 2487, 2022, 2145, 1037, 8348, 1997, 4398, 4294, 1998, 2711, 2487, 1997, 14246, 2063, 2487, 1998, 4175, 1997, 2711, 2487, 2022, 2019, 6581, 5276, 2006, 3221, 1998, 2036, 1037, 4802, 10381, 1996, 5998, 2007, 8917, 2487, 2035, 1996, 4390, 1997, 14246, 2063, 2487, 2005, 3058, 2487, 2031, 2175, 2000, 3623, 1996, 3997, 1997, 1996, 3804, 1997, 2711, 2487, 1996, 2221, 1998, 11068, 1997, 2029, 8917, 2487, 2022, 1996, 3007, 4682, 1999, 1996, 2087, 14946, 2212, 1997, 14246, 2063, 2487, 1998, 2031, 2004, 1011, 4013, 2078, 1011, 2031, 2156, 2022, 9530, 7512, 2006, 2711, 2487, 1996, 7782, 1011, 4013, 2078, 1011, 3510, 2031, 2507, 2000, 1011, 4013, 2078, 1011, 14246, 2063, 2487, 2007, 1037, 26984, 11760, 1998, 2007, 1996, 2708, 5814, 2103, 1997, 8840, 2278, 2487, 2711, 2487, 2022, 2365, 2711, 2487, 2031, 5914, 1037, 3203, 2040, 4821, 3288, 2046, 1996, 2155, 1996, 2307, 4461, 2221, 1997, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 1998, 1011, 4013, 2078, 1011, 2365, 3804, 2711, 2487, 1996, 2204, 2011, 5309, 2030, 12839, 6855, 6664, 1997, 2035, 1996, 13562, 2210, 10882, 12879, 2433, 1996, 2406, 2655, 1996, 14246, 2063, 2487, 2070, 7141, 2000, 1996, 3400, 2070, 2000, 14246, 2063, 2487, 2711, 2487, 2031, 2735, 1996, 4094, 1999, 1996, 5998, 2090, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 1998, 2004, 1011, 4013, 2078, 1011, 10377, 2031, 2663, 1996, 2103, 2006, 1996, 8840, 2278, 2487, 1011, 4013, 2078, 1011, 2031, 2947, 2468, 1996, 4138, 1998, 2087, 3928, 3159, 1999, 8840, 2278, 2487, 1998, 4025, 2006, 1996, 2391, 1997, 2179, 1037, 2690, 2110, 4688, 2090, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 5410, 2391, 2022, 2008, 1996, 4461, 10882, 12879, 1999, 2711, 2487, 1998, 2711, 2487, 4682, 2090, 1011, 4013, 2078, 1011, 3804, 9527, 1997, 2711, 2487, 1998, 1011, 4013, 2078, 1011, 2221, 1999, 1996, 14246, 2063, 2487, 2053, 4496, 2361, 2487, 2457, 5020, 1999, 11867, 7770, 26797, 2099, 2008, 1997, 2711, 2487, 1996, 2307, 2103, 1997, 14246, 2063, 2487, 14246, 2063, 2487, 1998, 1996, 2717, 2295, 2440, 1997, 9205, 1998, 24501, 4747, 10421, 2158, 3477, 1011, 4013, 2078, 1011, 2349, 2438, 2000, 2191, 1011, 4013, 2078, 1011, 1996, 4138, 1997, 3159, 1998, 1996, 4496, 2361, 2487, 5000, 2022, 2426, 1996, 7782, 1999, 8840, 2278, 2487, 2035, 1996, 2396, 1997, 2166, 2682, 2035, 4169, 1998, 4968, 4294, 2053, 9496, 4095, 2012, 14246, 2063, 2487, 1998, 7880, 2022, 10123, 2061, 2092, 6055, 20934, 27172, 2121, 2062, 18241, 4553, 2062, 6923, 2084, 1999, 1011, 4013, 2078, 1011, 5884, 2182, 2205, 2022, 1996, 2087, 8292, 28578, 10698, 3560, 14571, 1996, 2087, 21459, 19032, 1998, 1996, 2087, 6919, 4653, 1997, 13713, 5127, 1998, 8416, 1997, 2751, 2711, 2487, 1037, 12266, 2295, 1037, 3147, 18627, 11424, 16136, 2158, 2292, 2711, 2487, 2894, 2525, 2156, 2129, 1996, 2208, 2052, 2175, 2005, 1996, 2925, 2005, 2043, 1996, 2711, 2487, 2031, 26260, 2007, 1996, 5853, 8837, 1998, 2022, 19045, 4374, 2006, 1011, 4013, 2078, 1011, 3462, 2000, 2711, 2487, 1996, 2214, 2332, 1055, 19755, 2360, 2008, 1996, 3804, 2022, 6469, 1996, 8917, 2487, 2040, 2052, 8954, 1011, 4013, 2078, 1011, 7975, 2711, 2487, 2022, 2711, 2487, 9510, 1011, 4013, 2078, 1011, 2269, 2711, 2487, 1999, 1011, 4013, 2078, 1011, 2022, 1037, 2158, 1997, 2307, 8066, 1998, 7477, 2007, 2019, 3707, 2097, 1998, 11259, 2295, 6770, 9463, 4757, 3267, 2040, 2113, 1999, 2054, 1996, 2307, 2791, 1997, 1037, 2332, 8676, 1998, 2147, 2041, 1011, 4013, 2078, 1011, 2203, 21442, 6895, 10895, 1998, 4895, 11020, 21531, 21227, 1996, 2214, 16708, 3804, 1998, 4175, 2031, 2035, 3413, 2185, 3272, 1996, 3804, 1997, 14246, 2063, 2487, 2021, 1996, 3804, 1997, 8917, 2487, 1998, 14246, 2063, 2487, 2907, 22771, 10439, 5162, 3351, 1998, 2045, 2022, 1037, 22609, 11760, 2040, 2031, 4982, 2039, 2076, 1996, 2162, 3097, 1998, 2942, 1998, 2022, 8627, 2011, 1996, 7927, 13706, 2213, 1997, 2711, 2487, 2035, 2122, 3110, 2008, 14246, 2063, 2487, 2022, 1011, 4013, 2078, 1011, 3019, 22277, 8917, 2487, 2114, 1011, 4013, 2078, 1011, 1999, 2054, 2022, 2655, 1996, 2223, 1997, 1996, 2270, 2204, 2007, 1011, 4013, 2078, 1011, 2219, 2567, 1996, 3804, 1997, 14246, 2063, 2487, 1998, 4175, 2711, 2487, 2040, 2022, 2113, 2004, 2711, 2487, 7782, 1996, 2365, 1997, 3804, 8917, 2487, 2012, 1011, 4013, 2078, 1011, 2132, 14246, 2063, 2487, 2022, 2941, 4154, 2011, 2711, 2487, 1999, 1996, 2645, 1997, 14246, 2063, 2487, 2021, 1011, 4013, 2078, 1011, 9530, 18886, 3726, 2061, 12266, 2135, 2000, 3338, 2039, 1996, 2223, 2011, 4872, 2000, 2169, 2266, 1998, 2011, 2061, 2860, 4487, 14416, 10992, 2426, 1011, 4013, 2078, 1011, 2008, 1011, 4013, 2078, 1011, 2203, 2011, 2468, 2062, 3928, 2084, 2077, 2711, 2487, 7782, 2006, 1996, 2331, 1997, 2711, 2487, 1996, 2204, 1999, 2711, 2487, 7782, 9510, 2000, 1996, 11068, 1997, 2711, 2487, 1011, 4013, 2078, 1011, 7323, 2062, 25314, 2135, 1996, 2933, 1997, 2433, 1037, 14246, 2063, 2487, 1997, 2711, 2487, 1998, 2031, 2130, 3246, 1997, 2022, 5454, 3750, 2030, 18979, 2140, 2487, 2174, 1011, 4013, 2078, 1011, 2031, 2000, 24939, 1011, 4013, 2078, 1011, 13738, 2011, 2191, 1011, 4013, 2078, 1011, 3040, 1997, 1996, 2406, 2029, 2112, 2711, 2487, 2013, 1996, 14246, 2063, 2487, 2007, 2023, 3193, 1011, 4013, 2078, 1011, 6855, 2711, 2487, 1999, 16393, 2013, 1011, 4013, 2078, 1011, 3954, 1037, 23927, 2365, 1997, 8917, 2487, 2040, 2022, 2196, 3497, 2000, 2417, 21564, 1011, 4013, 2078, 1011, 2711, 2487, 2031, 2022, 22490, 2011, 2711, 2487, 1996, 2564, 1997, 2711, 2487, 1997, 14246, 2063, 2487, 1998, 12960, 2332, 1997, 14246, 2063, 2487, 1998, 2031, 3413, 2013, 1011, 4013, 2078, 1011, 2000, 1011, 4013, 2078, 1011, 2684, 2040, 2031, 5914, 1996, 2379, 8215, 1999, 1996, 3287, 2240, 1996, 4175, 1997, 14246, 2063, 2487, 2021, 2711, 2487, 7782, 4895, 29427, 2135, 15126, 1996, 3804, 9527, 3298, 2041, 1996, 26410, 8215, 14246, 2063, 2487, 2365, 1997, 2711, 2487, 12507, 2022, 2006, 1996, 3422, 2005, 2296, 7561, 1997, 2711, 2487, 1998, 7887, 2061, 2860, 5473, 1999, 1011, 4013, 2078, 1011, 4130, 2823, 1011, 4013, 2078, 1011, 3067, 15044, 2205, 2574, 2004, 2043, 1011, 4013, 2078, 1011, 2031, 2941, 2404, 1011, 4013, 2078, 1011, 2046, 2711, 2487, 2022, 2373, 2011, 3942, 1011, 4013, 2078, 1011, 2012, 2711, 2487, 2012, 1996, 2200, 2617, 2043, 1011, 4013, 2078, 1011, 12495, 25556, 2100, 2031, 8627, 1996, 2103, 1997, 2711, 2487, 2000, 4125, 1999, 10073, 2114, 1011, 4013, 2078, 1011, 3387, 2019, 9698, 1997, 1996, 3804, 1998, 1011, 4013, 2078, 1011, 2069, 4965, 1011, 4013, 2078, 1011, 4071, 2011, 11268, 8557, 4872, 1998, 2011, 4681, 2711, 2487, 1999, 1037, 2087, 9576, 6215, 1997, 2711, 2487, 2021, 2044, 2023, 1011, 4013, 2078, 1011, 14046, 3653, 3567, 4014, 1011, 4013, 2078, 1011, 2507, 3595, 2490, 2000, 1996, 25276, 3372, 1997, 14246, 2063, 2487, 1998, 20014, 27611, 2007, 1996, 4496, 2361, 2487, 2040, 2022, 2411, 2012, 3277, 2007, 1996, 4496, 2361, 2487, 15358, 13355, 1998, 5268, 2100, 1999, 2711, 2487, 20505, 18555, 2158, 2013, 3183, 1996, 2158, 1997, 2711, 2487, 10073, 1999, 7927, 1997, 1011, 4013, 2078, 1011, 2280, 4496, 2361, 2487, 2935, 12507, 2711, 2487, 1997, 2711, 2487, 2022, 2567, 1999, 2375, 2031, 2933, 2007, 1011, 4013, 2078, 1011, 2019, 5274, 1997, 14246, 2063, 2487, 1998, 2407, 1997, 1996, 14246, 2063, 2487, 1998, 1999, 2941, 2892, 1996, 2712, 2007, 1037, 21459, 3677, 2021, 2096, 2711, 2487, 2022, 4652, 2013, 3693, 1011, 4013, 2078, 1011, 2011, 1996, 6859, 1997, 14246, 2063, 2487, 1037, 2103, 1999, 4707, 2007, 2711, 2487, 1997, 14246, 2063, 2487, 3113, 2711, 2487, 2006, 1996, 2958, 1997, 21877, 2278, 15549, 19393, 1998, 2011, 6187, 5558, 3917, 2100, 27748, 1998, 19238, 1997, 2711, 2487, 9530, 18886, 3726, 2000, 13984, 1011, 4013, 2078, 1011, 2000, 4287, 2188, 1011, 4013, 2078, 1011, 2390, 2302, 4894, 1037, 6271, 2008, 3116, 2022, 1037, 8025, 7185, 2487, 1037, 4799, 8803, 2066, 1037, 3748, 6841, 2022, 7980, 2022, 14908, 1999, 1996, 2690, 1997, 1996, 2958, 2083, 2029, 1996, 7185, 2487, 2332, 3610, 7185, 2487, 2178, 2711, 2487, 2022, 1996, 4206, 1998, 8502, 2158, 2556, 1998, 21459, 2135, 20426, 14246, 2063, 2487, 2022, 2235, 1998, 2812, 2298, 1998, 8416, 2063, 1999, 2019, 2214, 2630, 4848, 2007, 1037, 6045, 29460, 2007, 2210, 2599, 2368, 3746, 1997, 1996, 3002, 2021, 1011, 4013, 2078, 1011, 5744, 4416, 3243, 9462, 1996, 10634, 24823, 1997, 2711, 2487, 1998, 1999, 1996, 2812, 2051, 1996, 4496, 2361, 2487, 5268, 2022, 9831, 1998, 3499, 1011, 4013, 2078, 1011, 2440, 7370, 1996, 4496, 2361, 2487, 2022, 9975, 3422, 2000, 4652, 2035, 26260, 2061, 8301, 10270, 18083, 2100, 2079, 14246, 2063, 2487, 6133, 2008, 2711, 2487, 9619, 2000, 2191, 3521, 1998, 2709, 2188, 1996, 2991, 1997, 2711, 2487, 7782, 2711, 2487, 2031, 2468, 4372, 23395, 1999, 2116, 7669, 1011, 4013, 2078, 1011, 2022, 1037, 8401, 8665, 2158, 2172, 18959, 1998, 1011, 4013, 2078, 1011, 3099, 1999, 2711, 2487, 2022, 9205, 6355, 2158, 2040, 2224, 2296, 3653, 18209, 2005, 8336, 2588, 21916, 1996, 3099, 1997, 2711, 2487, 2031, 2022, 2404, 2000, 2331, 1999, 1037, 2759, 4125, 4681, 2011, 8917, 2487, 1999, 1998, 1996, 2158, 1997, 2711, 2487, 1011, 4013, 2078, 1011, 5333, 2112, 1997, 1996, 7680, 2005, 2029, 1996, 2406, 2031, 2022, 16393, 1998, 10073, 2114, 8917, 2487, 2022, 4297, 4221, 2011, 14246, 2063, 2487, 2000, 3693, 1011, 4013, 2078, 1011, 2147, 1035, 1997, 1035, 2396, 2487, 2191, 2691, 3426, 2007, 1011, 4013, 2078, 1011, 1999, 7185, 2487, 2307, 7465, 12604, 3385, 1998, 2711, 2487, 1998, 2035, 1011, 4013, 2078, 1011, 9610, 10175, 2854, 2022, 3786, 2011, 1996, 4496, 2361, 2487, 12694, 2386, 2021, 1011, 4013, 2078, 1011, 5245, 2006, 1996, 2162, 2711, 2487, 1996, 2708, 2103, 1997, 2711, 2487, 2031, 4125, 2114, 1011, 4013, 2078, 1011, 1998, 1011, 4013, 2078, 1011, 2022, 11741, 3351, 1011, 4013, 2078, 1011, 2006, 2051, 2487, 2599, 1996, 4496, 2361, 2487, 2000, 15804, 1996, 2237, 2011, 2991, 1999, 2051, 2487, 2006, 1996, 2022, 11741, 4590, 3409, 2045, 2022, 1037, 6659, 2954, 1996, 4496, 2361, 2487, 2022, 2799, 1998, 2044, 2146, 3945, 1996, 11547, 1997, 3804, 2711, 2487, 2022, 2424, 1999, 1037, 7708, 4770, 6167, 20228, 20824, 1998, 3104, 2007, 2668, 1011, 4013, 2078, 1011, 2022, 1996, 2197, 1997, 1996, 3287, 2240, 1997, 2711, 2487, 1998, 1011, 4013, 2078, 1011, 2307, 6664, 3338, 2039, 2007, 1011, 4013, 2078, 1011, 2331, 1011, 4013, 2078, 1011, 2069, 2775, 2711, 2487, 2079, 2025, 22490, 1996, 4496, 2361, 2487, 3804, 9527, 4496, 1996, 2221, 2295, 2087, 1997, 1996, 10882, 12879, 1999, 1996, 2659, 2406, 2029, 2071, 18855, 2000, 1996, 2931, 2240, 2022, 1011, 4013, 2078, 1011, 6151, 2483, 29462, 4664, 14246, 2063, 2487, 3046, 2011, 16130, 2039, 1011, 4013, 2078, 1011, 3395, 2000, 2486, 1011, 4013, 2078, 1011, 2046, 1037, 3510, 2007, 1011, 4013, 2078, 1011, 2365, 2711, 2487, 2021, 1011, 4013, 2078, 1011, 5466, 1011, 4013, 2078, 1011, 2006, 1996, 3860, 1997, 8917, 2487, 1998, 5914, 2711, 2487, 2365, 1997, 1996, 3750, 2711, 2487, 4287, 1011, 4013, 2078, 1011, 3675, 2455, 2000, 18370, 1996, 2373, 1997, 1011, 4013, 2078, 1011, 2155, 14246, 2063, 2487, 2022, 2188, 2231, 14246, 2063, 2487, 2022, 2291, 1997, 22422, 1997, 1996, 7015, 2175, 2006, 2035, 2023, 2051, 1011, 4013, 2078, 1011, 9517, 10626, 2022, 1997, 2659, 4182, 2711, 2487, 1011, 4013, 2078, 1011, 2711, 2487, 2022, 1996, 2158, 1011, 4013, 2078, 1011, 2087, 3404, 1011, 4013, 2078, 1011, 10427, 10424, 16377, 2140, 1011, 4013, 2078, 1011, 5450, 3914, 1998, 19313, 2389, 1011, 4013, 2078, 1011, 2022, 14436, 5223, 1998, 29245, 1998, 1011, 4013, 2078, 1011, 2468, 7887, 2062, 8618, 10027, 1998, 21442, 6895, 3238, 2216, 2040, 2991, 2104, 1011, 4013, 2078, 1011, 28606, 2022, 17727, 6935, 2239, 1999, 3707, 7980, 2030, 2404, 2000, 2331, 1998, 1996, 2062, 22609, 2155, 2107, 2004, 1996, 8917, 2487, 1997, 2849, 8490, 18357, 2022, 7438, 2007, 25966, 3993, 18976, 2021, 1011, 4013, 2078, 1011, 2022, 2025, 2215, 2239, 4808, 1011, 4013, 2078, 1011, 2552, 2006, 1037, 3180, 2291, 1997, 2139, 20110, 1996, 2375, 3238, 11760, 1998, 3623, 8917, 2487, 2011, 3288, 1996, 2373, 1997, 1996, 2103, 2830, 2011, 3404, 2005, 3860, 2000, 1996, 3233, 2390, 15897, 1997, 10887, 8917, 2487, 1998, 4496, 2361, 2487, 1998, 2011, 3828, 2769, 2011, 2023, 2812, 1011, 4013, 2078, 1011, 2022, 2583, 2000, 5309, 1996, 2221, 1997, 8840, 2278, 2487, 1998, 14246, 2063, 2487, 2013, 1996, 2332, 1997, 14246, 2063, 2487, 2947, 2191, 1996, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 8880, 1998, 2006, 2195, 6686, 1011, 4013, 2078, 1011, 2191, 1011, 4013, 2078, 1011, 8917, 2487, 2954, 1011, 4013, 2078, 1011, 2645, 2612, 1997, 1996, 4690, 1997, 1011, 4013, 2078, 1011, 5000, 1011, 4013, 2078, 1011, 2444, 1999, 1996, 3317, 1997, 6904, 2278, 2487, 3457, 2011, 1996, 27917, 2396, 1997, 23050, 1998, 6039, 2007, 10887, 4496, 2361, 2487, 11024, 1997, 1011, 4013, 2078, 1011, 3457, 3183, 1011, 4013, 2078, 1011, 9544, 2004, 8291, 2000, 1011, 4013, 2078, 1011, 2219, 7015, 1011, 4013, 2078, 1011, 2022, 17003, 2135, 19657, 2007, 1011, 4013, 2078, 1011, 7015, 2021, 1996, 17689, 1998, 5272, 2711, 2487, 2040, 2031, 2175, 2058, 2000, 1011, 4013, 2078, 1011, 2013, 2711, 2487, 3193, 1011, 4013, 2078, 1011, 2004, 1996, 2204, 1998, 2583, 1997, 2332, 1011, 4013, 2078, 1011, 2079, 2172, 2000, 5326, 3119, 1998, 9922, 5335, 1996, 2103, 6469, 1996, 2118, 1998, 2022, 1999, 3606, 1996, 2030, 18979, 2140, 2487, 2332, 2144, 2711, 2487, 2040, 2031, 2151, 2613, 3168, 1997, 17689, 9650, 2021, 2295, 1996, 20934, 27172, 2121, 16215, 17597, 2104, 1011, 4013, 2078, 1011, 1998, 1996, 2375, 3238, 7015, 2022, 2139, 20110, 1996, 2110, 1997, 1996, 14539, 2022, 2025, 5335, 16708, 2157, 2811, 4600, 2006, 1011, 4013, 2078, 1011, 1998, 1011, 4013, 2078, 1011, 2022, 2210, 2092, 2084, 9576, 23088, 2091, 2011, 2711, 2487, 17607, 2011, 1011, 4013, 2078, 1011, 8917, 2487, 1998, 14246, 2063, 2487, 2031, 5587, 2172, 2000, 1996, 4496, 2361, 2487, 12078, 1011, 4013, 2078, 1011, 2031, 2663, 2067, 8917, 2487, 1011, 4013, 2078, 1011, 2031, 15126, 1996, 11068, 1998, 2221, 1997, 2711, 2487, 1011, 4013, 2078, 1011, 2031, 4965, 8840, 2278, 2487, 1011, 4013, 2078, 1011, 2197, 7654, 2022, 14246, 2063, 2487, 1996, 2030, 18979, 2140, 2487, 17076, 6777, 2378, 2155, 4088, 2007, 14246, 2063, 2487, 1996, 2365, 1997, 2711, 2487, 2031, 2196, 9510, 1999, 5114, 1037, 22849, 1999, 14246, 2063, 2487, 2295, 1011, 4013, 2078, 1011, 4562, 1996, 2548, 2516, 1011, 4013, 2078, 1011, 2907, 2174, 1996, 4461, 10882, 12879, 1997, 2711, 2487, 1998, 2711, 2487, 3005, 2388, 2031, 2022, 1997, 2023, 2155, 6855, 2013, 1011, 4013, 2078, 1011, 7185, 2487, 2567, 2711, 2487, 1998, 2711, 2487, 2008, 2711, 2487, 2323, 2022, 2022, 4226, 8988, 2000, 1011, 4013, 2078, 1011, 2612, 1997, 3413, 2000, 2711, 2487, 2022, 7631, 1996, 3804, 1997, 2711, 2487, 1996, 2332, 1997, 14246, 2063, 2487, 2022, 23166, 15628, 4175, 1997, 2711, 2487, 1998, 2295, 1996, 2221, 2022, 2025, 3193, 2004, 2112, 1997, 1996, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 2022, 8134, 7185, 2487, 2007, 1011, 4013, 2078, 1011, 1037, 2664, 2307, 7654, 2022, 2191, 2574, 2044, 14246, 2063, 2487, 2022, 2331, 1999, 1996, 2307, 4496, 2361, 2487, 11068, 1997, 14246, 2063, 2487, 2991, 2000, 1037, 2931, 2711, 2487, 1998, 1996, 4769, 1997, 14246, 2063, 2487, 2022, 2684, 1996, 3203, 1997, 2711, 2487, 2040, 2022, 11315, 1997, 1996, 8391, 3653, 3567, 4014, 2000, 5851, 1996, 2192, 1997, 1996, 20020, 2005, 1011, 4013, 2078, 1011, 2567, 2711, 2487, 2947, 1996, 4410, 1997, 14246, 2063, 2487, 2031, 2011, 5309, 9187, 2030, 12839, 6855, 2035, 1996, 2307, 16708, 2110, 2008, 2191, 2039, 1996, 2406, 2090, 1996, 4496, 2361, 2487, 3149, 1998, 1996, 14246, 2063, 2487, 2021, 2169, 2145, 3961, 1037, 3584, 2110, 2007, 2367, 2375, 1998, 7661, 1998, 1037, 3584, 8917, 2487, 1999, 2169, 2000, 4236, 2375, 1998, 2000, 2552, 2004, 1037, 2457, 1997, 3425, 10381, 1996, 4496, 2361, 2487, 2162, 3049, 1997, 2711, 2487, 2013, 10616, 2012, 2874, 2044, 2874, 2006, 1011, 4013, 2078, 1011, 2219, 3675, 2174, 1996, 4496, 2361, 2487, 2332, 2022, 2085, 2000, 2735, 2000, 2898, 3959, 1997, 9187, 6917, 2362, 2007, 14246, 2063, 2487, 2031, 4965, 2013, 2332, 2711, 2487, 2035, 1996, 4366, 1997, 1996, 8917, 2487, 1997, 14246, 2063, 2487, 2426, 2122, 2022, 2421, 1037, 4366, 2000, 14246, 2063, 2487, 2022, 2365, 2711, 2487, 1037, 15784, 1998, 8467, 14804, 2022, 8915, 27718, 2011, 1996, 6664, 1997, 2312, 8813, 1998, 1037, 2986, 2390, 2000, 4952, 2000, 1996, 27577, 1997, 2019, 4496, 2361, 2487, 20014, 27611, 2099, 2711, 2487, 1997, 14246, 2063, 2487, 1998, 2404, 2830, 2122, 3653, 29048, 2947, 4088, 1037, 2162, 2029, 2197, 3053, 2004, 2146, 2004, 3058, 2487, 2162, 2007, 14246, 2063, 2487, 2021, 1011, 4013, 2078, 1011, 2022, 1037, 2162, 1997, 14974, 2612, 1997, 1037, 2162, 1997, 2969, 4721, 2711, 2487, 2892, 1996, 8840, 2278, 2487, 1999, 2233, 1996, 2878, 3091, 1997, 14246, 2063, 2487, 2302, 4559, 1998, 2022, 4410, 2012, 14246, 2063, 2487, 2096, 1011, 4013, 2078, 1011, 2548, 2155, 2019, 18102, 12446, 23416, 2013, 1996, 2332, 1997, 14246, 2063, 2487, 10574, 2046, 14246, 2063, 2487, 1998, 2655, 2006, 14246, 2063, 2487, 2005, 2393, 2021, 1996, 16021, 9890, 3372, 6635, 3258, 1997, 1996, 4496, 2361, 2487, 5268, 2100, 3426, 1996, 2111, 2000, 4125, 2114, 1011, 4013, 2078, 1011, 1998, 2043, 2711, 2487, 2709, 1011, 4013, 2078, 1011, 2022, 2022, 13462, 2012, 8917, 2487, 2011, 1037, 2307, 2223, 1997, 4496, 2361, 2487, 2058, 3183, 1011, 4013, 2078, 1011, 5114, 1037, 3143, 3377, 2235, 1998, 26136, 2100, 2295, 1011, 4013, 2078, 1011, 2022, 1011, 4013, 2078, 1011, 2954, 2066, 1037, 7006, 1998, 4025, 3243, 18708, 2011, 1996, 12098, 26797, 2099, 1997, 4337, 1996, 4496, 2361, 2487, 8111, 2474, 2711, 2487, 2468, 1037, 6011, 15185, 2426, 1996, 4496, 2361, 2487, 2711, 2487, 19046, 2174, 2000, 4604, 2151, 4425, 2030, 23895, 2000, 1996, 8427, 1011, 4013, 2078, 1011, 2031, 2681, 2369, 1011, 4013, 2078, 1011, 1999, 14246, 2063, 2487, 1998, 1011, 4013, 2078, 1011, 2035, 2566, 4509, 2104, 2215, 15556, 1998, 1996, 4690, 1997, 1996, 4496, 2361, 2487, 1011, 4013, 2078, 1011, 2022, 19960, 17570, 2178, 5590, 2043, 1011, 4013, 2078, 1011, 4894, 1011, 4013, 2078, 1011, 2132, 2114, 1996, 2327, 1997, 1037, 7086, 1998, 3280, 1999, 3049, 1997, 2711, 2487, 5542, 2711, 2487, 5914, 1011, 4013, 2078, 1011, 7794, 1998, 2947, 4652, 14246, 2063, 2487, 2013, 2153, 2112, 2013, 1996, 4410, 14246, 2063, 2487, 2025, 2069, 9510, 2000, 1996, 17076, 6777, 2378, 2157, 2000, 14246, 2063, 2487, 2021, 2083, 1011, 4013, 2078, 1011, 7133, 1011, 4013, 2078, 1011, 3193, 1011, 4013, 2078, 1011, 2004, 8215, 1997, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 2022, 2711, 2487, 2564, 2000, 2008, 3804, 1997, 14246, 2063, 2487, 2040, 2031, 2022, 4028, 2011, 2711, 2487, 1011, 4013, 2078, 1011, 2196, 5083, 2521, 2084, 2000, 14246, 2063, 2487, 3005, 7806, 2191, 1011, 4013, 2078, 1011, 3040, 1997, 14246, 2063, 2487, 2029, 1011, 4013, 2078, 1011, 2907, 2005, 1996, 2307, 2112, 1997, 1011, 4013, 2078, 1011, 5853, 2021, 2044, 1037, 2096, 1996, 4496, 2361, 2487, 2332, 2711, 2487, 5993, 2007, 1011, 4013, 2078, 1011, 2000, 5466, 2058, 1996, 3426, 1997, 1996, 15140, 2548, 2155, 1997, 14246, 2063, 2487, 1998, 11443, 2008, 14246, 2063, 2487, 2090, 1011, 4013, 2078, 1011, 2711, 2487, 4604, 1037, 8235, 2390, 2000, 2202, 6664, 1997, 1011, 4013, 2078, 1011, 3745, 2021, 1996, 5391, 1997, 2169, 4664, 2031, 2025, 2022, 9375, 1998, 1996, 4496, 2361, 2487, 1998, 4496, 2361, 2487, 10123, 4088, 1037, 2162, 2130, 2096, 1011, 4013, 2078, 1011, 2332, 2022, 2145, 7438, 2007, 7185, 2487, 2178, 1996, 3265, 4496, 2361, 2487, 5000, 2079, 8235, 18077, 2005, 5262, 1011, 4013, 2078, 1011, 2022, 1996, 2051, 1997, 1996, 2708, 20593, 1997, 5470, 26336, 9610, 10175, 2854, 1037, 5000, 1997, 2711, 2487, 2171, 2711, 2487, 2655, 1996, 22518, 1998, 18676, 5000, 1998, 6225, 2011, 2767, 1998, 22277, 2021, 1996, 4496, 2361, 2487, 2022, 2104, 2711, 2487, 2655, 1996, 2307, 2952, 1998, 2044, 1996, 2645, 1997, 8292, 3089, 26745, 2721, 1998, 1996, 11721, 3089, 20011, 3630, 3298, 1996, 4496, 2361, 2487, 2041, 1997, 14246, 2063, 2487, 2295, 1996, 2162, 3613, 1999, 14246, 2063, 2487, 1996, 4151, 2223, 1011, 4013, 2078, 1011, 2022, 2019, 2287, 1997, 2223, 1996, 4496, 2361, 2487, 5223, 4496, 2361, 2487, 1998, 4496, 2361, 2487, 2119, 11455, 2022, 14678, 2433, 5257, 2426, 1011, 4013, 2078, 1011, 1998, 2007, 3097, 2373, 2114, 29221, 4148, 2000, 2022, 1996, 2844, 1996, 2708, 1997, 2122, 2022, 2655, 1996, 4151, 2223, 2138, 1011, 4013, 2078, 1011, 2022, 2433, 2011, 4831, 2711, 2487, 2040, 4009, 2046, 1011, 4013, 2078, 1011, 2711, 2487, 2059, 2132, 1997, 1996, 4496, 2361, 2487, 3400, 2711, 2487, 1997, 14246, 2063, 2487, 1998, 2711, 2487, 1997, 14246, 2063, 2487, 1996, 4496, 2361, 2487, 10123, 2022, 2886, 1999, 14246, 2063, 2487, 1998, 2295, 1011, 4013, 2078, 1011, 5114, 1996, 2645, 1997, 14246, 2063, 2487, 1999, 1011, 4013, 2078, 1011, 2022, 2007, 1996, 3279, 1997, 1011, 4013, 2078, 1011, 2236, 2711, 2487, 1997, 2711, 2487, 3005, 2331, 3710, 2004, 2019, 8016, 2000, 2711, 2487, 1997, 14246, 2063, 2487, 2005, 2275, 2039, 1037, 4366, 2000, 1996, 14246, 2063, 2487, 1997, 2711, 2487, 1011, 4013, 2078, 1011, 23626, 2135, 13984, 2711, 2487, 2000, 4681, 1011, 4013, 2078, 1011, 1999, 1996, 2886, 2011, 2907, 2041, 1996, 15784, 2801, 1997, 2175, 2006, 2000, 12452, 14246, 2063, 2487, 1998, 2096, 7185, 2487, 10123, 1997, 4496, 2361, 2487, 2022, 2886, 2711, 2487, 1011, 4013, 2078, 1011, 2455, 2012, 8917, 2487, 1998, 2202, 2711, 2487, 1998, 2711, 2487, 1996, 4496, 2361, 2487, 2486, 2022, 2012, 1996, 2168, 2051, 2022, 5252, 2041, 1997, 14246, 2063, 2487, 2174, 2043, 2711, 2487, 2031, 2022, 2202, 1998, 1996, 4496, 2361, 2487, 2633, 3298, 2041, 1997, 14246, 2063, 2487, 1996, 4831, 1998, 2332, 2040, 2031, 5114, 1011, 4013, 2078, 1011, 2203, 2681, 2711, 2487, 2000, 2954, 1011, 4013, 2078, 1011, 2219, 2645, 1011, 4013, 2078, 1011, 2947, 2022, 19653, 2000, 2191, 3521, 2507, 1011, 4013, 2078, 1011, 2402, 2905, 2711, 2487, 2004, 2030, 18979, 2140, 2487, 2564, 2000, 14246, 2063, 2487, 2021, 2008, 2332, 2058, 4654, 8743, 1011, 4013, 2078, 1011, 2012, 1996, 19032, 1998, 3280, 3058, 2487, 2044, 1996, 3510, 1999, 2076, 2023, 5853, 1996, 5949, 1997, 2668, 1998, 8813, 2006, 2162, 1997, 8210, 16290, 2022, 25966, 3993, 1998, 1996, 2406, 2031, 2022, 4600, 4171, 2021, 1037, 8235, 5268, 2100, 2031, 2022, 3345, 2039, 1998, 2120, 18736, 2031, 2172, 3623, 1996, 2332, 2295, 2302, 10107, 2172, 2293, 2022, 2061, 19045, 1999, 5450, 2008, 1011, 4013, 2078, 1011, 2022, 1037, 8837, 1998, 2022, 2655, 1996, 2269, 1997, 1996, 2111, 1011, 4013, 2078, 1011, 2030, 18979, 2140, 2487, 2564, 2711, 2487, 2022, 2019, 6581, 1998, 2152, 24462, 2450, 2040, 2562, 1996, 2457, 1997, 14246, 2063, 2487, 1999, 1037, 2092, 2110, 2084, 2412, 2077, 2030, 2144, 3049, 1997, 2711, 2487, 2681, 7185, 2487, 2684, 1996, 6422, 1997, 3183, 2711, 2487, 4287, 14246, 2063, 2487, 2000, 1011, 4013, 2078, 1011, 3287, 8215, 2711, 2487, 1997, 17076, 7140, 2140, 1999, 2063, 2711, 2487, 2031, 2022, 2172, 13642, 22573, 2000, 1996, 2674, 2021, 14246, 2063, 2487, 2360, 1011, 4013, 2078, 1011, 2562, 1011, 4013, 2078, 1011, 8000, 2005, 1011, 4013, 2078, 1011, 2219, 4937, 1998, 2507, 1011, 4013, 2078, 1011, 2684, 1998, 1011, 4013, 2078, 1011, 11068, 2000, 2711, 2487, 2004, 2574, 2004, 2711, 2487, 2022, 2757, 2711, 2487, 2022, 7185, 2487, 1997, 1996, 15784, 6270, 1998, 2087, 11454, 2075, 1997, 4496, 2361, 2487, 1999, 2755, 1011, 4013, 2078, 1011, 2022, 2019, 4654, 27609, 3370, 1999, 2296, 2126, 1997, 1996, 2120, 2839, 1998, 2947, 2468, 1037, 2120, 5394, 2172, 2058, 18098, 15593, 2063, 1011, 4013, 2078, 1011, 2012, 2320, 10663, 2000, 8980, 14246, 2063, 2487, 1998, 2044, 2892, 1996, 8840, 2278, 2487, 8087, 2019, 2390, 1997, 4496, 2361, 2487, 10123, 2040, 2031, 2022, 10887, 2000, 6985, 1996, 4496, 2361, 2487, 11068, 2006, 1996, 2492, 1997, 2711, 2487, 2031, 2000, 2954, 1037, 7143, 2645, 2007, 1011, 4013, 2078, 1011, 2044, 2029, 1011, 4013, 2078, 1011, 3426, 2711, 2487, 2000, 12931, 1011, 4013, 2078, 1011, 5000, 2295, 4496, 2361, 2487, 2332, 2022, 2360, 2000, 2022, 4562, 5000, 1999, 5114, 1996, 3377, 2058, 2122, 22146, 2040, 2031, 2022, 2718, 5886, 3406, 9266, 2213, 25018, 1011, 4013, 2078, 1011, 2330, 2005, 1011, 4013, 2078, 1011, 1037, 2126, 2046, 14246, 2063, 2487, 1998, 2031, 2035, 14246, 2063, 2487, 2012, 1011, 4013, 2078, 1011, 3329, 1996, 4831, 2711, 2487, 3113, 1011, 4013, 2078, 1011, 2012, 14102, 1998, 1037, 16557, 4017, 2202, 2173, 2011, 2029, 1996, 4496, 2361, 2487, 2277, 2468, 2062, 4498, 3395, 2000, 1996, 4831, 2096, 1999, 2709, 2035, 15694, 2022, 2507, 2039, 2000, 1996, 4410, 1996, 3466, 2022, 2574, 2156, 1999, 1996, 3623, 7897, 1997, 1996, 11646, 1998, 2111, 2711, 2487, 3288, 2188, 2013, 2023, 5590, 2172, 5510, 2005, 4496, 2361, 2487, 2396, 1998, 3906, 1998, 2035, 3043, 1997, 27745, 1998, 2030, 18442, 3372, 2191, 2307, 5082, 2013, 2023, 2051, 1996, 2307, 4496, 2361, 2487, 3040, 2147, 2005, 1011, 4013, 2078, 1011, 2711, 2487, 6773, 2070, 1997, 1011, 4013, 2078, 1011, 2087, 3376, 3861, 2005, 1011, 4013, 2078, 1011, 1998, 2711, 2487, 2272, 2000, 1011, 4013, 2078, 1011, 2457, 1998, 2045, 3280, 1999, 1011, 4013, 2078, 1011, 2849, 1011, 4013, 2078, 1011, 4186, 2926, 2008, 1997, 2711, 2487, 2022, 17003, 2135, 3376, 1999, 1996, 2047, 4438, 2806, 2655, 1996, 8028, 2307, 4138, 2791, 1998, 11867, 7770, 26797, 2099, 5853, 2012, 2457, 1998, 2275, 2125, 1011, 4013, 2078, 1011, 3653, 29048, 2000, 7472, 1998, 9610, 10175, 2854, 4083, 1998, 6566, 2926, 4556, 3623, 2172, 1998, 1996, 2332, 2022, 2905, 2711, 2487, 1997, 2711, 2487, 2022, 2019, 6581, 1998, 3811, 8754, 21466, 2450, 2021, 2130, 1011, 4013, 2078, 1011, 3015, 6011, 2008, 1996, 2878, 4309, 1997, 3110, 2022, 16668, 20392, 2043, 2025, 13925, 2711, 2487, 1996, 9187, 1997, 14246, 2063, 2487, 2191, 14246, 2063, 2487, 1996, 2307, 2373, 1999, 8917, 2487, 2021, 1011, 4013, 2078, 1011, 2332, 2022, 2574, 2000, 2424, 1037, 10478, 1998, 3161, 6538, 1996, 2214, 11150, 2090, 14246, 2063, 2487, 1998, 2711, 2487, 2153, 8300, 2711, 2487, 1996, 2684, 1997, 2711, 2487, 7782, 2031, 5914, 2711, 2487, 1997, 14246, 2063, 2487, 1998, 2332, 1997, 1996, 4496, 2361, 2487, 2295, 2196, 2941, 4410, 3750, 1011, 4013, 2078, 1011, 2365, 2711, 2487, 5914, 2711, 2487, 1996, 2684, 1997, 2711, 2487, 1998, 20020, 1997, 14246, 2063, 2487, 2040, 4558, 1011, 4013, 2078, 1011, 3168, 2013, 9940, 2006, 2711, 2487, 2022, 4895, 7292, 2135, 2331, 1998, 2947, 1996, 3622, 8215, 2000, 14246, 2063, 2487, 1998, 1996, 14246, 2063, 2487, 2022, 2711, 2487, 1011, 4013, 2078, 1011, 3449, 2094, 2365, 2006, 1996, 2331, 1997, 2711, 2487, 1999, 2711, 2487, 16599, 1011, 4013, 2078, 1011, 2000, 1996, 20374, 2004, 3750, 2021, 8246, 1999, 8741, 1997, 27748, 2711, 2487, 2022, 5454, 1998, 2013, 2008, 2051, 2711, 2487, 7323, 1011, 4013, 2078, 1011, 2007, 4895, 21456, 7741, 11150, 1996, 4366, 2000, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 2022, 20687, 2711, 2487, 4604, 10123, 2000, 11494, 14246, 2063, 2487, 1998, 2022, 3582, 1011, 4013, 2078, 1011, 1011, 4013, 2078, 1011, 2021, 1996, 2087, 3928, 1997, 2035, 1011, 4013, 2078, 1011, 7015, 1996, 3804, 1997, 15477, 12294, 1997, 14246, 2063, 2487, 2031, 2022, 7344, 3686, 2011, 2019, 21321, 2566, 22327, 11657, 2006, 1011, 4013, 2078, 1011, 1999, 7927, 1997, 1996, 2332, 2022, 2388, 1998, 5532, 2000, 1996, 4496, 2361, 2487, 3749, 2000, 6509, 1011, 4013, 2078, 1011, 1998, 1996, 4496, 2361, 2487, 1999, 11443, 14246, 2063, 2487, 2096, 1011, 4013, 2078, 1011, 3914, 2005, 1011, 4013, 2078, 1011, 2711, 2487, 1011, 4013, 2078, 1011, 5532, 3258, 17666, 2121, 2711, 2487, 2013, 4604, 2490, 2000, 1996, 10123, 1999, 14246, 2063, 2487, 2040, 2022, 2486, 2000, 2711, 2487, 2022, 5607, 1999, 1996, 8560, 2096, 6985, 1996, 4373, 3457, 1998, 2022, 2681, 2000, 3280, 2104, 1037, 3392, 1996, 27917, 6225, 2022, 2265, 1011, 4013, 2078, 1011, 2011, 1996, 4496, 2361, 2487, 2021, 2043, 15477, 2272, 2379, 1011, 4013, 2078, 1011, 1011, 4013, 2078, 1011, 7226, 1011, 4013, 2078, 1011, 2202, 12063, 2025, 2006, 7185, 2487, 2040, 2022, 3280, 2004, 1037, 2995, 5268, 2021, 2006, 1011, 4013, 2078, 1011, 2004, 1037, 17328, 2000, 2332, 1998, 2406, 2043, 1996, 4496, 2361, 2487, 1999, 18445, 2711, 2487, 9015, 1037, 6659, 4154, 2012, 6904, 2278, 2487, 1998, 2022, 4287, 1037, 7267, 2000, 14246, 2063, 2487, 2073, 1011, 4013, 2078, 1011, 3961, 2005, 3058, 2487, 1998, 2022, 2069, 2275, 2489, 2006, 2191, 1037, 5036, 2011, 2029, 1011, 4013, 2078, 1011, 2022, 2000, 2507, 2039, 2035, 4366, 1999, 14246, 2063, 2487, 2119, 2000, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 2036, 1996, 2221, 1997, 2711, 2487, 1998, 1996, 10514, 6290, 22325, 2100, 1997, 2216, 4496, 2361, 2487, 2221, 2029, 2031, 2022, 10882, 12879, 1997, 1996, 4496, 2361, 2487, 4410, 2004, 2092, 2004, 2000, 7806, 1011, 4013, 2078, 1011, 7185, 2487, 2365, 2004, 13446, 2005, 1996, 2836, 1997, 1996, 4650, 2162, 1997, 2711, 2487, 1998, 2711, 2487, 2035, 1996, 2717, 1997, 1996, 2332, 2022, 2166, 2022, 2019, 3535, 2000, 3449, 12672, 2030, 3338, 2122, 4650, 2114, 2029, 1011, 4013, 2078, 1011, 2031, 6186, 1999, 1011, 4013, 2078, 1011, 3827, 2021, 2043, 2045, 2022, 2053, 4496, 2361, 2487, 2556, 2000, 2963, 1011, 4013, 2078, 1011, 2079, 2061, 1996, 2221, 1997, 2711, 2487, 10214, 2000, 2022, 4651, 1998, 1996, 4831, 2711, 2487, 5223, 1996, 4496, 2361, 2487, 2373, 1999, 14246, 2063, 2487, 9530, 18886, 3726, 1037, 4840, 2223, 2114, 2711, 2487, 1999, 2029, 2711, 2487, 3693, 2021, 2022, 2074, 2135, 10377, 2011, 1996, 13736, 3279, 1997, 2178, 2390, 1011, 4013, 2078, 1011, 2388, 1998, 2711, 2487, 2022, 5916, 3113, 2012, 2711, 2487, 1998, 16519, 1999, 2054, 2022, 2655, 1996, 3203, 3521, 2029, 4562, 2004, 6684, 2006, 14246, 2063, 2487, 2004, 1996, 3521, 1997, 14246, 2063, 2487, 3272, 2008, 2711, 2487, 2507, 2039, 1011, 4013, 2078, 1011, 4366, 2000, 2711, 2487, 2145, 2711, 2487, 2022, 2933, 2022, 2025, 2012, 2019, 2203, 1011, 4013, 2078, 1011, 5914, 1011, 4013, 2078, 1011, 2030, 18979, 2140, 2487, 2365, 2711, 2487, 1996, 2069, 11476, 2775, 1997, 1996, 2307, 13109, 5686, 26730, 8917, 2487, 1997, 8917, 2487, 1998, 3046, 2000, 19653, 2711, 2487, 2000, 2275, 2039, 2019, 4496, 2361, 2487, 3804, 9527, 1997, 14246, 2063, 2487, 2005, 1996, 2402, 3940, 2021, 2043, 1996, 2711, 2487, 3280, 1998, 2711, 2487, 2468, 8215, 1997, 8917, 2487, 2052, 2025, 2507, 1011, 4013, 2078, 1011, 2151, 22849, 1999, 2711, 2487, 2196, 2292, 2151, 6686, 3413, 1997, 18820, 4757, 1996, 3750, 2021, 2022, 2467, 4154, 2711, 2487, 2320, 2941, 18445, 2711, 2487, 2021, 2022, 2486, 2000, 7822, 2083, 1996, 25594, 1997, 1996, 2406, 2077, 1011, 4013, 2078, 1011, 2011, 2711, 2487, 5728, 12294, 1997, 2711, 2487, 2011, 5189, 12087, 1998, 2011, 2831, 2172, 1997, 1011, 4013, 2078, 1011, 6225, 9530, 18886, 7178, 2000, 2191, 1996, 2088, 11281, 1011, 4013, 2078, 1011, 1996, 5229, 2158, 2096, 1011, 4013, 2078, 1011, 2022, 2428, 3338, 11292, 1999, 1037, 9467, 3238, 5450, 2012, 2197, 1999, 1996, 2332, 1998, 3750, 3113, 2012, 8917, 2487, 1998, 2272, 2000, 2744, 2711, 2487, 5914, 2004, 1011, 4013, 2078, 1011, 2030, 18979, 2140, 2487, 2564, 2711, 2487, 2022, 2711, 2487, 1998, 1999, 2043, 2711, 2487, 2022, 1999, 24748, 2000, 10861, 3363, 1037, 10073, 1999, 1996, 2659, 2406, 1011, 4013, 2078, 1011, 3198, 1037, 3647, 6204, 2083, 14246, 2063, 2487, 1998, 2022, 21459, 2135, 20432, 2012, 14246, 2063, 2487, 2664, 2061, 2659, 2022, 1996, 6225, 1997, 1996, 4496, 2361, 2487, 2008, 2711, 2487, 20071, 19319, 1996, 17232, 1997, 4654, 25485, 1996, 11068, 1997, 14246, 2063, 2487, 2013, 1011, 4013, 2078, 1011, 2043, 1999, 1011, 4013, 2078, 1011, 2373, 1998, 2507, 2061, 2116, 5041, 9374, 2008, 2711, 2487, 2022, 5580, 2000, 2022, 2627, 1996, 8880, 1996, 2162, 2022, 2574, 20687, 2711, 2487, 2275, 2039, 1037, 4366, 2000, 8917, 2487, 2004, 1996, 3145, 1997, 14246, 2063, 2487, 8917, 2487, 1011, 4013, 2078, 1011, 2007, 1996, 4496, 2361, 2487, 1998, 24812, 1998, 6658, 2202, 2011, 1011, 4013, 2078, 1011, 2006, 1996, 3023, 1997, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 2022, 2941, 3288, 2046, 14246, 2063, 2487, 3835, 2022, 6402, 2021, 1996, 15364, 2907, 2041, 1998, 2004, 2711, 2487, 2031, 8917, 2487, 1011, 4013, 2078, 1011, 2007, 1996, 3750, 1998, 2031, 2202, 2711, 2487, 2191, 1037, 2345, 3521, 2012, 8917, 2487, 1999, 1011, 4013, 2078, 1011, 3280, 3058, 2487, 1999, 2711, 2487, 1011, 4013, 2078, 1011, 2069, 5788, 2365, 2711, 2487, 3582, 1996, 2168, 3343, 1996, 4125, 1997, 8330, 2964, 2022, 2085, 11443, 1996, 3400, 1999, 14246, 2063, 2487, 1998, 2711, 2487, 2202, 5056, 1997, 1996, 27865, 2029, 3338, 2041, 2090, 2711, 2487, 1998, 1996, 4496, 2361, 2487, 3159, 2000, 2886, 1996, 3750, 1998, 2191, 9187, 2408, 1996, 4496, 2361, 2487, 3675, 1011, 4013, 2078, 1011, 2655, 1011, 4013, 2078, 1011, 16167, 1997, 1996, 7044, 1997, 1996, 4496, 2361, 2487, 1998, 2223, 1011, 4013, 2078, 1011, 2007, 1011, 4013, 2078, 1011, 15126, 2711, 2487, 2029, 1996, 3804, 1997, 21980, 9191, 2135, 6985, 2043, 1996, 3750, 3046, 2000, 2128, 15166, 1011, 4013, 2078, 1011, 2023, 18634, 1997, 2711, 2487, 2022, 1996, 2030, 18979, 2140, 2487, 3535, 1997, 14246, 2063, 2487, 2000, 2191, 9187, 1999, 14246, 2063, 2487, 1998, 1996, 2927, 1997, 1037, 5049, 2090, 1996, 4496, 2361, 2487, 1998, 4496, 2361, 2487, 2111, 2029, 2031, 2175, 2006, 2000, 3058, 2487, 2044, 1996, 6859, 1037, 3058, 2487, 18415, 2022, 2191, 2076, 2029, 2711, 2487, 12897, 1011, 4013, 2078, 1011, 4410, 1011, 4013, 2078, 1011, 2567, 2031, 2022, 2525, 11322, 2000, 1996, 3400, 2021, 1011, 4013, 2078, 1011, 2365, 2711, 2487, 2468, 2332, 1997, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 1998, 2036, 22490, 1996, 2659, 2406, 1996, 4831, 2711, 2487, 2040, 2022, 1037, 11265, 9331, 10893, 5794, 1998, 5223, 1996, 4496, 2361, 2487, 3627, 4297, 4221, 2711, 2487, 1037, 15784, 5410, 2158, 2000, 3338, 1996, 18415, 1998, 4604, 7185, 2487, 2390, 2000, 14246, 2063, 2487, 2104, 1996, 3804, 1997, 21980, 2096, 2178, 2886, 1996, 8880, 1997, 8917, 2487, 6509, 2011, 1996, 2486, 1997, 1011, 4013, 2078, 1011, 2564, 2711, 2487, 3113, 2023, 2197, 2886, 2007, 2019, 2390, 3094, 2011, 1996, 3804, 1997, 8917, 2487, 1011, 4013, 2078, 1011, 5083, 2046, 14246, 2063, 2487, 1998, 2022, 11741, 3351, 2711, 2487, 1996, 4496, 2361, 2487, 2104, 1996, 12294, 1997, 2711, 2487, 2272, 2000, 15804, 1996, 2103, 1998, 2022, 12580, 4154, 1996, 12294, 1011, 4013, 2078, 1011, 2022, 2191, 7267, 1011, 4013, 2078, 1011, 7833, 1996, 5902, 2711, 2487, 2907, 2041, 2711, 2487, 2000, 1996, 2197, 1998, 2947, 2507, 1996, 2406, 2051, 2000, 8320, 2114, 1996, 18445, 2099, 1998, 21980, 2022, 9131, 1999, 24748, 2013, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 2574, 2044, 4527, 8917, 2487, 2029, 2022, 2947, 9239, 2000, 1996, 4496, 2361, 2487, 2044, 2031, 2022, 2907, 2011, 1996, 4496, 2361, 2487, 2005, 3058, 2487, 2023, 2022, 1996, 2069, 9187, 1996, 4496, 2361, 2487, 9279, 2043, 1996, 2345, 3521, 1997, 2711, 2487, 2022, 2191, 1999, 3058, 2487, 2005, 2035, 2842, 2008, 2031, 2022, 2202, 2006, 2593, 2217, 2022, 2059, 9239, 8917, 2487, 2022, 2507, 2067, 2000, 1011, 4013, 2078, 1011, 3804, 2362, 2007, 1996, 2192, 1997, 2711, 2487, 2022, 2905, 2711, 2487, 2076, 1037, 2977, 2907, 1999, 6225, 1997, 1996, 5030, 2711, 2487, 2022, 26495, 1999, 25243, 2011, 1996, 27546, 1997, 1037, 9993, 1999, 1998, 1999, 1996, 2188, 4390, 2008, 3582, 2035, 3653, 29048, 2000, 4496, 2361, 2487, 2373, 2022, 4530, 2011, 14246, 2063, 2487, 2044, 2162, 2029, 2031, 2197, 3058, 2487, 10381, 1996, 2162, 1997, 4676, 1996, 15477, 1998, 21980, 2015, 2711, 2487, 2031, 2681, 7185, 2487, 2365, 1996, 3449, 2094, 1997, 3183, 2711, 2487, 2022, 3058, 2487, 1998, 1996, 2406, 2022, 11443, 2011, 7185, 2487, 2307, 10233, 7185, 2487, 2132, 2011, 1996, 21980, 2155, 2019, 12446, 23416, 1997, 1996, 8917, 2487, 1997, 2711, 2487, 1996, 2060, 2011, 1996, 15477, 2040, 2022, 18855, 1999, 1037, 3622, 3287, 2240, 2013, 1037, 2402, 2365, 1997, 14246, 2063, 2487, 2022, 1996, 2279, 8215, 2000, 1996, 6106, 1999, 2553, 1996, 8917, 2487, 1997, 2711, 2487, 2323, 2468, 8548, 2711, 2487, 1996, 2132, 1997, 1996, 15477, 2155, 2022, 2655, 2332, 1997, 2711, 2487, 2138, 1997, 1011, 4013, 2078, 1011, 3510, 2007, 2711, 2487, 1056, 1996, 3035, 1999, 1011, 4013, 2078, 1011, 2219, 2157, 1997, 2023, 1052, 16363, 22084, 2078, 14246, 2063, 2487, 2029, 2022, 1999, 2755, 4498, 1999, 1996, 2192, 1997, 1996, 4496, 2361, 2487, 2061, 2008, 1011, 4013, 2078, 1011, 2069, 5025, 6664, 8676, 1997, 1996, 2210, 4496, 2361, 2487, 2221, 1997, 1042, 10448, 2595, 1998, 1038, 2711, 2487, 1011, 4013, 2078, 1011, 2022, 10634, 1998, 11424, 16136, 2021, 1011, 4013, 2078, 1011, 2564, 2022, 1037, 2450, 1997, 2172, 3754, 1998, 1011, 4013, 2078, 1011, 2567, 2711, 2487, 1997, 8917, 2487, 2022, 2440, 1997, 4382, 1998, 2543, 1998, 2210, 13050, 2000, 9566, 1996, 2004, 23865, 11656, 2029, 1996, 3804, 1997, 21980, 1998, 1011, 4013, 2078, 1011, 2567, 5959, 2012, 2457, 6576, 1999, 9509, 1997, 1011, 4013, 2078, 1011, 18077, 2012, 8917, 2487, 1998, 6576, 2013, 2022, 4470, 2000, 1996, 2402, 3035, 2711, 2487, 1997, 14246, 2063, 2487, 2564, 1997, 2711, 2487, 1996, 15477, 10655, 2132, 1996, 2283, 2426, 1996, 7015, 2040, 3246, 2000, 5618, 2011, 1996, 2332, 2022, 3360, 2000, 8980, 1996, 14293, 1997, 2029, 1011, 4013, 2078, 1011, 2031, 2022, 6360, 2139, 18098, 3512, 2096, 1996, 8917, 2487, 1997, 21980, 2022, 3201, 2000, 5441, 1996, 2373, 1997, 1996, 4410, 2004, 2146, 2004, 2008, 2812, 1011, 4013, 2078, 1011, 2219, 2373, 1996, 13708, 1996, 4372, 16383, 1997, 2122, 7185, 2487, 2283, 2022, 2172, 3623, 2011, 1996, 4668, 2114, 1996, 15157, 8998, 1998, 1996, 7897, 1997, 1996, 11646, 2023, 4668, 2031, 4088, 1999, 1996, 5853, 1997, 2711, 2487, 2043, 1996, 2147, 1035, 1997, 1035, 2396, 2487, 2031, 2022, 17637, 2046, 4496, 2361, 2487, 2011, 7185, 2487, 3076, 2012, 8917, 2487, 1998, 1996, 2332, 2022, 2905, 2711, 2487, 1997, 2711, 2487, 2031, 8627, 1996, 24767, 2711, 2487, 2031, 2223, 2007, 1996, 4496, 2361, 2487, 4496, 2361, 2487, 2138, 1011, 4013, 2078, 1011, 2022, 22277, 2000, 1996, 3750, 2096, 1011, 4013, 2078, 1011, 2566, 3366, 26869, 1996, 2066, 5448, 2012, 2188, 2000, 13225, 1996, 4831, 2711, 2487, 1037, 3128, 1997, 14246, 2063, 2487, 1996, 16097, 4496, 2361, 2487, 24767, 2022, 13260, 2000, 1996, 2489, 2103, 1997, 14246, 2063, 2487, 1998, 2045, 2022, 2191, 2708, 9220, 2096, 1996, 5679, 1997, 8006, 2655, 1011, 4013, 2078, 1011, 2820, 2468, 1996, 3793, 2338, 1997, 1996, 9114, 1999, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 8998, 2022, 8401, 1998, 8665, 13659, 2012, 1996, 27917, 17839, 1997, 7425, 1998, 7939, 23709, 3401, 1996, 4839, 3218, 2061, 16265, 2008, 1996, 2111, 2040, 2907, 1011, 4013, 2078, 1011, 2000, 2031, 2022, 19863, 7699, 2599, 2004, 28473, 2011, 1011, 4013, 2078, 1011, 11646, 10797, 2107, 4808, 1999, 1996, 2277, 2008, 1996, 4496, 2361, 2487, 9928, 2655, 2005, 7750, 2006, 1011, 4013, 2078, 1011, 1996, 9467, 3993, 2166, 1997, 2116, 1997, 1996, 11646, 1998, 1996, 10433, 2791, 1997, 1996, 2457, 2031, 3426, 1037, 2844, 4668, 2114, 1011, 4013, 2078, 1011, 1998, 2307, 2193, 1997, 2119, 7015, 1998, 20934, 27172, 2121, 2468, 4496, 2361, 2487, 1011, 4013, 2078, 1011, 2744, 1011, 4013, 2078, 1011, 4496, 2361, 2487, 2030, 24767, 2021, 1011, 4013, 2078, 1011, 8367, 2022, 8917, 2487, 2763, 2013, 1996, 4496, 2361, 2487, 1041, 13623, 15460, 5054, 2030, 11292, 19033, 2711, 2487, 2066, 1011, 4013, 2078, 1011, 2269, 4047, 4496, 2361, 2487, 4496, 2361, 2487, 1998, 2566, 3366, 26869, 4496, 2361, 2487, 4496, 2361, 2487, 2021, 1996, 5160, 1997, 1996, 8917, 2487, 1997, 14246, 2063, 2487, 6970, 20688, 13520, 2008, 2158, 11276, 2025, 2000, 2022, 6402, 2005, 28354, 2127, 1037, 2473, 1997, 1996, 2277, 2323, 2031, 28887, 1011, 4013, 2078, 1011, 5448, 1998, 1011, 4013, 2078, 1011, 2022, 1999, 1996, 12930, 1997, 2023, 7593, 2008, 2711, 2487, 2022, 22889, 4710, 1996, 9714, 1997, 8840, 2278, 2487, 1996, 21980, 2155, 2022, 2844, 4496, 2361, 2487, 1996, 15477, 2022, 1996, 2132, 1997, 8917, 2487, 15897, 2013, 3343, 2021, 5902, 2711, 2487, 1998, 1011, 4013, 2078, 1011, 2567, 1996, 2711, 2487, 1040, 1998, 18349, 2102, 2022, 18006, 1998, 17300, 24767, 1037, 2030, 18979, 2140, 2487, 2283, 2132, 2011, 1996, 2214, 12294, 2711, 2487, 2022, 4496, 2361, 2487, 1999, 4752, 2021, 2025, 15175, 2000, 3693, 2007, 1996, 8917, 2487, 1999, 4139, 2091, 1996, 21980, 1998, 20865, 1996, 2373, 1997, 1996, 11760, 1037, 9714, 2005, 15126, 1996, 2711, 1997, 1996, 2332, 1998, 6033, 1996, 21980, 2012, 1996, 3317, 1997, 8840, 2278, 2487, 2022, 11487, 1999, 2051, 2000, 2191, 1011, 4013, 2078, 1011, 5909, 3238, 1996, 7185, 2487, 15477, 3159, 2562, 1999, 1996, 4281, 2295, 8917, 2487, 2022, 21186, 2113, 2000, 2031, 2022, 1996, 2995, 2132, 1998, 2693, 2099, 1999, 1011, 4013, 2078, 1011, 1998, 1011, 4013, 2078, 1011, 2022, 2941, 3288, 2000, 3979, 1996, 5456, 2069, 12919, 1996, 2192, 1997, 21980, 2711, 2487, 1997, 2711, 2487, 2130, 2059, 2174, 2711, 2487, 2022, 3280, 1998, 1011, 4013, 2078, 1011, 2567, 2711, 2487, 2040, 9510, 1011, 4013, 2078, 1011, 1999, 2022, 2021, 3058, 2487, 1996, 2711, 2487, 3413, 2000, 1011, 4013, 2078, 1011, 2388, 1996, 13109, 5686, 26730, 2711, 2487, 1037, 19863, 2100, 4937, 2066, 2450, 2040, 2031, 2467, 2718, 5886, 3406, 2022, 2562, 1999, 1996, 4281, 1998, 3005, 2708, 4792, 2022, 2000, 2562, 2518, 4251, 2011, 2377, 2125, 7185, 2487, 2283, 2114, 1996, 2060, 1011, 4013, 2078, 1011, 2012, 2320, 2713, 8917, 2487, 1998, 7927, 1996, 15477, 1998, 1996, 8917, 2487, 2000, 2562, 2091, 1996, 21980, 2130, 9146, 3034, 2000, 2156, 3251, 1996, 4496, 2361, 2487, 2277, 2071, 2022, 5290, 2061, 2004, 2000, 13225, 1996, 4496, 2361, 2487, 6378, 2022, 4604, 2011, 21980, 2022, 2567, 1996, 7185, 1997, 2711, 2487, 2000, 1996, 2473, 2059, 4133, 2012, 14246, 2063, 2487, 2005, 18485, 2326, 1996, 3510, 1997, 1996, 11646, 1998, 2060, 26014, 2029, 2089, 2663, 2067, 1996, 24767, 2021, 2019, 2886, 2011, 1996, 22399, 1997, 21980, 2006, 1037, 3116, 1997, 4496, 2361, 2487, 2012, 14246, 2063, 2487, 1997, 3005, 13060, 1997, 4330, 1011, 4013, 2078, 1011, 2388, 2031, 17612, 2599, 2000, 1996, 2030, 18979, 2140, 2487, 2668, 14740, 1998, 1996, 8293, 1997, 1037, 2942, 2162, 1996, 3412, 2162, 2000, 7637, 2169, 2754, 1997, 1996, 2162, 2052, 2022, 5263, 2306, 2122, 5787, 1011, 4013, 2078, 1011, 2022, 1037, 2162, 2411, 11320, 3363, 2005, 1037, 2460, 2051, 1998, 2411, 3338, 2041, 2153, 1998, 1999, 2029, 1996, 3364, 4982, 2062, 1998, 2062, 10311, 1996, 5290, 3747, 2022, 1999, 1996, 2148, 1996, 4496, 2361, 2487, 1999, 1996, 8840, 2278, 2487, 2087, 1997, 1996, 4992, 2103, 2012, 2030, 18979, 2140, 2487, 2907, 2007, 1996, 15477, 2005, 1996, 8739, 1997, 2942, 1998, 3412, 4071, 2295, 1996, 21980, 2155, 9510, 2000, 1996, 6217, 1997, 1996, 4496, 2361, 2487, 3804, 1999, 14246, 2063, 2487, 2145, 2711, 2487, 13984, 2711, 2487, 1997, 15477, 2000, 2709, 2000, 2457, 2074, 2004, 1011, 4013, 2078, 1011, 2564, 2711, 2487, 1997, 2711, 2487, 2031, 2468, 1037, 26355, 4496, 2361, 2487, 1998, 2096, 3959, 1997, 3863, 1011, 4013, 2078, 1011, 4366, 2006, 2711, 2487, 2005, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 2022, 3102, 2006, 1996, 4496, 2361, 2487, 2217, 2096, 2022, 11741, 3351, 14246, 2063, 2487, 2012, 1996, 2030, 18979, 2140, 2487, 8293, 1996, 8917, 2487, 4025, 2000, 2031, 2011, 2521, 1996, 2307, 3747, 2019, 26911, 2022, 2191, 2000, 15126, 1996, 2332, 2022, 2711, 1998, 2023, 2599, 2000, 1037, 2645, 2012, 2852, 20860, 2096, 1011, 4013, 2078, 1011, 2022, 21888, 2711, 2487, 2941, 13520, 1011, 4013, 2078, 1011, 4618, 2031, 2000, 2360, 1011, 4013, 2078, 1011, 7083, 1999, 4496, 2361, 2487, 21980, 2174, 12850, 3058, 2487, 1998, 2295, 2711, 2487, 2022, 2191, 7267, 2006, 1996, 7185, 2487, 2217, 8917, 2487, 2022, 2202, 2006, 1996, 2060, 14246, 2063, 2487, 2022, 1996, 14246, 2063, 2487, 8320, 2173, 1998, 2096, 2022, 11741, 3351, 1011, 4013, 2078, 1011, 21980, 1011, 4013, 2078, 1011, 2022, 25683, 1011, 4013, 2078, 1011, 2331, 2022, 2903, 2011, 1011, 4013, 2078, 1011, 2155, 2000, 2022, 2349, 2000, 1996, 5902, 2711, 2487, 1996, 2103, 1997, 14246, 2063, 2487, 3481, 8757, 2011, 2711, 2487, 1997, 2711, 2487, 2468, 1996, 16995, 1997, 1996, 8917, 2487, 3003, 2044, 3003, 2991, 2711, 2487, 2006, 1996, 7185, 2487, 2192, 2022, 3102, 2012, 2711, 2487, 2006, 1996, 2060, 2022, 5607, 1999, 3147, 2668, 2044, 1996, 2954, 1997, 2711, 2487, 1037, 18415, 3582, 2021, 2022, 2574, 3338, 2153, 1998, 1999, 2711, 2487, 2022, 1996, 2069, 2158, 1997, 2287, 1998, 3233, 2012, 1996, 2132, 1997, 8917, 2487, 2096, 1996, 4496, 2361, 2487, 2031, 2004, 3003, 2711, 2487, 1997, 6904, 2278, 2487, 2022, 2567, 1998, 2711, 2487, 1997, 21980, 2119, 2402, 2158, 1997, 2210, 2062, 2084, 3174, 1996, 8917, 2487, 2031, 2022, 3786, 2012, 2035, 2391, 2021, 2022, 2145, 2844, 2438, 2000, 2031, 23277, 2075, 2013, 1011, 4013, 2078, 1011, 4099, 6656, 2000, 2907, 3116, 2005, 2270, 7425, 2306, 4895, 9628, 2098, 2237, 1998, 2006, 1996, 3776, 1997, 2107, 7015, 2004, 2907, 2007, 1011, 4013, 2078, 1011, 2711, 2487, 2022, 2711, 2487, 2191, 2224, 1997, 1996, 8636, 1997, 2849, 2000, 3046, 2000, 20010, 6776, 1996, 14246, 2063, 2487, 3003, 2011, 4372, 23395, 1011, 4013, 2078, 1011, 1999, 1996, 5165, 1997, 1996, 2457, 1998, 2896, 1011, 4013, 2078, 1011, 3168, 1997, 4611, 1996, 2457, 2022, 2996, 27191, 8235, 2711, 2487, 15161, 1011, 4013, 2078, 1011, 2007, 1037, 2022, 10736, 1997, 3203, 2655, 1996, 3035, 2388, 2022, 3704, 3005, 9778, 2022, 2424, 2005, 1996, 2878, 2154, 1996, 3203, 4133, 2012, 1011, 4013, 2078, 1011, 25213, 4853, 2096, 4496, 2361, 2487, 4623, 1998, 7472, 2022, 3191, 2030, 2293, 2299, 6170, 2011, 1996, 10170, 1011, 4013, 2078, 1011, 2031, 3871, 2208, 1998, 5690, 2283, 2007, 2296, 3098, 2005, 1996, 3203, 2000, 2552, 2004, 19558, 2000, 2151, 3183, 1996, 3035, 4299, 2000, 20010, 6776, 2013, 1996, 6958, 1997, 6225, 1998, 11870, 1998, 14187, 2000, 1011, 4013, 2078, 1011, 2326, 3608, 12438, 1998, 8900, 3582, 1999, 1996, 3944, 1998, 2045, 2022, 6684, 1037, 3159, 2030, 7015, 1999, 14246, 2063, 2487, 2040, 2022, 2025, 4287, 2185, 2011, 2122, 26962, 2046, 2601, 10427, 1997, 11268, 14715, 5666, 2711, 2487, 1997, 2711, 2487, 14436, 1011, 4013, 2078, 1011, 2005, 1011, 4013, 2078, 1011, 2365, 2711, 2487, 3183, 1011, 4013, 2078, 1011, 2562, 2004, 2146, 2004, 2825, 2104, 2731, 1999, 4676, 4083, 1998, 9532, 10427, 1999, 1996, 3137, 1997, 1038, 12098, 2078, 1998, 2043, 2711, 2487, 3046, 2000, 4009, 1011, 4013, 2078, 1011, 2000, 2457, 2011, 16599, 1037, 3510, 2090, 1011, 4013, 2078, 1011, 1998, 1011, 4013, 2078, 1011, 2402, 2684, 2711, 2487, 2681, 1011, 4013, 2078, 1011, 2012, 2188, 1998, 2175, 1011, 4013, 2078, 1011, 2000, 2457, 2711, 2487, 3046, 1999, 15784, 2000, 8815, 1011, 4013, 2078, 1011, 2097, 2030, 7523, 1011, 4013, 2078, 1011, 3595, 1998, 1011, 4013, 2078, 1011, 2331, 2220, 1999, 2096, 2145, 2012, 2457, 2022, 17961, 2000, 1996, 3035, 2388, 9288, 1997, 2711, 2487, 2022, 2365, 2711, 2487, 2022, 3202, 18654, 2000, 16519, 1996, 3510, 1998, 2272, 5463, 2011, 2035, 1996, 2087, 5182, 8917, 2487, 2295, 1996, 2062, 15705, 1997, 1011, 4013, 2078, 1011, 3961, 2012, 2188, 1998, 1996, 5797, 1997, 2711, 2487, 2360, 2065, 2008, 5030, 2202, 2173, 1996, 7927, 2097, 2022, 11466, 1996, 3804, 1997, 21980, 4025, 2000, 2031, 10663, 2006, 2202, 2023, 4495, 1997, 7195, 1011, 4013, 2078, 1011, 2005, 1011, 4013, 2078, 1011, 2269, 2022, 4028, 2021, 1996, 3035, 2388, 2022, 6151, 8586, 14097, 2127, 1011, 4013, 2078, 1011, 2424, 2008, 1011, 4013, 2078, 1011, 2365, 2711, 2487, 2040, 2031, 2022, 7226, 2000, 6187, 5558, 2571, 1998, 2831, 2058, 1996, 14246, 2063, 2487, 2708, 2031, 2022, 9958, 2011, 1011, 4013, 2078, 1011, 16718, 1998, 10051, 2791, 1998, 2022, 3201, 2000, 5466, 1011, 4013, 2078, 1011, 2046, 1011, 4013, 2078, 1011, 2192, 1998, 4019, 2013, 1011, 4013, 2078, 1011, 2019, 11113, 11589, 3512, 3535, 2006, 21980, 2022, 2112, 2000, 4028, 1996, 5902, 2711, 2487, 2599, 2000, 2035, 1996, 8917, 2487, 2175, 2055, 4273, 1998, 2191, 10467, 2029, 8598, 2119, 1996, 3035, 1998, 1996, 2111, 1997, 14246, 2063, 2487, 21980, 1998, 1996, 3804, 1997, 14246, 2063, 2487, 2022, 3568, 3499, 2000, 2147, 1011, 4013, 2078, 1011, 2097, 1998, 2000, 27384, 1996, 2668, 15222, 12096, 9961, 1997, 1996, 14246, 2063, 2487, 11240, 2012, 2051, 2487, 1997, 1996, 16215, 1997, 3058, 2487, 2711, 2487, 2022, 2051, 2487, 1996, 4330, 1997, 1996, 2277, 1997, 6904, 2278, 2487, 4088, 2000, 3614, 1998, 1996, 14574, 2022, 4088, 2011, 2158, 10782, 2011, 1037, 2317, 10353, 1996, 2332, 7713, 1011, 4013, 2078, 1011, 14246, 2063, 2487, 9431, 1998, 6821, 1999, 1011, 4013, 2078, 1011, 2282, 1996, 2402, 2332, 1997, 2711, 2487, 1998, 3159, 1997, 8917, 2487, 2022, 15686, 2046, 23758, 2000, 1996, 2277, 2021, 2296, 2060, 14246, 2063, 2487, 2040, 2071, 2022, 2424, 2022, 9288, 2013, 2711, 2487, 2040, 2022, 22889, 4710, 6181, 2140, 1999, 1011, 4013, 2078, 1011, 5010, 2011, 1996, 22399, 1997, 21980, 2091, 2000, 1996, 3532, 1998, 2402, 1998, 1996, 2395, 24501, 28819, 2007, 1996, 5390, 3102, 3102, 1999, 2296, 2103, 2073, 2548, 10123, 1998, 26458, 10286, 2094, 14254, 2031, 2022, 2444, 2426, 8917, 2487, 1996, 2168, 22293, 2147, 2202, 2173, 2005, 3058, 2487, 8622, 4445, 2287, 4496, 3348, 2129, 2116, 7185, 2487, 3280, 1011, 4013, 2078, 1011, 2022, 5263, 2000, 29072, 2021, 1996, 2147, 2022, 2061, 17264, 2008, 3904, 2022, 2681, 3272, 2216, 1999, 1996, 2670, 2103, 2073, 1996, 8917, 2487, 2031, 2022, 2205, 2844, 2000, 2022, 2886, 1998, 1999, 2216, 3317, 2073, 1996, 7367, 23773, 11236, 2022, 1997, 1996, 4676, 8917, 2487, 2228, 1996, 6215, 3143, 1996, 2457, 2175, 1999, 2110, 2000, 2709, 4067, 2005, 8116, 6651, 2013, 1037, 6814, 5436, 2096, 2711, 2487, 2022, 2303, 2022, 6865, 2006, 1037, 21025, 19473, 2102, 1996, 4831, 2344, 2270, 15060, 2096, 2711, 2487, 2404, 2006, 16236, 1998, 1996, 3750, 2711, 2487, 2894, 2426, 4496, 2361, 2487, 3159, 2265, 2151, 5469, 2030, 27427, 23773, 3370, 2021, 1996, 2540, 1997, 1996, 12511, 2402, 2332, 2022, 3338, 2011, 1996, 8056, 1011, 4013, 2078, 1011, 2031, 4297, 3126, 2711, 2487, 7752, 2046, 1037, 6689, 1998, 3280, 1999, 2424, 2053, 7216, 3828, 1999, 1996, 9431, 1998, 6821, 1011, 4013, 2078, 1011, 2031, 3828, 1996, 2223, 1011, 4013, 2078, 1011, 2567, 2711, 2487, 2040, 2031, 2022, 11322, 2332, 1997, 14246, 2063, 2487, 5466, 2039, 2008, 4410, 1999, 7927, 1997, 2008, 1997, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 2022, 1997, 1037, 15784, 6270, 5410, 2839, 3565, 16643, 20771, 2135, 26092, 1998, 2012, 1996, 2168, 2051, 27863, 2061, 2004, 2000, 7344, 3686, 2296, 7185, 2487, 2035, 2022, 14984, 1997, 1037, 2158, 2040, 4377, 1999, 1996, 6034, 1997, 1042, 7361, 4842, 2100, 2007, 1037, 28291, 1997, 2331, 2022, 2132, 2012, 1011, 4013, 2078, 1011, 21025, 4103, 2571, 1998, 3413, 2013, 3748, 4487, 18719, 24952, 2239, 2000, 11113, 20614, 19409, 5897, 1011, 4013, 2078, 1011, 2022, 2655, 1996, 14246, 2063, 2487, 2277, 13745, 1998, 1996, 3035, 2022, 2606, 16200, 18116, 2005, 1011, 4013, 2078, 1011, 3413, 2013, 1011, 4013, 2078, 1011, 11848, 2618, 2000, 1996, 11446, 1997, 1996, 2813, 1997, 2277, 2007, 21203, 3013, 2041, 1997, 2214, 2326, 2338, 2823, 1011, 4013, 2078, 1011, 2175, 2055, 15161, 2007, 2210, 3899, 2823, 13109, 8649, 1011, 4013, 2078, 1011, 3328, 22985, 1999, 1037, 14385, 1998, 1011, 4013, 2078, 1011, 7185, 2487, 2030, 8837, 2022, 1996, 9446, 1997, 1996, 2406, 2011, 1011, 4013, 2078, 1011, 6620, 6105, 1998, 9576, 15046, 1996, 2162, 3338, 2041, 2153, 1998, 1011, 4013, 2078, 1011, 2069, 3961, 2567, 2711, 2487, 1997, 2711, 2487, 2006, 2019, 8053, 5223, 3993, 1998, 17152, 7028, 2108, 10574, 2013, 2457, 2000, 8917, 2487, 3246, 2000, 2486, 1011, 4013, 2078, 1011, 2567, 2046, 4965, 1011, 4013, 2078, 1011, 12339, 2021, 2043, 1996, 2332, 1997, 2711, 2487, 2031, 3582, 1011, 4013, 2078, 1011, 1998, 4088, 1996, 5998, 1999, 17300, 1011, 4013, 2078, 1011, 5138, 1996, 11068, 1997, 14246, 2063, 2487, 1998, 2709, 2000, 1011, 4013, 2078, 1011, 14588, 2711, 2487, 2022, 13260, 2011, 1996, 16021, 27176, 4496, 2361, 2487, 2000, 2468, 1011, 4013, 2078, 1011, 2708, 1998, 5247, 2070, 2051, 1999, 14246, 2063, 2487, 2021, 2709, 7736, 1998, 3280, 2004, 1996, 2332, 2022, 26983, 1996, 2279, 3287, 8215, 2022, 2711, 2487, 1997, 2711, 2487, 2040, 2031, 10574, 2013, 2457, 2574, 2044, 2711, 2487, 2006, 2709, 2000, 1996, 14246, 2063, 2487, 4752, 1998, 2022, 5853, 1999, 1011, 4013, 2078, 1011, 7185, 2487, 2221, 1997, 1038, 12098, 2078, 1998, 1042, 10448, 2595, 1996, 2132, 1997, 1996, 8917, 2487, 1999, 1996, 10663, 2196, 2000, 9146, 1037, 2182, 4588, 2000, 4929, 1996, 4496, 2361, 2487, 4410, 21980, 1998, 1011, 4013, 2078, 1011, 2283, 2433, 8917, 2487, 2000, 2486, 2711, 2487, 2000, 5454, 2178, 6332, 14246, 2063, 2487, 2022, 23313, 2000, 21980, 1998, 1996, 2332, 2424, 1011, 4013, 2078, 1011, 2471, 1037, 7267, 2045, 2681, 1996, 2103, 2021, 2022, 2153, 3040, 2011, 1996, 3804, 2012, 2711, 2487, 1998, 2071, 2061, 2711, 2487, 1011, 4013, 2078, 1011, 24416, 2004, 2000, 2031, 28667, 22957, 2063, 2000, 10102, 1011, 4013, 2078, 1011, 3426, 1011, 4013, 2078, 1011, 2000, 2022, 22889, 4710, 2012, 1996, 4186, 2012, 2711, 2487, 1999, 1996, 8111, 1997, 1996, 2223, 2022, 2061, 2307, 2008, 2711, 2487, 2022, 3298, 2000, 2202, 9277, 2007, 1996, 2332, 1997, 2711, 2487, 1998, 1011, 4013, 2078, 1011, 2022, 2362, 2022, 11741, 3351, 14246, 2063, 2487, 2043, 2711, 2487, 2022, 1999, 1011, 4013, 2078, 1011, 2735, 4028, 2011, 1037, 8284, 2171, 2711, 2487, 1999, 2711, 2487, 1996, 2223, 2099, 4013, 25154, 2004, 2332, 2019, 2214, 4470, 1997, 1996, 2332, 1997, 2147, 1035, 1997, 1035, 2396, 2487, 8320, 2461, 2711, 2487, 1997, 2711, 2487, 2040, 2202, 1996, 2516, 1997, 2711, 2487, 2012, 14246, 2063, 2487, 1999, 14246, 2063, 2487, 3113, 1996, 2486, 1997, 2223, 2099, 1998, 4154, 1011, 4013, 2078, 1011, 2011, 1011, 4013, 2078, 1011, 8235, 8424, 3582, 1011, 4013, 2078, 1011, 2317, 26888, 1011, 4013, 2078, 1011, 2197, 2344, 2000, 1011, 4013, 2078, 1011, 10123, 2468, 7185, 2487, 1997, 1996, 3038, 1996, 4496, 2361, 2487, 2293, 2000, 3342, 2021, 1011, 4013, 2078, 1011, 3426, 2022, 2145, 2025, 2663, 14246, 2063, 2487, 2907, 2041, 2114, 1011, 4013, 2078, 1011, 2019, 21499, 2011, 2471, 5470, 12070, 2389, 8111, 1998, 2096, 1011, 4013, 2078, 1011, 2022, 2022, 11741, 3351, 1011, 4013, 2078, 1011, 14246, 2063, 2487, 2022, 18445, 2013, 1996, 14246, 2063, 2487, 1996, 2214, 7185, 1997, 15477, 2022, 2085, 2757, 1998, 2711, 2487, 5136, 1011, 4013, 2078, 1011, 2684, 2711, 2487, 3005, 2388, 2022, 1996, 3449, 2094, 2684, 1997, 2711, 2487, 2000, 2022, 27167, 3035, 1997, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 4604, 3568, 1011, 4013, 2078, 1011, 2583, 2236, 1996, 3804, 1997, 2711, 2487, 2000, 2522, 5452, 2007, 1996, 2223, 2099, 1998, 2173, 1011, 4013, 2078, 1011, 2006, 1996, 6106, 1037, 2162, 1997, 5656, 2022, 4287, 2006, 2076, 2029, 2711, 2487, 2562, 1996, 4099, 2012, 3016, 2021, 2071, 2079, 2053, 2062, 2144, 1996, 2312, 2193, 1997, 1011, 4013, 2078, 1011, 2111, 2295, 13566, 2000, 2031, 2053, 2332, 2021, 1011, 4013, 2078, 1011, 2079, 2025, 4299, 1011, 4013, 2078, 1011, 2000, 5114, 2205, 3733, 1037, 3377, 26693, 1999, 2008, 2553, 1011, 4013, 2078, 1011, 2323, 3961, 1037, 4496, 2361, 2487, 2174, 1011, 4013, 2078, 1011, 2022, 2069, 3524, 2000, 28667, 4630, 6229, 1011, 4013, 2078, 1011, 2071, 2079, 2061, 2007, 1037, 2204, 4519, 1011, 4013, 2078, 1011, 2428, 9544, 4496, 2361, 2487, 1998, 2031, 2069, 2022, 1037, 2576, 14246, 2063, 2487, 1998, 1011, 4013, 2078, 1011, 2204, 1998, 2087, 11633, 11747, 1996, 5797, 1997, 2711, 2487, 2092, 2113, 2004, 3804, 1997, 2711, 2487, 2295, 1037, 26355, 4496, 2361, 2487, 1011, 4013, 2078, 1011, 16755, 1996, 2689, 2004, 1996, 2069, 2812, 1997, 9239, 3521, 2000, 1996, 14246, 2063, 2487, 2045, 2022, 2210, 2062, 5012, 2000, 2711, 2487, 2044, 1011, 4013, 2078, 1011, 2031, 2153, 2022, 4374, 2011, 1996, 2277, 1999, 14246, 2063, 2487, 16040, 1997, 2724, 2487, 2330, 1011, 4013, 2078, 1011, 4796, 1999, 1998, 1996, 21490, 4630, 4306, 2461, 1011, 4013, 2078, 1011, 2007, 19069, 2061, 2008, 1011, 4013, 2078, 1011, 2360, 3532, 2111, 1011, 4013, 2078, 1011, 2022, 7501, 2005, 1996, 4356, 1997, 1037, 2332, 1996, 2223, 2099, 2191, 1011, 4013, 2078, 1011, 3521, 1998, 2043, 2711, 2487, 1997, 14246, 2063, 2487, 2153, 2886, 2711, 2487, 1997, 21980, 2022, 7185, 2487, 1997, 1996, 2030, 18979, 2140, 2487, 2000, 24748, 2078, 2000, 1996, 4721, 2711, 2487, 2156, 2008, 2045, 2022, 2053, 2582, 3246, 2005, 1011, 4013, 2078, 1011, 2684, 1998, 3521, 2022, 2191, 1999, 1996, 24754, 1997, 14246, 2063, 2487, 3058, 2487, 1999, 2711, 2487, 2404, 5743, 2054, 2022, 2655, 1996, 24754, 1997, 14246, 2063, 2487, 2138, 2030, 18979, 2140, 2487, 4236, 1999, 2008, 8917, 2487, 1011, 4013, 2078, 1011, 5851, 2000, 1996, 8917, 2487, 5020, 2942, 2157, 2007, 2216, 1997, 1996, 4496, 2361, 2487, 5138, 1011, 4013, 2078, 1011, 3510, 2507, 1011, 4013, 2078, 1011, 2104, 16840, 6656, 2000, 3113, 2005, 7425, 1998, 2005, 16053, 1998, 3946, 1011, 4013, 2078, 1011, 2103, 2005, 1996, 3036, 1997, 1011, 4013, 2078, 1011, 2157, 1997, 2029, 8917, 2487, 2022, 1996, 2708, 1996, 4496, 2361, 2487, 2031, 2022, 3053, 4654, 3334, 19269, 1999, 1996, 2167, 2021, 2045, 2022, 2145, 1037, 2312, 2193, 1999, 1996, 2148, 1997, 14246, 2063, 2487, 1998, 1996, 20934, 27172, 2121, 1997, 1996, 2708, 2670, 2103, 2022, 3262, 14246, 2063, 2487, 1996, 2162, 2031, 2022, 2013, 1996, 2030, 18979, 2140, 2487, 1037, 2200, 9202, 7185, 2487, 2045, 2031, 2022, 9576, 14574, 1998, 2145, 2062, 9576, 16360, 6935, 2389, 2006, 2169, 2217, 1996, 2402, 7015, 2031, 2022, 3345, 2046, 2191, 1037, 4827, 1997, 10768, 21735, 1998, 10975, 18908, 5562, 19415, 2126, 1997, 4894, 2331, 6271, 2878, 2212, 2031, 2022, 3913, 5949, 2277, 1998, 6103, 6033, 8136, 5883, 1998, 1996, 2878, 2313, 16222, 19966, 5358, 2000, 2296, 4066, 1997, 5469, 1998, 6114, 2096, 6343, 2021, 2711, 2487, 1011, 4013, 2078, 1011, 1998, 1996, 3804, 1997, 2711, 2487, 2031, 2151, 9366, 2593, 1997, 17689, 9650, 2030, 1997, 3412, 2000, 3917, 3370, 2711, 2487, 2022, 2933, 2074, 2004, 1996, 5853, 1997, 2711, 2487, 2031, 2022, 1037, 2558, 1997, 2717, 1998, 7233, 2013, 2724, 2487, 2061, 2008, 1997, 2711, 2487, 2022, 7185, 2487, 1997, 6418, 2013, 1996, 10958, 3567, 3351, 1997, 3058, 2487, 1997, 23852, 2942, 2162, 1996, 2332, 1011, 4013, 2078, 1011, 2025, 2069, 2031, 4408, 1998, 11973, 5450, 2021, 2022, 1037, 2158, 1997, 2312, 2540, 1998, 2568, 1998, 2711, 2487, 2079, 2172, 2005, 1996, 7574, 1997, 1996, 2406, 2346, 5033, 2958, 10690, 4807, 9922, 7949, 6236, 2035, 12533, 1011, 4013, 2078, 1011, 4712, 2000, 1011, 4013, 2078, 1011, 1998, 3288, 14165, 2000, 1996, 20934, 27172, 2121, 2465, 1998, 1996, 2332, 2022, 2926, 2203, 14644, 2000, 1996, 14539, 2854, 2011, 1011, 4013, 2078, 1011, 2360, 2008, 1011, 4013, 2078, 1011, 3246, 2005, 1996, 2051, 2043, 2053, 9151, 2052, 2022, 2302, 1037, 2204, 1042, 5004, 2140, 1999, 1011, 4013, 2078, 1011, 8962, 1996, 2307, 6953, 2158, 16093, 18908, 10253, 1997, 2670, 14246, 2063, 2487, 15897, 13368, 2104, 1011, 4013, 2078, 1011, 15846, 1998, 2045, 2022, 14165, 1997, 2296, 2785, 1996, 2277, 1011, 4013, 2078, 1011, 2022, 1999, 1037, 2521, 2092, 2110, 2084, 2077, 2070, 1997, 1996, 2204, 2158, 1997, 2151, 2051, 2022, 2059, 2444, 1999, 2711, 2487, 2040, 2079, 2172, 2000, 5335, 1996, 2731, 1997, 1996, 28773, 11646, 1998, 2040, 2179, 1996, 2344, 1997, 2905, 1997, 5952, 2040, 4652, 1996, 14624, 1997, 1996, 2395, 1997, 14246, 2063, 2487, 2013, 2412, 2022, 2061, 25966, 3993, 2004, 1999, 3058, 2487, 2043, 5532, 2775, 2468, 1996, 8336, 1997, 4702, 3899, 1998, 10369, 1996, 7015, 2040, 2031, 4982, 2046, 16021, 9890, 5897, 2076, 1996, 2162, 2593, 2004, 8837, 1997, 2711, 2487, 2030, 2004, 27838, 23067, 2271, 10129, 1997, 1996, 14246, 2063, 2487, 3426, 2022, 4942, 20041, 1998, 24763, 1996, 2087, 3602, 1997, 2122, 2022, 1996, 3804, 1997, 2711, 2487, 1996, 3954, 1997, 1996, 2235, 18018, 1997, 15134, 2040, 2022, 5547, 2000, 22645, 2011, 1996, 4356, 1997, 2711, 2487, 2022, 18085, 3345, 1997, 4893, 1998, 1996, 8610, 3804, 1997, 2711, 2487, 2040, 2228, 2008, 2711, 2487, 2031, 2025, 12949, 10377, 1011, 4013, 2078, 1011, 2326, 20014, 27611, 2007, 14246, 2063, 2487, 1998, 8917, 2487, 1998, 2022, 2022, 4974, 2005, 1011, 4013, 2078, 1011, 14712, 11150, 2000, 8917, 2487, 1999, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 2022, 2004, 10326, 2004, 2412, 1999, 14246, 2063, 2487, 1998, 1999, 2711, 2487, 2022, 7374, 2005, 2178, 2162, 2006, 1996, 14865, 1997, 1037, 7593, 8338, 2000, 1996, 11068, 1997, 2711, 2487, 1996, 2214, 5470, 12070, 2964, 2145, 26577, 1999, 14246, 2063, 2487, 1998, 2711, 2487, 2031, 2022, 18012, 2000, 2022, 8059, 1997, 12438, 2045, 2021, 1011, 4013, 2078, 1011, 2022, 4072, 2008, 1011, 4013, 2078, 1011, 2030, 18979, 2140, 2487, 2564, 2711, 2487, 2323, 2022, 4410, 2077, 1011, 4013, 2078, 1011, 2175, 2000, 1996, 2162, 2004, 1011, 4013, 2078, 1011, 2022, 2000, 2022, 2681, 11315, 3058, 2487, 2044, 1996, 12773, 2004, 2711, 2487, 2022, 2175, 2000, 1996, 9433, 2000, 3942, 1011, 4013, 2078, 1011, 2214, 2767, 2711, 2487, 1011, 4013, 2078, 1011, 2022, 17079, 2000, 1996, 2540, 1999, 1011, 4013, 2078, 1011, 2873, 1999, 1996, 2395, 1997, 14246, 2063, 2487, 2011, 1037, 5470, 12070, 2171, 14246, 2063, 2487, 1996, 4496, 2361, 2487, 2655, 1011, 4013, 2078, 1011, 6904, 2278, 2487, 1998, 1011, 4013, 2078, 1011, 2022, 7185, 2487, 1997, 1996, 2087, 8702, 1998, 25786, 1997, 2158, 2663, 1996, 2540, 1997, 2035, 2040, 3921, 1011, 4013, 2078, 1011, 2021, 1996, 10047, 22049, 3012, 1997, 1011, 4013, 2078, 1011, 2166, 2079, 2172, 2000, 12210, 1996, 2525, 2659, 3115, 2008, 3653, 3567, 4014, 2426, 3159, 1998, 7015, 1999, 14246, 2063, 2487, 1996, 2163, 2236, 1997, 2711, 2487, 2022, 2030, 18979, 2140, 2487, 2564, 2711, 2487, 2468, 11315, 2005, 1011, 4013, 2078, 1011, 2711, 2487, 15031, 2022, 2069, 3058, 2487, 1998, 5262, 1011, 4013, 2078, 1011, 2839, 2022, 2061, 5410, 2008, 1011, 4013, 2078, 1011, 2878, 5853, 2022, 2069, 7185, 2487, 2146, 7162, 2711, 2487, 2022, 4498, 2104, 1996, 13738, 1997, 2019, 4496, 2361, 2487, 8837, 2171, 9530, 27085, 1998, 1011, 4013, 2078, 1011, 2564, 1998, 1011, 4013, 2078, 1011, 2878, 26911, 2022, 2000, 25933, 4757, 4138, 2005, 1011, 4013, 2078, 1011, 1998, 2562, 1996, 2402, 2332, 1999, 13346, 18173, 2096, 1011, 4013, 2078, 1011, 25672, 2035, 2008, 2711, 2487, 2031, 3466, 1998, 2202, 26470, 9467, 10895, 1996, 3159, 1997, 8917, 2487, 3046, 2000, 16857, 1011, 4013, 2078, 1011, 1998, 1999, 3246, 1997, 12919, 1011, 4013, 2078, 1011, 1999, 2711, 2487, 18654, 2362, 1996, 2110, 2236, 2045, 2272, 2266, 2005, 1996, 7015, 2005, 1996, 11646, 1998, 2005, 1996, 2030, 18979, 2140, 2487, 3776, 1045, 1041, 1996, 20934, 27172, 2121, 1998, 2122, 2022, 3262, 5160, 1998, 14351, 2013, 1996, 2874, 2022, 10663, 2000, 2191, 1011, 4013, 2078, 1011, 2376, 2963, 14952, 2022, 4982, 2919, 1998, 2919, 2025, 2069, 2022, 1011, 4013, 2078, 1011, 9530, 23460, 2000, 1996, 20934, 27172, 2121, 1998, 14539, 2465, 11819, 1996, 11646, 1998, 1996, 7015, 2426, 2029, 2197, 2022, 2421, 1011, 4013, 2078, 1011, 2155, 2000, 1996, 6556, 4245, 2021, 1011, 4013, 2078, 1011, 2031, 2468, 1996, 2457, 7661, 2000, 4800, 22086, 2436, 1999, 2344, 2000, 11550, 1996, 7015, 1998, 2562, 1011, 4013, 2078, 1011, 4251, 1998, 2023, 2362, 2007, 1996, 10961, 1997, 1996, 2390, 2191, 1996, 3635, 1997, 14952, 10083, 3560, 9308, 1996, 8312, 2000, 1996, 2942, 2436, 2907, 2011, 5160, 2022, 2191, 14800, 1999, 1011, 4013, 2078, 1011, 2155, 2006, 7909, 1997, 1037, 7680, 2091, 1998, 1997, 7408, 2012, 1996, 2331, 1997, 2169, 9111, 2035, 2122, 6905, 2022, 17612, 1997, 1998, 7185, 2487, 1997, 1996, 4112, 2130, 2425, 1996, 11760, 2008, 2065, 1011, 4013, 2078, 1011, 2079, 2025, 4553, 2000, 7438, 1996, 26626, 2465, 2917, 1011, 4013, 2078, 1011, 2004, 2402, 2567, 1011, 4013, 2078, 1011, 2052, 3913, 2039, 1037, 6659, 3573, 1997, 25928, 2005, 1011, 4013, 2078, 1011, 1037, 9964, 2000, 1996, 2332, 2022, 4009, 2039, 1998, 2022, 4374, 2021, 2196, 3437, 1996, 2341, 1997, 8917, 2487, 2022, 2485, 1996, 2266, 2022, 2425, 1011, 4013, 2078, 1011, 2022, 2011, 2344, 1997, 1996, 2332, 1998, 1996, 2110, 2236, 2196, 3113, 2153, 2005, 3058, 2487, 2043, 1996, 4040, 2022, 2074, 3201, 2000, 2991, 1996, 6859, 1997, 14246, 2063, 2487, 1996, 11083, 2791, 1997, 1996, 2110, 2022, 15897, 12533, 2000, 1996, 11760, 2040, 2004, 2146, 2004, 1011, 4013, 2078, 1011, 2022, 3499, 2000, 23088, 2091, 1011, 4013, 2078, 1011, 14539, 1998, 12342, 2012, 2457, 2031, 2053, 3168, 1997, 4611, 2030, 2270, 4382, 1998, 5223, 1996, 20934, 27172, 2121, 1998, 5160, 2521, 2205, 2172, 2000, 2191, 2691, 3426, 2007, 1011, 4013, 2078, 1011, 2114, 1996, 7887, 3623, 2373, 1997, 1996, 6106, 1011, 4013, 2078, 1011, 2069, 20014, 27611, 1998, 5998, 2005, 3167, 5056, 1998, 10685, 1998, 2196, 2228, 1997, 1996, 2204, 1997, 1996, 2110, 1011, 4013, 2078, 1011, 19248, 5223, 9530, 27085, 1996, 8610, 1040, 2019, 16748, 2004, 1011, 4013, 2078, 1011, 2031, 2022, 3443, 2021, 1011, 4013, 2078, 1011, 3961, 1999, 2373, 6229, 2043, 7185, 2487, 1997, 1996, 2332, 2022, 10170, 2711, 2487, 5436, 2007, 1996, 2332, 1011, 4013, 2078, 1011, 1998, 1037, 2261, 1997, 1011, 4013, 2078, 1011, 3457, 2005, 1011, 4013, 2078, 1011, 8116, 6651, 2498, 2071, 2022, 3733, 2084, 1996, 7781, 1996, 2332, 2344, 1996, 2952, 1997, 1996, 3457, 2000, 6545, 9530, 27085, 1998, 3102, 1011, 4013, 2078, 1011, 2065, 1011, 4013, 2078, 1011, 9507, 1998, 2023, 2022, 2079, 9530, 27085, 2022, 3013, 2091, 2006, 1996, 3357, 1997, 1996, 25110, 1998, 14246, 2063, 2487, 4654, 25154, 2012, 2197, 1045, 2022, 1037, 2332, 2021, 1011, 4013, 2078, 1011, 2022, 2025, 1999, 1011, 4013, 2078, 1011, 2000, 2022, 1037, 2332, 1998, 1011, 4013, 2078, 1011, 2196, 2022, 7185, 2487, 2035, 1011, 4013, 2078, 1011, 2166, 1011, 4013, 2078, 1011, 2069, 3413, 2104, 1996, 13738, 1997, 2711, 2487, 2040, 2022, 1037, 2152, 24462, 2402, 7015, 1996, 8917, 2487, 2031, 2022, 2907, 3320, 2029, 2022, 5136, 2062, 2576, 2084, 3412, 1998, 1011, 4013, 2078, 1011, 2237, 1997, 3036, 2022, 1037, 24665, 2666, 21789, 2000, 16664, 2162, 3338, 2041, 2153, 1998, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 2175, 2007, 2711, 2487, 2000, 2022, 11741, 3351, 18318, 4887, 8193, 1996, 2173, 2022, 2202, 2021, 4295, 3338, 2041, 1999, 1996, 2390, 1998, 2711, 2487, 3280, 2045, 2022, 1037, 4840, 5998, 2005, 2373, 2090, 1996, 3035, 2388, 1998, 1996, 3159, 1997, 8917, 2487, 4566, 1999, 2119, 2022, 2275, 4998, 2011, 1996, 3035, 2022, 2711, 2487, 3387, 1997, 2711, 2487, 2006, 1998, 5728, 1037, 7185, 1996, 2583, 17689, 2059, 1999, 8840, 2278, 2487, 2040, 5114, 3143, 13738, 2058, 1996, 2332, 1998, 2406, 1998, 3627, 1011, 4013, 2078, 1011, 2119, 2007, 1037, 8473, 1997, 3707, 1996, 8917, 2487, 2022, 6360, 3298, 2041, 1997, 2035, 1011, 4013, 2078, 1011, 16995, 6229, 2069, 14246, 2063, 2487, 3961, 2000, 1011, 4013, 2078, 1011, 2023, 2103, 2022, 9191, 2135, 1998, 19080, 6985, 2011, 1996, 14351, 1998, 1996, 3804, 1997, 2711, 2487, 2007, 3246, 1997, 10514, 21408, 3126, 2013, 14246, 2063, 2487, 2127, 2122, 2022, 12532, 5897, 5339, 2011, 1996, 4028, 1997, 1996, 3804, 1997, 2711, 2487, 1011, 4013, 2078, 1011, 2022, 2486, 2000, 7806, 2044, 2031, 2907, 2041, 2005, 3058, 2487, 2711, 2487, 4607, 1999, 10911, 2139, 18098, 3512, 1996, 2103, 1997, 2035, 1011, 4013, 2078, 1011, 14293, 1998, 2947, 1999, 16519, 1996, 2162, 2008, 2031, 4088, 2011, 1996, 2886, 1997, 1996, 26458, 10286, 2094, 2006, 1996, 7769, 2012, 14246, 2063, 2487, 1999, 1996, 2166, 1998, 3200, 1997, 1996, 8917, 2487, 2022, 2145, 5851, 2021, 2035, 7927, 2022, 2485, 2114, 1011, 4013, 2078, 1011, 1998, 2296, 15846, 2907, 2041, 2000, 1011, 4013, 2078, 1011, 2000, 3693, 1996, 2277, 2116, 1997, 1996, 2919, 9446, 2031, 2022, 6366, 1998, 1996, 11646, 2022, 2172, 5335, 1998, 2013, 3649, 15793, 1011, 4013, 2078, 1011, 2089, 2022, 2116, 1997, 1996, 2062, 6383, 8917, 2487, 4088, 2000, 23758, 2000, 1996, 2110, 4676, 10381, 1996, 2373, 1997, 1996, 4410, 8917, 2487, 2022, 3447, 7185, 2711, 2487, 2022, 2878, 2801, 1997, 17689, 9650, 8676, 1999, 2191, 2711, 2487, 1996, 2307, 1997, 3159, 2012, 2188, 1998, 6917, 2000, 2191, 2505, 2307, 1997, 2711, 2487, 2040, 2022, 7408, 3468, 11455, 1999, 2568, 1998, 2303, 2022, 3458, 2151, 7185, 2487, 2022, 2373, 1998, 8917, 2487, 2562, 1011, 4013, 2078, 1011, 1999, 7619, 3395, 3258, 3499, 1011, 4013, 2078, 1011, 1037, 8837, 2007, 3183, 2000, 5690, 2831, 1998, 2572, 8557, 1011, 4013, 2078, 1011, 2021, 2065, 1996, 2767, 3535, 2000, 27384, 1996, 2332, 2000, 6073, 2125, 1996, 10930, 3489, 10188, 1011, 4013, 2078, 1011, 18101, 2135, 1011, 4013, 2078, 1011, 2022, 1996, 4410, 2738, 2084, 1996, 2332, 2008, 1996, 7185, 4654, 2389, 2102, 2404, 2091, 3649, 9507, 2711, 2487, 1997, 14246, 2063, 2487, 1996, 2332, 2022, 2069, 2567, 2191, 1037, 24495, 5998, 2005, 2373, 1998, 4071, 1997, 3601, 1999, 3510, 2021, 2022, 2574, 9462, 1011, 4013, 2078, 1011, 2022, 8622, 2004, 2022, 1996, 2069, 8215, 2000, 1996, 14246, 2063, 2487, 2021, 1996, 3804, 1997, 2711, 2487, 2040, 2031, 2022, 2599, 2046, 1011, 4013, 2078, 1011, 7417, 2022, 3288, 2000, 1996, 3796, 13463, 1996, 12063, 1998, 7404, 1997, 2035, 14246, 2063, 2487, 9444, 4025, 4795, 2000, 1996, 2110, 2030, 2265, 2151, 4382, 1997, 4336, 2022, 2928, 2011, 1996, 7185, 1998, 9015, 1037, 20625, 10219, 2065, 2498, 2919, 2021, 2012, 1996, 2168, 2051, 1011, 4013, 2078, 1011, 2231, 2022, 9414, 1998, 2583, 1998, 5326, 14165, 2004, 2521, 2004, 2022, 2825, 2073, 2045, 2022, 2107, 1037, 14527, 1997, 3265, 4382, 1998, 6960, 8917, 2487, 2022, 2933, 1999, 2755, 2022, 2000, 2424, 1037, 4078, 11008, 2964, 2295, 1037, 7968, 1998, 2092, 2344, 4078, 11008, 2964, 2012, 2188, 2096, 1011, 4013, 2078, 1011, 2191, 14246, 2063, 2487, 2307, 2011, 9187, 6917, 1998, 2012, 2023, 2051, 1996, 16290, 1997, 14246, 2063, 2487, 2424, 1037, 18731, 2492, 1999, 1996, 2110, 2119, 1997, 14246, 2063, 2487, 1998, 1997, 14246, 2063, 2487, 1996, 2162, 1999, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 3058, 2487, 2162, 2031, 2022, 7385, 1999, 14246, 2063, 2487, 2005, 3058, 2487, 1998, 14246, 2063, 2487, 2031, 2202, 2053, 2112, 1999, 1011, 4013, 2078, 1011, 3458, 8627, 1996, 4496, 2361, 2487, 1998, 4496, 2361, 2487, 2004, 1996, 4099, 1997, 1996, 3750, 2021, 1996, 3343, 1997, 8917, 2487, 5478, 2008, 1996, 4487, 19729, 3258, 2090, 1011, 4013, 2078, 1011, 4496, 2361, 2487, 1998, 4496, 2361, 2487, 2110, 2323, 2022, 5441, 1998, 2043, 2518, 4088, 2000, 7166, 2875, 3521, 2013, 8203, 15575, 1996, 7185, 15115, 1998, 19653, 8917, 2487, 2000, 3613, 1996, 2162, 2011, 2507, 1011, 4013, 2078, 1011, 2769, 1998, 23895, 1037, 2162, 2031, 2525, 4088, 1999, 14246, 2063, 2487, 2006, 6852, 1997, 1996, 3804, 1997, 2196, 2015, 2040, 2031, 2468, 8215, 2000, 1996, 11068, 1997, 14246, 2063, 2487, 2021, 3005, 2155, 2031, 2444, 1999, 14246, 2063, 2487, 2061, 2146, 2008, 1996, 3750, 1998, 1996, 2332, 1997, 14246, 2063, 2487, 2490, 1037, 2062, 6802, 4366, 1997, 1996, 3804, 1997, 8917, 2487, 2000, 2112, 1997, 1996, 11068, 2738, 2084, 6449, 1037, 4496, 2361, 2487, 3159, 2046, 14246, 2063, 2487, 8917, 2487, 2022, 4248, 2000, 15126, 2023, 3653, 18209, 2005, 2886, 14246, 2063, 2487, 2005, 14246, 2063, 2487, 2022, 2085, 3280, 2046, 1037, 5410, 2373, 1998, 1011, 4013, 2078, 1011, 2156, 1999, 1996, 2162, 1037, 2965, 1997, 9878, 1996, 14246, 2063, 2487, 2029, 7141, 2000, 1996, 4496, 2361, 2487, 4410, 2012, 2030, 18979, 2140, 2487, 2498, 2590, 2022, 2079, 2021, 1996, 4496, 2361, 2487, 1998, 4496, 2361, 2487, 2022, 4929, 2041, 2096, 7185, 2487, 2402, 1998, 2583, 2952, 2022, 4982, 2039, 2426, 1996, 4496, 2361, 2487, 1996, 12182, 1997, 2711, 2487, 2402, 2365, 2000, 1996, 3804, 1997, 2711, 2487, 1998, 1996, 3804, 1997, 2711, 2487, 3449, 2094, 2365, 1997, 1996, 3159, 1997, 8917, 2487, 1998, 8917, 2487, 2022, 3343, 2574, 5851, 1037, 8235, 2476, 1997, 3112, 2711, 2487, 1998, 8917, 2487, 2035, 2991, 2046, 1996, 2192, 1997, 1996, 4496, 2361, 2487, 1998, 2013, 1037, 4574, 1997, 15556, 1996, 7185, 3622, 1996, 6771, 1997, 7185, 2487, 2390, 2004, 2092, 2004, 2191, 1011, 4013, 2078, 1011, 3571, 1998, 4847, 2011, 1996, 2878, 14246, 2063, 2487, 25022, 2078, 4160, 7733, 1996, 2197, 8837, 1011, 4013, 2078, 1011, 2031, 2507, 1996, 2332, 5436, 1011, 4013, 2078, 1011, 16857, 2007, 1996, 2393, 1997, 1996, 4496, 2361, 2487, 2021, 2022, 11487, 1998, 15389, 2043, 1996, 2307, 2704, 2022, 2525, 2012, 2331, 2022, 2341, 8917, 2487, 16755, 2019, 4496, 2361, 2487, 5011, 2711, 2487, 3183, 1011, 4013, 2078, 1011, 2031, 3345, 2000, 2147, 2104, 1011, 4013, 2078, 1011, 2000, 4287, 2006, 1996, 2231, 1998, 3280, 1999, 1996, 3058, 2487, 1997, 1996, 2332, 2069, 5788, 1011, 4013, 2078, 1011, 3058, 2487, 3280, 2006, 1996, 16215, 1997, 2089, 1996, 2162, 2022, 3613, 2006, 1996, 2240, 8917, 2487, 2031, 3913, 2091, 1998, 3058, 2487, 2044, 1996, 2331, 1997, 2711, 2487, 1996, 2390, 1999, 1996, 2659, 2406, 5114, 1037, 21459, 3377, 2012, 14246, 2063, 2487, 2104, 1996, 3804, 1997, 2711, 2487, 4498, 6033, 1996, 2214, 4496, 2361, 2487, 3939, 1996, 2645, 1997, 14246, 2063, 2487, 13926, 2989, 2368, 1998, 10014, 5333, 1996, 4476, 1997, 1996, 4496, 2361, 2487, 2236, 2000, 1996, 2152, 6510, 1998, 1999, 5547, 1996, 3750, 2000, 2191, 3521, 1999, 1996, 5036, 1997, 1049, 24978, 3334, 14246, 2063, 2487, 6855, 2004, 1011, 4013, 2078, 1011, 27594, 1996, 7185, 2487, 20664, 2015, 2711, 2487, 1998, 6904, 2278, 2487, 2103, 1999, 14246, 2063, 2487, 1998, 1996, 3103, 2094, 20420, 2007, 1996, 16394, 4232, 2237, 1997, 14246, 2063, 2487, 2021, 1996, 2162, 2007, 14246, 2063, 2487, 3613, 6229, 2043, 2711, 2487, 8526, 2000, 5914, 2711, 2487, 1037, 2684, 1997, 1996, 2332, 1997, 14246, 2063, 2487, 1996, 2711, 2487, 2043, 2019, 8215, 2031, 2146, 2022, 13905, 1997, 2711, 2487, 1996, 2564, 1997, 2711, 2487, 2031, 2468, 1996, 2388, 1997, 7185, 2487, 2365, 1996, 3449, 2094, 1997, 3183, 2711, 2487, 2022, 3058, 2487, 2012, 1996, 2051, 1997, 1011, 4013, 2078, 1011, 2269, 2022, 2331, 1996, 3035, 2388, 2468, 11315, 1998, 3404, 4498, 2000, 2711, 2487, 2040, 2031, 2468, 1037, 7185, 1998, 7323, 1996, 3343, 1997, 8917, 2487, 2021, 2054, 2031, 2022, 18094, 2013, 1037, 2158, 2011, 4182, 1037, 4496, 2361, 2487, 7015, 2022, 2046, 3917, 3085, 2013, 1037, 2659, 4562, 4496, 2361, 2487, 2044, 1996, 7006, 2272, 1996, 8917, 2487, 2022, 1996, 3038, 1998, 1996, 8917, 2487, 1997, 14246, 2063, 2487, 2191, 1037, 2197, 3233, 2011, 10214, 2000, 4236, 1996, 2548, 24754, 2005, 4840, 25964, 2022, 2490, 2119, 2011, 1996, 20934, 27172, 2121, 1997, 14246, 2063, 2487, 1998, 2011, 1037, 2307, 2193, 1997, 1996, 11760, 2040, 2022, 7714, 9981, 1997, 2711, 2487, 2023, 2283, 2022, 2655, 1996, 2711, 2487, 2138, 1999, 1011, 4013, 2078, 1011, 6594, 2169, 2158, 3233, 5743, 4888, 1011, 4013, 2078, 1011, 4613, 1998, 7822, 2074, 2004, 1996, 2879, 2079, 2007, 27076, 2711, 2487, 1998, 2962, 1999, 1996, 2395, 1996, 5998, 2468, 3809, 2021, 2069, 1037, 2261, 1997, 1996, 5160, 1999, 1996, 8917, 2487, 2031, 2151, 2613, 6958, 2030, 2270, 4382, 2035, 1996, 2060, 3364, 9298, 2389, 2041, 1997, 14225, 1998, 2283, 4382, 2191, 6994, 1997, 1996, 2158, 1997, 1996, 11739, 3183, 1011, 4013, 2078, 1011, 5223, 1998, 4078, 18136, 2063, 2295, 3262, 2521, 1011, 4013, 2078, 1011, 6020, 1999, 4276, 1998, 4454, 2711, 2487, 2907, 3435, 2011, 2711, 2487, 1998, 2022, 2490, 2011, 1996, 3804, 1997, 2711, 2487, 3183, 1011, 4013, 2078, 1011, 2269, 2022, 2331, 2031, 2191, 3159, 1997, 8917, 2487, 8917, 2487, 2022, 5375, 9585, 1011, 4013, 2078, 1011, 2000, 15823, 14246, 2063, 2487, 1998, 3288, 1996, 8917, 2487, 2000, 2744, 2029, 16519, 1996, 2030, 18979, 2140, 2487, 2552, 1997, 1996, 2711, 2487, 2007, 1996, 7221, 21808, 1997, 2711, 2487, 2004, 1037, 3521, 5378, 8917, 2487, 2174, 2468, 2061, 15818, 1998, 2058, 4783, 22397, 2008, 1996, 3035, 3426, 1011, 4013, 2078, 1011, 2000, 2022, 17727, 6935, 2239, 26090, 1011, 4013, 2078, 1011, 2564, 1998, 1011, 4013, 2078, 1011, 2060, 2767, 4088, 1037, 4840, 2162, 2005, 1011, 4013, 2078, 1011, 7931, 1998, 1996, 3035, 2022, 2486, 2000, 10750, 2021, 1011, 4013, 2078, 1011, 2153, 2265, 1011, 4013, 2078, 1011, 2061, 5939, 5521, 20913, 2008, 1996, 3035, 1998, 1996, 8917, 2487, 2468, 21063, 1998, 8917, 2487, 2000, 2404, 1011, 4013, 2078, 1011, 2091, 2507, 1996, 3094, 1997, 1996, 10123, 2000, 2711, 2487, 2153, 2045, 2022, 1037, 2645, 2012, 1996, 4796, 1997, 14246, 2063, 2487, 1999, 2029, 2035, 8917, 2487, 2022, 2767, 2022, 6357, 1998, 1011, 4013, 2078, 1011, 1011, 4013, 2078, 1011, 2061, 4498, 5409, 2008, 1011, 4013, 2078, 1011, 2031, 2000, 2175, 2046, 8340, 2043, 1011, 4013, 2078, 1011, 4607, 1996, 4496, 2361, 2487, 2326, 2096, 2711, 2487, 2709, 2000, 2373, 2012, 2188, 1996, 2457, 1997, 2711, 2487, 1996, 2457, 1997, 14246, 2063, 2487, 2295, 2196, 5760, 2022, 2172, 5335, 2076, 1996, 5853, 1997, 2711, 2487, 1998, 1996, 2711, 2487, 1997, 2711, 2487, 2045, 2022, 1037, 4382, 1997, 7472, 1998, 4519, 2055, 1011, 4013, 2078, 1011, 5399, 13988, 12618, 2271, 1998, 2110, 2135, 2021, 15436, 2135, 5760, 1998, 15514, 1998, 3243, 1037, 3357, 2041, 1997, 1996, 7977, 1998, 2330, 3580, 1997, 1996, 2280, 5853, 1996, 11017, 2711, 2487, 1037, 3203, 1997, 2307, 4519, 1998, 15966, 2191, 1011, 4013, 2078, 1011, 8917, 2487, 1996, 2803, 1997, 1037, 8235, 2554, 2029, 2275, 1011, 4013, 2078, 1011, 2000, 5333, 1998, 25416, 3170, 1996, 5450, 3906, 1998, 2653, 1997, 1996, 2051, 2053, 2773, 2008, 2022, 5136, 29364, 2030, 20392, 2022, 3499, 2000, 3413, 20327, 1998, 2295, 1999, 2832, 1997, 2051, 2023, 15657, 2468, 21877, 28210, 2594, 1998, 11612, 2045, 2022, 2053, 4797, 2008, 2172, 2022, 2079, 2000, 16405, 3089, 12031, 2119, 1996, 2653, 1998, 1996, 4309, 1997, 2245, 5961, 2377, 4958, 8004, 6444, 2711, 2487, 1998, 2130, 18408, 2022, 2128, 26560, 3366, 2077, 1996, 2837, 1997, 5510, 1999, 1996, 1044, 10093, 2711, 2487, 1998, 1037, 6919, 2047, 19220, 2022, 2045, 2507, 2025, 2069, 2000, 18200, 2021, 2000, 5024, 3906, 2116, 1997, 1996, 2307, 2158, 2040, 2191, 14246, 2063, 2487, 26150, 2022, 2593, 2203, 2030, 4088, 1011, 4013, 2078, 1011, 2476, 2012, 2023, 2051, 12558, 3015, 11974, 24299, 1998, 1996, 2839, 1997, 1996, 2158, 1998, 2450, 1997, 1996, 2457, 2022, 2113, 2000, 1011, 4013, 2078, 1011, 2006, 2035, 2217, 7185, 2711, 2487, 1998, 1996, 3804, 1997, 2711, 2487, 2119, 6171, 5117, 1999, 1996, 2711, 2487, 2031, 2681, 1996, 7185, 2487, 12558, 1996, 2060, 20446, 1997, 2307, 2373, 1997, 19728, 2711, 2487, 7185, 2487, 1997, 1996, 3035, 2022, 3203, 4339, 1037, 2440, 2381, 1997, 1996, 2457, 2711, 2487, 7185, 2487, 1997, 1996, 2307, 11067, 1997, 2035, 2051, 2022, 22476, 1011, 4013, 2078, 1011, 2000, 1996, 5553, 5054, 2923, 2023, 3412, 2283, 2061, 2655, 2013, 2711, 2487, 1037, 4496, 2361, 2487, 5011, 3005, 5448, 2022, 17727, 10421, 2000, 1011, 4013, 2078, 1011, 2031, 3500, 2039, 2105, 1996, 5290, 10664, 1997, 3417, 2548, 1998, 2193, 2426, 1011, 4013, 2078, 1011, 2070, 1997, 1996, 2583, 1998, 2204, 2158, 1997, 1996, 2051, 2021, 1996, 4496, 2361, 2487, 5136, 1011, 4013, 2078, 1011, 2000, 2907, 6270, 8998, 1998, 2045, 2022, 1037, 27222, 5981, 2203, 2012, 3091, 1999, 1996, 14522, 1997, 1996, 2711, 2487, 2022, 4992, 3661, 14451, 1996, 13840, 2291, 2022, 2426, 1996, 2583, 3015, 1997, 1996, 2287, 4695, 4623, 2671, 2381, 2396, 2022, 2035, 2191, 2307, 5082, 2295, 2045, 2022, 1037, 2110, 20942, 1998, 5337, 3012, 1999, 2035, 2008, 2022, 2360, 1998, 2079, 2417, 9890, 3372, 1997, 1996, 4496, 2361, 2487, 3035, 2022, 3802, 7413, 4674, 1998, 1996, 3435, 28173, 3560, 25416, 3170, 3672, 1997, 8917, 2487, 1997, 2711, 2487, 1996, 3535, 2013, 1996, 2220, 2051, 1997, 1996, 4496, 2361, 2487, 12078, 2031, 2022, 2000, 4009, 2035, 2231, 2046, 1996, 2192, 1997, 1996, 11074, 1998, 1996, 16341, 1997, 1996, 2711, 2487, 3143, 1996, 2147, 2711, 2487, 2295, 5665, 16957, 2022, 1037, 2158, 1997, 6196, 3754, 2172, 3068, 1998, 2307, 2486, 1997, 2839, 13368, 2013, 1037, 13769, 6772, 2008, 14246, 2063, 2487, 2022, 1996, 2030, 18979, 2140, 2487, 2406, 1999, 1996, 2088, 1998, 1011, 4013, 2078, 1011, 1996, 2030, 18979, 2140, 2487, 1997, 4496, 2361, 2487, 1998, 1011, 4013, 2078, 1011, 2031, 1037, 12047, 14571, 1997, 17183, 11219, 8162, 2029, 2061, 17894, 2035, 2040, 2272, 2379, 1011, 4013, 2078, 1011, 2004, 2000, 2191, 1011, 4013, 2078, 1011, 1011, 4013, 2078, 1011, 5627, 6658, 2045, 2022, 2438, 1999, 1011, 4013, 2078, 1011, 2000, 2191, 7185, 2487, 2332, 1998, 7185, 2487, 19416, 2158, 4661, 2022, 2054, 2711, 2487, 2360, 1997, 1011, 4013, 2078, 1011, 1998, 2043, 1999, 1996, 7185, 3280, 1996, 2332, 2265, 1011, 4013, 2078, 1011, 3929, 5020, 2000, 2468, 1011, 4013, 2078, 1011, 2219, 3539, 2704, 1996, 2110, 2022, 1011, 4013, 2078, 1011, 1011, 4013, 2078, 1011, 2360, 1998, 2035, 2803, 2588, 1011, 4013, 2078, 1011, 2061, 2008, 2053, 2282, 2022, 2681, 2005, 17689, 1996, 2457, 2022, 2174, 1999, 1037, 2087, 8235, 2110, 2045, 2031, 2022, 2019, 5866, 27719, 1997, 5848, 1997, 2296, 2785, 1999, 1996, 11320, 3363, 2044, 1996, 2162, 1997, 4676, 1998, 1999, 2236, 2228, 2121, 3063, 1998, 2158, 1997, 3906, 14246, 2063, 2487, 2022, 12890, 4138, 1996, 2332, 2031, 1037, 6919, 2373, 1997, 2969, 23617, 2029, 22476, 1011, 4013, 2078, 1011, 2035, 2000, 1011, 4013, 2078, 1011, 2471, 2004, 2065, 1011, 4013, 2078, 1011, 2022, 1037, 4066, 1997, 16968, 1996, 2110, 2135, 9603, 4496, 2361, 2487, 3802, 7413, 4674, 3288, 1999, 2011, 1011, 4013, 2078, 1011, 2388, 2711, 2487, 2468, 7078, 2019, 3194, 1997, 2231, 2711, 2487, 2031, 4088, 1996, 4763, 7661, 1997, 2562, 1996, 7015, 4251, 2011, 2507, 1011, 4013, 2078, 1011, 3663, 2012, 2457, 2007, 11550, 22476, 1998, 2122, 2436, 2022, 4800, 22086, 2000, 1996, 2087, 8216, 1998, 18691, 3014, 2061, 2008, 2296, 2548, 16115, 3351, 2031, 2070, 7185, 2487, 1997, 3167, 16742, 3159, 1997, 1996, 2668, 1998, 7015, 1997, 2296, 3014, 2022, 4180, 2098, 2000, 6865, 2055, 1996, 2457, 4306, 2046, 1996, 2087, 4867, 26859, 2012, 14246, 2063, 2487, 1998, 16215, 17583, 1011, 4013, 2078, 1011, 14405, 10624, 5358, 1998, 2000, 2022, 2344, 2000, 3961, 1999, 1996, 2406, 2022, 1037, 2087, 5729, 7750, 14246, 2063, 2487, 2104, 2711, 2487, 2045, 2022, 1999, 2755, 2498, 2021, 1996, 5252, 2000, 11494, 1037, 10170, 2006, 1011, 4013, 2078, 1011, 2219, 3776, 2005, 1011, 4013, 2078, 1011, 2022, 3499, 2053, 4611, 2030, 5368, 2169, 2874, 2031, 1037, 3099, 2030, 13566, 4630, 1037, 4066, 1997, 19007, 1998, 1996, 3447, 1997, 1996, 2103, 2022, 6133, 15897, 2006, 1996, 2112, 1997, 1996, 2332, 2130, 1996, 3664, 6855, 1011, 4013, 2078, 1011, 2695, 2011, 5309, 1996, 12511, 14539, 2031, 2000, 3477, 1999, 1996, 2030, 18979, 2140, 2487, 2173, 1996, 25964, 2000, 2231, 2041, 1997, 2029, 2022, 13366, 9447, 2019, 2046, 3917, 3085, 2193, 1997, 11550, 2116, 2005, 11809, 2436, 2279, 1996, 9278, 1998, 2349, 2029, 2490, 1011, 4013, 2078, 1011, 2935, 2022, 20700, 2012, 2457, 1998, 2353, 2135, 1996, 14841, 10760, 1998, 7408, 1997, 1996, 11646, 4661, 2029, 1011, 4013, 2078, 1011, 2022, 2655, 2125, 2013, 1996, 13142, 1997, 1011, 4013, 2078, 1011, 2219, 2492, 2005, 1037, 3056, 2193, 1997, 3058, 2487, 2000, 2147, 2012, 1996, 2346, 1011, 4013, 2078, 1011, 3586, 2089, 2022, 2224, 2011, 2548, 11981, 1011, 4013, 2078, 1011, 2935, 2022, 10416, 2031, 2000, 2022, 2131, 1999, 2011, 1011, 4013, 2078, 1011, 4428, 24665, 10450, 2015, 2096, 1011, 4013, 2078, 1011, 2219, 2022, 27594, 1998, 1999, 2460, 1996, 2069, 4687, 2022, 2129, 1011, 4013, 2078, 1011, 4839, 2012, 2035, 1011, 4013, 2078, 1011, 25215, 2140, 1998, 1011, 4013, 2078, 1011, 2833, 2022, 23277, 29574, 1998, 2151, 3535, 2000, 27950, 1011, 4013, 2078, 1011, 4650, 2006, 1996, 2112, 1997, 1011, 4013, 2078, 1011, 2935, 2052, 2031, 2022, 2298, 2006, 2004, 6655, 11045, 2078, 4795, 2640, 1998, 2763, 2031, 2455, 1011, 4013, 2078, 1011, 1999, 1996, 14246, 2063, 2487, 1996, 14539, 1997, 14246, 2063, 2487, 2073, 1996, 2214, 4552, 2031, 2022, 2625, 4498, 10083, 1998, 2216, 1997, 14246, 2063, 2487, 2022, 1999, 1037, 2625, 6728, 19811, 4650, 1998, 1999, 1996, 2103, 3119, 24299, 2711, 2487, 1996, 4012, 13876, 26611, 2236, 1997, 1996, 5446, 2022, 2061, 6581, 1037, 3208, 2008, 1996, 3778, 1997, 14952, 2022, 2203, 23086, 1999, 1011, 4013, 2078, 1011, 2051, 1998, 1011, 4013, 2078, 1011, 5326, 2047, 9922, 2107, 2004, 3221, 2012, 8917, 2487, 2012, 6904, 2278, 2487, 2012, 2711, 2487, 1011, 4013, 2078, 1011, 2036, 3046, 2000, 5326, 6236, 1998, 18962, 1998, 2000, 3443, 1037, 8917, 2487, 2045, 2022, 1037, 2307, 3311, 1997, 14165, 1998, 1999, 2296, 2533, 2045, 2022, 6919, 3754, 1996, 13708, 2031, 2599, 2000, 1037, 6196, 6308, 2426, 1996, 4496, 2361, 2487, 4496, 2361, 2487, 1011, 4013, 2078, 1011, 1996, 9208, 2267, 5323, 1999, 1996, 2197, 5853, 2031, 2172, 5335, 1996, 4309, 1997, 1996, 11646, 5795, 23361, 3387, 1997, 2711, 2487, 2022, 7185, 2487, 1997, 1996, 2087, 3602, 14512, 2040, 2412, 4839, 1998, 1042, 1050, 8840, 2078, 6507, 1997, 2711, 2487, 7185, 2487, 1997, 1996, 2204, 1997, 2158, 1037, 5290, 1997, 9009, 4088, 1999, 1996, 10664, 1997, 3417, 2548, 2203, 2011, 9958, 1998, 8587, 2362, 2070, 1997, 1996, 2087, 6581, 1998, 2583, 2711, 1999, 14246, 2063, 2487, 2426, 1011, 4013, 2078, 1011, 2711, 2487, 1037, 2158, 1997, 8348, 15534, 11067, 1998, 5995, 1997, 2245, 1998, 2711, 2487, 1996, 2708, 4496, 2361, 2487, 6918, 4802, 1011, 4013, 2078, 1011, 2708, 2472, 2711, 2487, 1997, 2711, 2487, 2022, 2174, 1037, 11136, 1997, 2711, 2487, 1037, 4496, 2361, 2487, 14925, 18954, 7951, 4588, 3005, 3193, 2006, 14689, 16344, 8557, 3160, 1997, 4519, 2022, 28887, 2011, 1996, 4496, 2361, 2487, 1998, 2004, 1996, 3417, 20796, 2052, 2025, 4487, 6499, 7962, 1996, 8998, 17961, 2000, 1011, 4013, 2078, 1011, 1011, 4013, 2078, 1011, 2022, 28085, 1998, 2566, 3366, 26869, 2802, 14246, 2063, 2487, 2022, 5853, 2062, 2138, 1011, 4013, 2078, 1011, 2022, 9981, 1997, 2054, 2052, 2025, 8815, 2000, 1011, 4013, 2078, 1011, 2097, 2084, 2005, 2151, 2613, 2215, 1997, 23758, 3012, 2711, 2487, 2022, 3297, 4992, 3661, 2022, 2404, 5743, 2076, 2023, 6704, 1998, 1999, 2755, 1996, 3906, 1997, 14246, 2063, 2487, 3362, 1011, 4013, 2078, 1011, 13635, 2078, 2287, 2076, 2023, 5853, 1998, 1996, 2653, 9878, 1011, 4013, 2078, 1011, 3115, 15401, 2162, 1999, 1996, 2659, 2406, 2711, 2487, 1996, 3035, 1997, 2711, 2487, 2022, 1996, 2775, 1997, 1996, 2030, 18979, 2140, 2487, 3510, 1997, 2711, 2487, 1997, 14246, 2063, 2487, 1998, 2006, 1011, 4013, 2078, 1011, 2269, 2022, 2331, 1999, 14246, 2063, 2487, 2006, 3653, 18209, 1997, 2019, 2214, 2375, 1999, 14246, 2063, 2487, 2029, 2507, 1996, 2684, 1997, 1037, 2030, 18979, 2140, 2487, 3510, 1996, 12157, 2058, 1996, 2365, 1997, 1037, 2030, 18979, 2140, 2487, 4366, 1996, 2659, 2406, 2013, 1996, 2402, 2711, 2487, 1997, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 2947, 4088, 1037, 2162, 2029, 2022, 2428, 1037, 9530, 7629, 26620, 1997, 1996, 2214, 5998, 2090, 14246, 2063, 2487, 1998, 2711, 2487, 1998, 1997, 1996, 26911, 1997, 14246, 2063, 2487, 2000, 7683, 1011, 4013, 2078, 1011, 8880, 2000, 1996, 8840, 2278, 2487, 2012, 2030, 18979, 2140, 2487, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 2114, 1011, 4013, 2078, 1011, 1998, 27885, 3669, 3351, 1011, 4013, 2078, 1011, 2000, 2191, 1996, 3521, 1997, 8917, 2487, 1999, 2021, 1011, 4013, 2078, 1011, 2059, 9510, 1999, 26470, 2711, 2487, 2000, 2005, 3736, 3489, 1996, 3426, 1997, 1996, 4496, 2361, 2487, 1998, 1996, 2162, 2022, 20687, 1999, 2711, 2487, 1997, 14246, 2063, 2487, 2022, 2087, 4340, 4099, 2083, 2166, 2562, 2039, 1996, 4382, 1997, 1996, 4496, 2361, 2487, 1998, 1011, 4013, 2078, 1011, 6855, 4681, 2013, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 2083, 3058, 2487, 6659, 2162, 1999, 2029, 1996, 2307, 2711, 2487, 2022, 3102, 2012, 5474, 2480, 7693, 1999, 14246, 2063, 2487, 2012, 2197, 2013, 15575, 2035, 2283, 2022, 4012, 11880, 2000, 16519, 1996, 3521, 1997, 9152, 4168, 9077, 2078, 1999, 2202, 5056, 1997, 6151, 28344, 2744, 1999, 2023, 14246, 2063, 2487, 15126, 2536, 2103, 7141, 2000, 4496, 2361, 2487, 3159, 1998, 10655, 1996, 2489, 4461, 2103, 1997, 14246, 2063, 2487, 2043, 2035, 14246, 2063, 2487, 2022, 2205, 2172, 4929, 2041, 2011, 2724, 2487, 2000, 3749, 5012, 14246, 2063, 2487, 2022, 2440, 1997, 2969, 1043, 10626, 9031, 1996, 2332, 2022, 3193, 2471, 2004, 1037, 14246, 2063, 2487, 1998, 1996, 11867, 7770, 26797, 2099, 1997, 1011, 4013, 2078, 1011, 2457, 1998, 1997, 1011, 4013, 2078, 1011, 2311, 2926, 1996, 4186, 2012, 14246, 2063, 2487, 2007, 1011, 4013, 2078, 1011, 3871, 1998, 9545, 2562, 2039, 1996, 3972, 14499, 1997, 1011, 4013, 2078, 1011, 2307, 2791, 7065, 23909, 1997, 1996, 24754, 1997, 14246, 2063, 2487, 1999, 14246, 2063, 2487, 6814, 2008, 1996, 8917, 2487, 2031, 2022, 2061, 5547, 1999, 2193, 2008, 1996, 24754, 1997, 14246, 2063, 2487, 2071, 2022, 21825, 2035, 4071, 1997, 7425, 2022, 9772, 1011, 4013, 2078, 1011, 1011, 4013, 2078, 1011, 2704, 2022, 7221, 4509, 2021, 1011, 4013, 2078, 1011, 19311, 2022, 2025, 3499, 2000, 3582, 1011, 4013, 2078, 1011, 2065, 2202, 2096, 3046, 2000, 4019, 2158, 2022, 4604, 2000, 1996, 27124, 2450, 2000, 16187, 1998, 2775, 2000, 10664, 2005, 2495, 8011, 7828, 2022, 4284, 2006, 2155, 2000, 21741, 1011, 4013, 2078, 1011, 2046, 2175, 2000, 3742, 1037, 2261, 2191, 2132, 1999, 1996, 3748, 16808, 1997, 1996, 8292, 8159, 2638, 2104, 1037, 9191, 3360, 2171, 28778, 1998, 2060, 18094, 5729, 14522, 1999, 1996, 2148, 1997, 14246, 2063, 2487, 8011, 7828, 2022, 4284, 2006, 1011, 4013, 2078, 1011, 2040, 2191, 1011, 4013, 2078, 1011, 1011, 4013, 2078, 1011, 2449, 2000, 21741, 1998, 15301, 1011, 4013, 2078, 1011, 1011, 4013, 2078, 1011, 3510, 2022, 13520, 19528, 1011, 4013, 2078, 1011, 2775, 2202, 2013, 1011, 4013, 2078, 1011, 2000, 2022, 16957, 1999, 1996, 4496, 2361, 2487, 4496, 2361, 2487, 4752, 1037, 2307, 2193, 3815, 2000, 2012, 2560, 9510, 1999, 4019, 15897, 2000, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 1059, 16584, 5886, 1011, 4013, 2078, 1011, 4287, 2116, 1997, 1996, 9922, 2008, 2711, 2487, 2031, 2202, 2061, 2172, 3255, 2000, 5323, 2116, 1997, 2216, 2040, 7392, 1999, 14246, 2063, 2487, 2022, 6953, 14077, 1998, 1037, 2312, 5701, 2022, 2947, 5323, 2012, 14246, 2063, 2487, 2029, 2146, 2562, 2039, 1011, 4013, 2078, 1011, 4496, 2361, 2487, 2839, 1996, 2162, 1997, 1996, 18990, 2023, 12077, 2552, 1997, 5939, 5521, 4890, 2022, 3582, 2011, 1037, 4840, 2886, 2006, 14246, 2063, 2487, 2006, 1996, 14865, 1997, 1037, 6814, 12839, 1997, 1011, 4013, 2078, 1011, 2905, 1999, 2375, 1996, 11017, 1997, 14246, 2063, 2487, 18445, 1996, 18990, 2006, 1996, 8840, 2278, 2487, 1998, 4287, 2006, 7185, 2487, 1997, 1996, 2087, 27863, 2162, 1999, 2381, 2096, 1011, 4013, 2078, 1011, 2022, 2012, 1996, 2168, 2051, 2490, 1996, 3426, 1997, 1011, 4013, 2078, 1011, 5542, 2711, 2487, 1997, 14246, 2063, 2487, 2044, 1011, 4013, 2078, 1011, 2031, 10574, 1998, 19935, 24695, 2006, 1996, 5508, 1997, 2711, 2487, 2076, 2023, 2162, 2174, 2008, 4245, 1997, 2583, 2158, 2040, 2031, 4982, 2039, 2007, 14246, 2063, 2487, 4088, 2000, 3413, 2185, 1998, 1011, 4013, 2078, 1011, 3112, 2022, 2025, 2061, 6375, 2096, 2711, 2487, 2022, 2757, 14952, 4088, 2000, 2022, 2062, 2514, 2011, 1996, 9069, 2111, 1998, 3521, 2022, 2191, 2012, 2711, 2487, 1999, 1996, 2162, 1997, 1996, 8338, 1999, 14246, 2063, 2487, 1996, 2197, 1997, 1996, 7185, 2487, 2307, 2162, 1997, 14246, 2063, 2487, 2022, 5853, 2022, 2521, 2062, 15140, 2711, 2487, 1997, 14246, 2063, 2487, 3280, 26983, 10324, 2004, 1011, 4013, 2078, 1011, 6332, 1037, 4496, 2361, 2487, 3159, 2711, 2487, 1997, 14246, 2063, 2487, 1996, 2030, 18979, 2140, 2487, 2365, 1997, 1996, 2069, 2365, 1997, 2711, 2487, 2022, 3449, 2094, 2905, 1996, 3035, 1997, 2711, 2487, 2021, 1996, 2373, 1997, 8840, 2278, 2487, 2012, 1996, 3521, 1997, 2711, 2487, 2031, 5993, 2008, 1996, 4410, 1997, 14246, 2063, 2487, 2323, 2175, 2000, 2711, 2487, 1997, 14246, 2063, 2487, 2030, 18979, 2140, 2487, 2365, 1997, 1996, 3750, 2711, 2487, 2040, 2022, 1996, 12608, 1997, 2402, 2905, 1997, 1996, 2548, 4496, 2361, 2487, 2240, 2021, 2079, 2025, 4654, 17847, 1996, 3571, 1998, 14225, 1997, 8840, 2278, 2487, 2004, 2079, 1037, 16596, 2239, 1997, 1996, 2525, 2058, 28394, 5582, 8917, 2487, 1997, 15477, 2023, 2599, 2000, 1996, 2162, 1997, 1996, 4496, 2361, 2487, 8338, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 2490, 2711, 2487, 1998, 2954, 2007, 14246, 2063, 2487, 1999, 14246, 2063, 2487, 8917, 2487, 1998, 1996, 2659, 2406, 1999, 14246, 2063, 2487, 2022, 4821, 3144, 1998, 1011, 4013, 2078, 1011, 7631, 2711, 2487, 9279, 1996, 6106, 2021, 1996, 10123, 2029, 1011, 4013, 2078, 1011, 9698, 1996, 20374, 1997, 14246, 2063, 2487, 8970, 2046, 14246, 2063, 2487, 2022, 6135, 16857, 2012, 14246, 2063, 2487, 2011, 1996, 4496, 2361, 2487, 2390, 2104, 1996, 3804, 1997, 14246, 2063, 2487, 1998, 1996, 4496, 2361, 2487, 2104, 3159, 2711, 2487, 1037, 2365, 1997, 1037, 2402, 3589, 1997, 1996, 8917, 2487, 1997, 8917, 2487, 2031, 2022, 8843, 2039, 1999, 14246, 2063, 2487, 2021, 2031, 19248, 2125, 10497, 14246, 2063, 2487, 2011, 2655, 1011, 4013, 2078, 1011, 1037, 2754, 2332, 2005, 2265, 1998, 1037, 7433, 2332, 2005, 2224, 2031, 4607, 1996, 3750, 2022, 2326, 1998, 2022, 7185, 2487, 1997, 1011, 4013, 2078, 1011, 2708, 4099, 1011, 4013, 2078, 1011, 4681, 1011, 4013, 2078, 1011, 5542, 3804, 2711, 2487, 1997, 8917, 2487, 1999, 16360, 28426, 2063, 1996, 4496, 2361, 2487, 2886, 1999, 3058, 2487, 5114, 1037, 2307, 3377, 2012, 14246, 2063, 2487, 1998, 5083, 2046, 8917, 2487, 2022, 10655, 1999, 2440, 2476, 1997, 3377, 1999, 1996, 2659, 2406, 1998, 5114, 2045, 1996, 2645, 1997, 8840, 2278, 2487, 1997, 8917, 2487, 2031, 2041, 3669, 3726, 1011, 4013, 2078, 1011, 2204, 7280, 1011, 4013, 2078, 1011, 2307, 2236, 1998, 17689, 2031, 3413, 2185, 1996, 2406, 2022, 15095, 15625, 2022, 8336, 2006, 1996, 23277, 29574, 14539, 2854, 4425, 2071, 2025, 2022, 2424, 1998, 7185, 2487, 2103, 2044, 2178, 1997, 2216, 14246, 2063, 2487, 2031, 15126, 2022, 2128, 15166, 2047, 3377, 2012, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 2022, 5114, 2058, 1996, 4496, 2361, 2487, 2390, 1998, 2295, 14246, 2063, 2487, 2022, 2004, 24501, 4747, 10421, 1998, 6151, 4887, 14706, 2004, 2412, 1011, 4013, 2078, 1011, 6771, 2022, 1999, 1037, 7143, 2110, 2043, 1011, 4013, 2078, 1011, 2022, 3828, 2011, 1037, 5573, 2689, 1997, 3343, 2006, 1996, 2112, 1997, 3035, 2711, 2487, 1997, 14246, 2063, 2487, 2040, 9131, 1011, 4013, 2078, 1011, 2390, 1998, 2681, 1011, 4013, 2078, 1011, 9698, 2000, 3613, 1996, 5049, 2894, 2711, 2487, 2022, 2025, 1037, 2674, 2005, 14246, 2063, 2487, 2302, 14246, 2063, 2487, 1998, 1996, 27463, 2711, 2487, 2031, 9510, 1011, 4013, 2078, 1011, 2567, 1996, 3750, 2507, 2039, 1011, 4013, 2078, 1011, 3653, 29048, 2000, 1996, 4410, 1997, 14246, 2063, 2487, 2061, 2008, 1011, 4013, 2078, 1011, 2468, 2825, 2000, 16519, 1037, 2236, 3521, 2012, 14246, 2063, 2487, 1999, 2011, 2023, 2051, 14246, 2063, 2487, 2022, 3058, 2487, 1998, 2031, 9015, 24665, 2666, 6767, 2271, 2155, 3279, 2030, 18979, 2140, 2487, 2011, 1996, 2331, 1997, 1011, 4013, 2078, 1011, 2069, 2365, 1998, 2059, 1997, 1011, 4013, 2078, 1011, 3449, 2094, 7631, 1037, 2402, 2158, 1997, 2172, 4872, 1997, 8012, 2040, 2007, 1011, 4013, 2078, 1011, 2564, 3280, 1997, 16007, 27881, 2033, 3022, 2571, 2763, 2013, 21591, 2966, 3949, 2144, 1011, 4013, 2078, 1011, 10527, 3005, 7355, 2022, 19819, 2011, 1011, 4013, 2078, 1011, 6821, 2022, 1996, 2069, 7185, 2487, 1997, 1996, 2155, 2040, 5788, 1996, 2214, 2332, 1999, 8741, 1997, 14038, 1998, 7901, 2000, 4014, 2007, 11424, 22930, 3085, 2943, 2000, 1996, 2203, 1997, 1011, 4013, 2078, 1011, 5853, 1996, 2146, 2006, 2501, 2031, 2197, 3058, 2487, 2043, 1011, 4013, 2078, 1011, 3280, 1999, 1011, 4013, 2078, 1011, 2031, 5333, 1996, 4496, 2361, 2487, 4410, 2000, 1011, 4013, 2078, 1011, 2307, 11867, 7770, 26797, 2099, 2021, 2031, 8688, 1996, 2406, 2000, 1011, 4013, 2078, 1011, 1998, 1011, 4013, 2078, 1011, 6270, 9366, 1997, 2307, 2791, 1996, 2711, 2487, 1996, 4410, 2085, 18855, 2000, 2711, 2487, 1037, 17541, 2775, 1997, 3058, 2487, 1011, 4013, 2078, 1011, 2307, 5615, 2031, 3046, 2000, 3073, 2005, 1011, 4013, 2078, 1011, 2204, 2011, 2681, 1996, 2708, 2835, 1999, 8917, 2487, 2000, 1011, 4013, 2078, 1011, 2219, 18102, 2365, 1996, 3804, 1997, 14246, 2063, 2487, 1996, 2087, 7481, 1998, 9530, 11020, 11638, 6313, 2158, 2059, 1999, 1996, 2155, 2021, 2295, 12266, 4895, 14244, 1998, 2200, 19657, 1011, 4013, 2078, 1011, 4182, 3426, 1996, 6098, 2000, 2022, 3193, 2004, 2019, 19006, 2011, 1996, 11760, 1998, 1996, 2332, 2022, 2097, 2022, 2275, 4998, 1996, 2030, 18979, 2140, 2487, 3159, 1997, 1996, 2668, 2548, 2711, 2487, 1997, 14246, 2063, 2487, 1996, 2397, 2332, 2022, 7833, 2468, 7082, 11315, 1037, 2158, 1997, 2204, 3754, 2021, 1997, 3733, 11424, 16136, 3267, 1998, 2040, 1999, 1996, 16306, 18373, 2791, 1997, 1011, 4013, 2078, 1011, 2166, 2031, 2468, 4487, 18719, 17585, 1998, 13925, 3458, 2035, 9647, 2030, 6412, 1011, 4013, 2078, 1011, 2022, 19045, 1998, 24665, 20113, 1998, 1011, 4013, 2078, 1011, 2388, 2360, 1997, 1011, 4013, 2078, 1011, 2008, 1011, 4013, 2078, 1011, 2022, 2066, 1996, 3159, 1999, 1037, 28458, 3183, 2035, 1996, 8867, 2031, 2203, 5004, 2007, 5592, 3272, 7185, 2487, 16007, 27881, 11867, 17625, 2040, 2031, 4652, 2151, 7927, 2108, 1997, 2224, 2000, 1011, 4013, 2078, 1011, 1999, 1996, 2236, 15575, 3965, 2011, 1996, 2162, 1997, 2711, 2487, 1037, 2711, 2487, 2171, 2711, 2487, 4088, 1996, 2307, 2291, 1997, 8892, 12143, 2029, 2031, 3613, 2412, 2144, 2000, 8915, 27718, 2111, 2000, 1011, 4013, 2078, 1011, 10083, 1011, 4013, 2078, 1011, 3046, 5333, 7680, 1997, 2769, 2006, 2120, 4923, 1998, 2036, 14386, 3366, 1037, 2194, 2040, 2022, 2000, 18496, 2769, 2000, 2424, 1037, 2307, 4093, 2006, 1996, 8840, 2278, 2487, 1996, 2709, 2013, 2029, 2022, 2000, 2022, 8216, 2296, 7185, 2487, 28699, 9869, 1999, 3745, 1998, 1996, 3748, 8277, 3653, 3567, 4014, 2375, 2022, 8917, 2487, 2022, 11240, 2011, 2111, 6148, 4357, 2007, 1011, 4013, 2078, 1011, 1998, 7015, 14249, 1011, 4013, 2078, 1011, 1999, 18475, 2000, 2131, 3229, 2000, 1011, 4013, 2078, 1011, 7280, 2022, 2191, 3058, 2487, 1998, 4558, 1996, 2279, 1998, 2633, 1996, 2878, 2933, 6011, 2000, 2031, 2022, 1037, 8210, 14040, 7971, 5679, 10083, 3582, 1998, 1996, 14624, 1997, 1996, 2406, 3623, 1996, 3804, 1997, 14246, 2063, 2487, 3280, 3402, 1999, 1996, 2332, 2022, 2085, 10142, 1997, 2287, 2021, 1011, 4013, 2078, 1011, 2022, 10634, 1998, 8848, 1998, 2210, 4906, 2005, 2231, 1998, 1996, 2406, 2022, 2428, 3627, 2011, 1996, 3804, 1997, 15477, 1998, 2044, 1011, 4013, 2078, 1011, 2011, 2711, 2487, 2019, 4793, 17689, 2021, 6039, 2007, 1996, 2168, 5679, 1997, 16290, 2004, 8917, 2487, 2030, 2711, 2487, 2162, 1997, 1996, 4496, 2361, 2487, 8338, 2947, 14246, 2063, 2487, 25912, 2046, 2047, 2162, 2711, 2487, 5914, 1996, 2684, 1997, 2711, 2487, 1037, 4496, 2361, 2487, 7015, 2040, 2044, 2022, 5333, 2000, 1996, 6106, 2022, 4654, 11880, 2011, 4496, 2361, 2487, 20014, 27611, 1998, 4808, 14246, 2063, 2487, 2022, 27885, 3669, 3351, 2000, 2202, 2039, 2849, 2006, 6852, 1997, 1011, 4013, 2078, 1011, 2269, 1999, 2375, 2021, 2022, 4965, 2125, 2011, 1037, 5592, 2013, 1996, 3750, 2711, 2487, 1997, 1996, 11068, 1997, 2711, 2487, 2000, 2711, 2487, 2000, 7065, 8743, 2000, 1011, 4013, 2078, 1011, 2684, 2044, 1011, 4013, 2078, 1011, 2331, 1998, 2947, 2468, 8917, 2487, 2000, 14246, 2063, 2487, 7141, 2000, 3804, 2711, 2487, 1996, 3129, 1997, 2711, 2487, 3449, 2094, 2684, 2000, 1996, 3750, 1998, 2711, 2487, 4374, 2612, 1996, 11068, 1997, 14246, 2063, 2487, 2096, 2035, 1996, 2708, 2373, 1999, 8840, 2278, 2487, 5993, 2000, 1996, 2061, 2655, 10975, 8490, 12644, 2624, 7542, 2011, 2029, 2711, 2487, 10037, 2008, 2711, 2487, 2323, 22490, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 1998, 1996, 2060, 14800, 2110, 2006, 1011, 4013, 2078, 1011, 2269, 2022, 2331, 2000, 1996, 15945, 1997, 1996, 2684, 1997, 1011, 4013, 2078, 1011, 3449, 2094, 2567, 2711, 2487, 2043, 2711, 2487, 3280, 2174, 1999, 1037, 2724, 2487, 4088, 2006, 2023, 3043, 2711, 2487, 1997, 14246, 2063, 2487, 2052, 4445, 3499, 2711, 2487, 2022, 4366, 2000, 1996, 14800, 2110, 4496, 3693, 1999, 11322, 1011, 4013, 2078, 1011, 3129, 2000, 1996, 3400, 1998, 14246, 2063, 2487, 2202, 2112, 2114, 1011, 4013, 2078, 1011, 4604, 8610, 2711, 2487, 2000, 2490, 1996, 20374, 1997, 14246, 2063, 2487, 2040, 2031, 2022, 5454, 3750, 2711, 2487, 1997, 14246, 2063, 2487, 2907, 2007, 2711, 2487, 1998, 5114, 1037, 3377, 2058, 1996, 4496, 2361, 2487, 2012, 8917, 2487, 1999, 2711, 2487, 2059, 3693, 1011, 4013, 2078, 1011, 2390, 1998, 1996, 2645, 1997, 15489, 16515, 2100, 1999, 2022, 7185, 2487, 1997, 1996, 4678, 3377, 1997, 14246, 2063, 2487, 2058, 14246, 2063, 2487, 2178, 3377, 3582, 2012, 2711, 2487, 2021, 6974, 14246, 2063, 2487, 2031, 2031, 3082, 3279, 1998, 1999, 2044, 1996, 2331, 1997, 2711, 2487, 2022, 2191, 2012, 8917, 2487, 3058, 2487, 10634, 1998, 14337, 2011, 3267, 2031, 2022, 7078, 2599, 2046, 3580, 2011, 1011, 4013, 2078, 1011, 2457, 3771, 2926, 1996, 3804, 1997, 15477, 2040, 3571, 1011, 4013, 2078, 1011, 2468, 3161, 1999, 2270, 6771, 1011, 4013, 2078, 1011, 2031, 2053, 3168, 1997, 4611, 2000, 1011, 4013, 2078, 1011, 2111, 1998, 6168, 1011, 4013, 2078, 1011, 2307, 5615, 2031, 6148, 4653, 1998, 2061, 2655, 8294, 1011, 4013, 2078, 1011, 2729, 9578, 2005, 5165, 1998, 2008, 1997, 1996, 7977, 1998, 2087, 18753, 2344, 2061, 2008, 1011, 4013, 2078, 1011, 2457, 2022, 1037, 2980, 8270, 1997, 9467, 3238, 3580, 2035, 2008, 2071, 2022, 23277, 2075, 2013, 1996, 25488, 2406, 2022, 22689, 2006, 1996, 26433, 5069, 1997, 2296, 2266, 1997, 1996, 2548, 2155, 1999, 11550, 2000, 7015, 1998, 1999, 9467, 3993, 9778, 1997, 1996, 2332, 1999, 2178, 2162, 3338, 2041, 1999, 9509, 1997, 1996, 11150, 2681, 2090, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 2011, 1996, 2280, 5998, 2711, 2487, 2031, 2011, 2711, 2487, 1011, 4013, 2078, 1011, 11276, 2000, 2031, 25134, 5114, 2058, 14246, 2063, 2487, 2000, 2202, 2112, 2007, 1011, 4013, 2078, 1011, 1998, 14246, 2063, 2487, 2022, 8917, 2487, 2007, 2711, 2487, 1999, 2023, 2162, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 15897, 2954, 1999, 1011, 4013, 2078, 1011, 6802, 6664, 2073, 1996, 4496, 2361, 2487, 2022, 27423, 3144, 1998, 2044, 3058, 2487, 2178, 3521, 3582, 2681, 1996, 6192, 1997, 1996, 4496, 2361, 2487, 2110, 2074, 2073, 1011, 4013, 2078, 1011, 2022, 2077, 2044, 1037, 25966, 3993, 3815, 1997, 2668, 14740, 2021, 14246, 2063, 2487, 2031, 2031, 6659, 3279, 1011, 4013, 2078, 1011, 2022, 3298, 2013, 14246, 2063, 2487, 1998, 4558, 2035, 1011, 4013, 2078, 1011, 4093, 1999, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 14246, 2063, 2487, 2104, 2711, 2487, 12507, 1996, 7977, 3580, 1998, 5622, 13013, 6313, 2791, 1997, 1996, 2332, 2022, 3458, 6412, 1998, 1996, 11760, 9279, 2055, 1996, 2457, 2011, 1996, 2291, 5323, 2011, 2711, 2487, 2022, 2065, 2025, 1011, 4013, 2078, 1011, 5020, 1999, 4126, 8053, 2655, 3560, 2000, 1996, 6114, 3426, 2011, 1996, 18555, 6450, 2791, 1997, 1996, 2457, 1996, 2878, 3465, 1997, 2029, 2022, 13366, 9447, 2011, 1996, 20934, 27172, 2121, 1998, 14539, 2053, 25964, 2022, 3198, 2013, 11646, 2030, 7015, 1998, 2023, 3732, 2744, 2421, 2035, 3500, 1997, 1037, 7015, 2240, 2000, 1996, 27917, 4245, 1996, 3954, 1997, 2019, 3776, 2031, 2053, 2812, 1997, 5770, 1011, 4013, 2078, 1011, 16713, 2130, 2065, 1011, 4013, 2078, 1011, 4299, 1011, 4013, 2078, 1011, 2005, 2035, 3043, 2130, 1997, 2334, 2231, 12530, 2006, 1996, 4410, 2035, 1011, 4013, 2078, 1011, 2071, 2079, 2022, 2000, 4009, 1011, 4013, 2078, 1011, 3318, 2013, 1011, 4013, 2078, 1011, 1998, 1011, 4013, 2078, 1011, 2022, 2411, 2486, 2593, 2011, 5635, 2030, 2011, 1011, 4013, 2078, 1011, 6450, 2166, 2000, 10178, 2000, 1996, 27917, 1996, 2214, 16708, 2291, 2065, 1011, 4013, 2078, 1011, 2444, 2012, 2457, 1011, 4013, 2078, 1011, 10961, 2022, 3082, 1998, 2069, 6576, 3113, 2011, 1011, 4013, 2078, 1011, 11550, 10655, 5333, 2013, 1996, 25964, 3477, 2011, 1996, 3532, 7500, 2065, 1011, 4013, 2078, 1011, 2444, 1999, 1996, 2406, 1011, 4013, 2078, 1011, 2022, 1037, 2145, 2307, 26508, 1998, 2022, 2655, 2011, 1996, 2111, 1037, 8840, 2278, 2487, 2030, 20497, 2053, 2476, 2022, 2330, 2000, 1011, 4013, 2078, 1011, 2402, 2365, 3272, 1999, 1996, 2457, 1996, 2277, 2030, 1996, 2390, 1998, 2182, 1011, 4013, 2078, 1011, 18847, 18155, 4697, 1996, 3396, 6855, 2035, 1996, 4138, 5801, 1998, 6103, 1998, 2035, 1996, 4712, 1999, 1996, 2390, 1996, 23848, 2923, 22648, 2100, 2022, 2471, 2035, 14800, 2426, 5160, 2040, 2031, 4965, 1011, 4013, 2078, 1011, 2005, 1011, 4013, 2078, 1011, 2155, 2013, 1996, 4410, 1998, 3477, 2005, 1996, 6098, 1997, 2169, 2365, 1996, 2880, 22476, 2000, 2169, 2266, 1997, 1996, 2548, 2155, 2022, 2471, 9788, 1999, 2193, 1998, 2035, 3477, 2011, 1996, 25964, 1996, 2214, 8917, 2487, 2030, 5474, 4171, 2031, 2175, 2006, 2412, 2144, 2724, 2487, 1998, 2296, 2266, 1997, 1037, 2155, 2031, 2000, 3477, 1011, 4013, 2078, 1011, 2025, 15802, 2000, 2054, 1011, 4013, 2078, 1011, 2224, 2021, 2054, 1011, 4013, 2078, 1011, 2022, 6814, 2000, 2342, 2296, 10369, 2022, 3446, 2012, 2054, 1011, 4013, 2078, 1011, 11276, 2000, 5478, 2005, 5474, 2296, 11190, 8351, 2030, 21863, 2031, 1037, 9565, 2000, 3477, 2000, 2332, 2935, 3387, 2823, 2036, 2000, 5011, 1998, 6103, 1996, 14539, 2022, 2655, 2125, 2013, 1011, 4013, 2078, 1011, 2219, 2147, 2000, 2507, 1996, 2349, 1997, 4428, 2000, 1996, 2346, 2030, 2000, 1011, 4013, 2078, 1011, 2935, 1011, 4013, 2078, 1011, 2089, 2025, 3659, 2158, 5397, 2008, 2071, 15115, 2007, 1996, 2208, 4496, 3298, 2185, 1996, 26079, 2008, 4521, 1011, 4013, 2078, 1011, 9781, 2061, 13594, 3723, 2022, 1011, 4013, 2078, 1011, 10416, 2008, 15625, 22889, 4710, 7185, 2487, 3413, 22719, 1998, 2130, 2065, 2011, 2151, 4687, 14165, 2868, 2006, 1996, 14539, 1011, 4013, 2078, 1011, 4241, 12096, 2025, 2444, 1999, 2151, 2785, 1997, 7216, 26693, 1996, 17946, 1997, 1011, 4013, 2078, 1011, 2935, 2030, 1997, 2231, 2323, 13433, 17457, 2006, 1011, 4013, 2078, 1011, 7177, 4668, 12507, 2045, 2022, 1037, 2844, 3110, 2008, 2689, 2442, 2272, 4556, 3906, 2022, 2817, 1998, 4496, 2361, 2487, 1998, 4496, 2361, 2487, 5450, 1998, 5145, 2022, 2228, 7812, 15401, 2045, 2022, 2307, 12721, 2012, 1996, 10768, 12079, 1997, 1037, 3811, 7976, 2166, 1999, 2029, 2296, 7185, 2487, 2022, 14187, 1998, 2012, 1996, 5145, 2029, 2031, 2022, 2061, 28616, 8557, 3213, 13368, 2426, 3183, 2711, 2487, 1998, 2711, 2487, 2022, 1996, 2087, 14953, 2040, 6614, 2012, 1996, 16857, 1997, 2035, 1996, 2801, 2029, 2031, 2272, 2000, 2022, 2947, 6905, 1996, 7185, 2487, 2011, 1011, 4013, 2078, 1011, 6187, 19966, 2594, 15966, 1996, 2060, 2011, 1011, 4013, 2078, 1011, 14727, 17839, 5114, 5627, 4540, 1998, 1996, 3213, 1999, 1037, 2307, 4372, 5666, 20464, 7361, 22939, 2059, 1999, 2607, 1997, 4772, 9530, 18886, 3726, 2000, 2886, 2087, 1997, 1996, 9366, 2029, 2031, 2022, 2718, 5886, 3406, 2202, 2005, 3946, 1998, 2022, 4876, 7532, 2007, 4752, 1998, 2007, 2231, 1996, 2332, 1011, 4013, 2078, 1011, 2022, 10634, 2100, 5204, 2008, 1011, 4013, 2078, 1011, 2022, 2444, 2006, 1996, 19116, 1997, 1037, 12779, 2021, 1011, 4013, 2078, 1011, 2360, 1011, 4013, 2078, 1011, 2052, 2197, 1011, 4013, 2078, 1011, 2051, 1998, 2061, 1011, 4013, 2078, 1011, 2079, 2711, 2487, 3280, 1997, 25765, 1999, 2681, 1011, 4013, 2078, 1011, 7631, 2000, 2128, 9331, 1996, 11203, 2008, 4245, 2031, 2022, 2061, 2860, 10381, 1996, 4329, 3535, 2012, 5290, 1011, 4013, 2078, 1011, 2022, 10358, 2008, 1037, 2689, 2442, 2022, 2191, 2711, 2487, 1011, 4013, 2078, 1011, 2113, 1011, 4013, 2078, 1011, 1998, 22889, 3126, 2058, 1996, 2773, 1999, 1011, 4013, 2078, 1011, 12773, 11292, 2008, 14187, 1011, 4013, 2078, 1011, 2000, 4654, 3775, 14536, 3686, 28354, 2021, 1011, 4013, 2078, 1011, 2022, 1037, 4030, 10634, 2158, 1998, 6771, 2031, 2272, 2000, 2107, 1037, 3413, 2008, 1037, 2521, 2583, 2158, 2084, 1011, 4013, 2078, 1011, 2071, 6684, 2031, 3066, 2007, 1996, 2757, 5843, 2682, 2302, 3426, 1037, 25966, 3993, 8293, 1997, 1996, 7279, 2039, 3742, 2063, 2917, 1011, 4013, 2078, 1011, 3035, 2711, 2487, 2022, 5223, 2005, 2022, 1997, 4496, 2361, 2487, 4182, 1998, 2295, 1037, 3962, 3238, 1998, 7015, 2450, 1011, 4013, 2078, 1011, 2087, 20610, 2895, 2507, 6686, 2000, 2711, 2487, 2179, 2006, 1996, 4126, 1997, 1996, 2197, 4245, 6854, 1996, 2332, 2295, 2019, 7481, 1998, 2092, 6808, 2098, 2158, 2022, 6135, 29439, 2000, 5009, 1037, 2406, 2083, 1037, 4795, 5325, 1011, 4013, 2078, 1011, 8424, 2022, 13135, 1011, 4013, 2078, 1011, 5450, 2022, 3082, 10634, 1998, 11004, 1998, 2295, 11328, 27746, 18886, 3560, 1011, 4013, 2078, 1011, 2022, 4030, 1997, 26683, 1998, 4895, 16416, 5149, 1999, 2895, 1998, 13708, 2022, 1996, 2062, 3697, 2138, 2000, 11113, 20872, 2232, 1996, 11809, 2457, 2436, 2052, 2031, 2022, 14395, 22611, 2000, 2116, 1997, 1011, 4013, 2078, 1011, 9111, 2040, 2031, 2498, 2021, 1011, 4013, 2078, 1011, 11550, 2000, 2444, 2588, 2664, 2045, 2022, 1037, 2236, 6896, 2005, 5290, 2035, 4635, 11455, 2298, 2000, 2070, 2689, 2000, 2489, 1011, 4013, 2078, 1011, 2013, 1996, 2757, 5843, 2029, 2191, 7620, 5263, 1996, 2231, 2022, 17482, 2096, 1996, 25964, 2022, 2046, 3917, 3085, 1998, 3058, 2487, 1997, 1996, 5853, 2022, 5247, 1999, 7551, 3300, 2121, 1037, 4496, 2361, 2487, 13448, 2022, 13260, 2000, 2202, 1996, 3715, 1997, 1996, 5446, 1998, 2312, 5414, 2022, 2191, 2000, 2231, 2005, 2029, 1011, 4013, 2078, 1011, 9530, 18886, 3726, 2000, 3477, 3037, 5570, 2070, 7312, 2022, 2191, 1999, 1996, 20700, 2021, 1996, 2332, 2022, 2214, 2704, 2711, 2487, 4982, 9981, 1997, 1011, 4013, 2078, 1011, 6217, 1998, 6855, 1011, 4013, 2078, 1011, 15322, 1996, 4496, 2361, 2487, 2202, 1996, 2112, 1997, 1996, 4496, 2361, 2487, 5701, 1999, 1011, 4013, 2078, 1011, 10073, 2013, 14246, 2063, 2487, 1998, 1996, 2162, 2947, 6686, 3288, 2006, 2019, 3623, 1997, 1996, 7170, 1997, 7016, 1996, 2236, 12893, 3623, 1998, 1011, 4013, 2078, 1011, 2468, 4072, 2000, 14386, 3366, 2070, 5549, 1997, 10095, 3070, 2029, 2089, 11443, 1996, 2711, 2487, 2090, 1996, 2878, 3842, 2612, 1997, 2191, 1996, 14539, 3477, 2035, 1998, 1996, 7015, 1998, 11646, 2498, 14246, 2063, 2487, 5630, 2006, 2655, 2362, 1996, 3862, 2030, 2152, 11760, 2021, 1011, 4013, 2078, 1011, 2022, 2011, 2053, 2965, 27764, 2000, 4171, 1011, 4013, 2078, 1011, 1998, 2069, 6905, 1011, 4013, 2078, 1011, 2704, 1011, 4013, 2078, 1011, 2059, 10663, 2006, 9530, 6767, 3489, 1996, 2878, 2110, 2236, 1997, 1996, 14246, 2063, 2487, 2029, 2031, 2196, 3113, 2144, 1996, 5853, 1997, 2711, 2487, 1996, 2163, 2236, 2053, 7185, 2487, 3599, 2113, 1996, 5787, 1997, 1996, 2373, 1997, 1996, 2110, 2236, 2043, 1011, 4013, 2078, 1011, 3113, 1999, 7015, 11646, 1998, 1996, 4112, 2040, 5050, 1996, 2691, 2389, 3723, 2035, 2433, 1996, 3320, 2012, 14246, 2063, 2487, 1998, 2295, 1996, 2332, 2052, 2031, 2562, 4237, 2122, 2197, 2040, 2022, 2655, 1996, 7563, 3802, 1056, 2030, 2030, 18979, 2140, 2487, 3776, 1011, 4013, 2078, 1011, 10214, 2000, 10632, 2013, 6904, 2278, 2487, 1997, 14246, 2063, 2487, 1996, 4175, 1997, 2711, 2487, 1996, 2402, 2365, 1997, 1037, 7015, 2155, 2040, 4133, 2004, 1037, 4112, 13520, 2008, 2498, 2460, 1997, 3016, 5643, 2102, 2323, 3298, 2041, 2216, 2040, 4133, 2011, 1996, 2097, 1997, 1996, 2111, 1998, 14246, 2063, 2487, 10750, 23166, 15628, 1996, 3789, 1997, 1037, 7015, 1037, 3387, 2030, 1037, 4112, 2035, 4175, 11455, 1996, 2283, 2171, 1997, 4496, 2361, 2487, 2005, 2216, 2040, 2215, 2000, 4654, 2389, 2102, 1996, 2373, 1997, 1996, 2111, 1998, 1997, 10488, 16033, 23185, 2005, 2216, 2040, 5441, 1996, 14293, 1997, 1996, 7015, 2272, 2046, 2224, 1998, 1996, 2087, 6034, 4496, 2361, 2487, 2022, 2655, 6213, 2378, 2013, 2019, 2214, 10664, 1997, 4496, 2361, 2487, 25287, 2073, 1011, 4013, 2078, 1011, 2224, 2000, 3113, 1996, 11240, 1997, 14246, 2063, 2487, 2467, 9461, 10882, 19250, 1998, 2411, 2668, 24907, 2022, 7568, 2000, 1996, 2197, 3014, 2011, 1996, 5981, 1998, 2440, 1997, 1996, 19451, 1997, 1996, 16021, 9890, 5897, 1998, 18186, 1997, 1996, 7015, 2823, 4125, 1998, 5690, 2091, 2711, 3183, 1011, 4013, 2078, 1011, 9266, 2213, 10488, 16033, 23185, 6865, 1011, 4013, 2078, 1011, 2000, 1996, 3707, 8473, 2011, 2029, 10437, 2022, 28324, 2058, 1996, 2395, 1996, 2332, 1999, 8598, 4009, 1996, 2390, 2379, 1998, 1011, 4013, 2078, 1011, 2022, 6814, 2008, 1011, 4013, 2078, 1011, 2022, 2175, 2000, 4652, 2035, 2689, 2011, 2486, 1997, 2849, 2045, 6279, 2239, 1996, 6926, 4372, 13153, 1011, 4013, 2078, 1011, 2004, 1037, 2120, 3457, 4929, 10338, 9648, 1997, 2417, 2630, 1998, 2317, 1998, 3094, 2011, 8917, 2487, 1037, 7015, 1997, 4496, 2361, 2487, 5448, 2040, 2031, 2448, 2185, 2012, 7185, 2487, 2000, 3710, 1999, 2724, 2487, 2006, 1037, 3189, 2008, 1996, 8854, 1997, 1996, 14246, 2063, 2487, 2031, 2022, 2391, 2588, 14246, 2063, 2487, 1996, 11240, 4125, 1999, 1037, 21517, 5481, 2588, 1011, 4013, 2078, 1011, 6865, 1996, 3457, 1998, 7078, 7697, 2091, 1996, 2214, 3317, 2000, 1011, 4013, 2078, 1011, 3192, 2295, 1011, 4013, 2078, 1011, 2079, 2025, 2424, 1037, 2309, 7267, 1999, 1011, 4013, 2078, 1011, 2023, 2022, 1037, 10073, 2360, 14246, 2063, 2487, 2043, 1011, 4013, 2078, 1011, 2963, 1997, 1011, 4013, 2078, 1011, 15785, 1011, 4013, 2078, 1011, 2022, 1037, 4329, 2022, 1996, 3437, 2375, 2487, 1996, 11240, 2031, 2424, 2041, 1011, 4013, 2078, 1011, 2373, 1996, 3869, 10169, 1997, 1996, 3006, 2467, 1037, 14099, 1998, 21598, 2465, 2022, 16460, 4654, 17847, 1998, 2022, 2469, 2000, 2022, 16097, 1999, 2035, 1996, 10467, 16130, 2039, 2011, 6213, 2378, 2045, 2022, 1037, 2307, 11228, 12972, 1997, 9347, 1999, 14246, 2063, 2487, 1998, 2023, 2362, 2007, 1996, 27222, 14436, 2008, 5290, 2052, 2022, 4638, 2011, 4808, 24890, 1996, 2111, 2006, 1037, 3189, 2008, 1996, 3457, 2031, 2265, 12024, 2005, 1996, 2332, 1996, 2878, 22508, 2272, 10364, 2041, 1997, 14246, 2063, 2487, 2000, 14246, 2063, 2487, 1998, 2044, 15686, 1996, 2166, 1997, 1996, 3035, 3288, 1996, 2155, 2067, 2007, 1011, 4013, 2078, 1011, 2000, 14246, 2063, 2487, 1998, 2562, 1011, 4013, 2078, 1011, 2471, 2004, 7267, 2096, 1996, 3320, 2029, 3582, 1011, 4013, 2078, 1011, 2000, 14246, 2063, 2487, 5981, 2006, 2375, 2487, 1996, 7015, 2022, 3193, 2004, 1996, 2919, 4099, 1997, 1996, 3842, 1998, 2035, 2058, 1996, 2406, 2045, 2022, 4803, 1997, 1996, 14539, 2132, 2011, 4496, 2361, 2487, 2013, 1996, 2237, 2040, 12803, 1011, 4013, 2078, 1011, 3317, 1998, 2411, 15126, 1011, 4013, 2078, 1011, 2711, 2116, 10574, 2000, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 1998, 1996, 14436, 2008, 2122, 2052, 15908, 1998, 2709, 2000, 3288, 2067, 1996, 2214, 2291, 14678, 3623, 1996, 8111, 1997, 1996, 2111, 1996, 3320, 2085, 2113, 2004, 1996, 13794, 3320, 11740, 2185, 2035, 2516, 1998, 14293, 1998, 2053, 7185, 2487, 2022, 6516, 15628, 2000, 4562, 2151, 17576, 2000, 1011, 4013, 2078, 1011, 2171, 2021, 6926, 2096, 2012, 1996, 2168, 2051, 1996, 11646, 2022, 2000, 17738, 17457, 2035, 1996, 3200, 1997, 1996, 2277, 1998, 2000, 8415, 2008, 1011, 4013, 2078, 1011, 2436, 1998, 3222, 2022, 18547, 2013, 1996, 2097, 1997, 1996, 2111, 2894, 1998, 2008, 1011, 4013, 2078, 1011, 12533, 2053, 22645, 3828, 2000, 1996, 2110, 1996, 3776, 2947, 10750, 2039, 2022, 6814, 2000, 2022, 2438, 2000, 4425, 2035, 2110, 10961, 2302, 25964, 2021, 2004, 1011, 4013, 2078, 1011, 2071, 2025, 2012, 2320, 2022, 2735, 2046, 2769, 20877, 14643, 10253, 3602, 2030, 2711, 2487, 2022, 3277, 2021, 2004, 9226, 2022, 18782, 2122, 2022, 2025, 4276, 3053, 1011, 4013, 2078, 1011, 11268, 7971, 3643, 1998, 1996, 2236, 12893, 2022, 2947, 2172, 3623, 1996, 2060, 11292, 1996, 2307, 2303, 1997, 1996, 11646, 12580, 10214, 1998, 1011, 4013, 2078, 1011, 2022, 3568, 3298, 2041, 1997, 1011, 4013, 2078, 1011, 3841, 12879, 6610, 1998, 2468, 4874, 1997, 2307, 10928, 2000, 1996, 4496, 2361, 2487, 2035, 1996, 2214, 6192, 1998, 2060, 7835, 2090, 1996, 2874, 2022, 6033, 1998, 14246, 2063, 2487, 2022, 11443, 2046, 2533, 2169, 1997, 2029, 2022, 2000, 11322, 4112, 1999, 3005, 3320, 2035, 2373, 2022, 2000, 2022, 17447, 3272, 2008, 1996, 2332, 9279, 1037, 2157, 1997, 22102, 1045, 1041, 1997, 10214, 1011, 4013, 2078, 1011, 2624, 7542, 2000, 2151, 5468, 1011, 4013, 2078, 1011, 8415, 2006, 1996, 16215, 1997, 3058, 2487, 2000, 11949, 2023, 2047, 4552, 1996, 3072, 1996, 13794, 3320, 2085, 21969, 1011, 4013, 2078, 1011, 1998, 1037, 4840, 3320, 2655, 1996, 4884, 2202, 1011, 4013, 2078, 1011, 2173, 2005, 1037, 2051, 2518, 2175, 2006, 2062, 21614, 29245, 2022, 2174, 6171, 2061, 2860, 1996, 2332, 2022, 4876, 3422, 2004, 2019, 4099, 1998, 2216, 1997, 1996, 7015, 2040, 2031, 12495, 22780, 4088, 2000, 2433, 2390, 4681, 2011, 1996, 4496, 2361, 2487, 2006, 1996, 8880, 2005, 1011, 4013, 2078, 1011, 5343, 2023, 4372, 24449, 1996, 2111, 2040, 5987, 2008, 1011, 4013, 2078, 1011, 4397, 2663, 7044, 2052, 2022, 16857, 1996, 2030, 18979, 2140, 2487, 2051, 1996, 2332, 6912, 1011, 4013, 2078, 1011, 2157, 1997, 22102, 1996, 11240, 4125, 1999, 8111, 1998, 2295, 1011, 4013, 2078, 1011, 2059, 2079, 2053, 2062, 2084, 15686, 2006, 1996, 5083, 1997, 1996, 12495, 18980, 2390, 2006, 1996, 16215, 1997, 3058, 2487, 1037, 2062, 6659, 4125, 2202, 2173, 1996, 4496, 2361, 2487, 2022, 12803, 1996, 3457, 14574, 1996, 4895, 6072, 2923, 2075, 2332, 1998, 1011, 4013, 2078, 1011, 2155, 2139, 20688, 1998, 17727, 6935, 2239, 1999, 1996, 3578, 1997, 1996, 3379, 1999, 7404, 26693, 1996, 7015, 1999, 1996, 3827, 2323, 15908, 2007, 1996, 12495, 18980, 1011, 4013, 2078, 1011, 2022, 9288, 2011, 17264, 2096, 2007, 1037, 6819, 3995, 3126, 4562, 1997, 1996, 8277, 1996, 12495, 18980, 2390, 2022, 16360, 28426, 2063, 1998, 3786, 1996, 12078, 2272, 2000, 2019, 2203, 1998, 14246, 2063, 2487, 2468, 1037, 3072, 1999, 2029, 1996, 2120, 4680, 2029, 3582, 8917, 2487, 2022, 4259, 1996, 2062, 8777, 2266, 1997, 2023, 2022, 2655, 2711, 2487, 2013, 1996, 8840, 2278, 2487, 1996, 18056, 1997, 1996, 14246, 2063, 2487, 2013, 1996, 10971, 1997, 2029, 2116, 1997, 1011, 4013, 2078, 1011, 2272, 1011, 4013, 2078, 1011, 2022, 2583, 2158, 6288, 1998, 9667, 2440, 1997, 5679, 2005, 17995, 4556, 2051, 2021, 4299, 2000, 2644, 2460, 1997, 1996, 2933, 1997, 1996, 6213, 2378, 1997, 3183, 1996, 2708, 2022, 2711, 2487, 1037, 5160, 2013, 8917, 2487, 6039, 2007, 5470, 12070, 2389, 9366, 1997, 1996, 2157, 1997, 2158, 1011, 4013, 2078, 1011, 2007, 1037, 2283, 1997, 2060, 6355, 4496, 2361, 2487, 2655, 1996, 3137, 1997, 3183, 2711, 2487, 1998, 2711, 2487, 2022, 2087, 3602, 2275, 2000, 2147, 2000, 6033, 2035, 2008, 15115, 2007, 1011, 4013, 2078, 1011, 2933, 1997, 2236, 9945, 1996, 26458, 7174, 10196, 1037, 3728, 1999, 15338, 3698, 2005, 2022, 4974, 2075, 2022, 2275, 1999, 2035, 1996, 2708, 3006, 2173, 1998, 7185, 2487, 2022, 2404, 2000, 2331, 2006, 1996, 3715, 1997, 9530, 13102, 7442, 2114, 1996, 3842, 2711, 2487, 2022, 15389, 2220, 1999, 1998, 1011, 4013, 2078, 1011, 2022, 2438, 2000, 2031, 2151, 4066, 1997, 4182, 15950, 2000, 2022, 2228, 4795, 1998, 2404, 2000, 2331, 1996, 5853, 1997, 7404, 5469, 2012, 1996, 2668, 14740, 2566, 22327, 11657, 2011, 1996, 3137, 2599, 1037, 2402, 2611, 2171, 2711, 2487, 3058, 2487, 2000, 25683, 2711, 2487, 3183, 1011, 4013, 2078, 1011, 6814, 2000, 2022, 1996, 2708, 3426, 1997, 1996, 18186, 2008, 2022, 2202, 2173, 2021, 1011, 4013, 2078, 1011, 2331, 2069, 5587, 2000, 1996, 14436, 1997, 4668, 1037, 2837, 1997, 2270, 3808, 2022, 16823, 2011, 1996, 4680, 1998, 26911, 2000, 11740, 2185, 2296, 2108, 2040, 2593, 4025, 15316, 2000, 9945, 2030, 2040, 2089, 22490, 2151, 4366, 2000, 4635, 1996, 3035, 2022, 2404, 2000, 2331, 3058, 2487, 2044, 1011, 4013, 2078, 1011, 3129, 1998, 1996, 2711, 2487, 2040, 2031, 4088, 2000, 3046, 2000, 7872, 1996, 10401, 1997, 14574, 2574, 2991, 2104, 1996, 7939, 24101, 1997, 1996, 2062, 6355, 2000, 2022, 26960, 1997, 9530, 13102, 7442, 2114, 1996, 2110, 2022, 6880, 10611, 1998, 2053, 7185, 2487, 2022, 2166, 2022, 3647, 2711, 2487, 2022, 7939, 23709, 3401, 2011, 2711, 2487, 1998, 2566, 4509, 1998, 2005, 3058, 2487, 1996, 5853, 1997, 7404, 2197, 1996, 12495, 18980, 2011, 2433, 2019, 2390, 1998, 5083, 2006, 14246, 2063, 2487, 6509, 2011, 1996, 2486, 1997, 14246, 2063, 2487, 2069, 2191, 3043, 4788, 2045, 2022, 2107, 1037, 14436, 1997, 1996, 2214, 20489, 2272, 2067, 2008, 1996, 14539, 2022, 3201, 2000, 2954, 2000, 1996, 2331, 2114, 1996, 2709, 1997, 1996, 7015, 1996, 2390, 2073, 4712, 2224, 2000, 2175, 2011, 4635, 2612, 1997, 7857, 2022, 2061, 5580, 1997, 1996, 2689, 2008, 1011, 4013, 2078, 1011, 2022, 2440, 1997, 4840, 4382, 1998, 16360, 28426, 2063, 1996, 2390, 1997, 4496, 2361, 2487, 1998, 12495, 18980, 2035, 2247, 1996, 8880, 1996, 2103, 1997, 2711, 2487, 2029, 2031, 3046, 2000, 9507, 1996, 2689, 2022, 2202, 1998, 25966, 7699, 2224, 2011, 8902, 10994, 1040, 12810, 10054, 1037, 2266, 1997, 8917, 2487, 1996, 26458, 7174, 10196, 2022, 2205, 4030, 2005, 1011, 4013, 2078, 1011, 1998, 1011, 4013, 2078, 1011, 2031, 1996, 2111, 9587, 2860, 2091, 2007, 14722, 2915, 13520, 2008, 1997, 2023, 2307, 2103, 2498, 2323, 2022, 2681, 2021, 1037, 6104, 16021, 26775, 20755, 2711, 2487, 9507, 8917, 2487, 2022, 2053, 2062, 1999, 2474, 2310, 4859, 1041, 1037, 2212, 1997, 14246, 2063, 2487, 2073, 1996, 14539, 2022, 2172, 22476, 2000, 1011, 4013, 2078, 1011, 11646, 1998, 7015, 1011, 4013, 2078, 1011, 4125, 1998, 5114, 2107, 3112, 2008, 1011, 4013, 2078, 1011, 3959, 2005, 1037, 2210, 2096, 1997, 5343, 1998, 9239, 1996, 2210, 12481, 2365, 1997, 2711, 2487, 2021, 1011, 4013, 2078, 1011, 2022, 4154, 1998, 2404, 2091, 2011, 2543, 1998, 4690, 1998, 2012, 14246, 2063, 2487, 2019, 14269, 2193, 1997, 7781, 2202, 2173, 15897, 2011, 19549, 1011, 4013, 2078, 1011, 2022, 29072, 2008, 2053, 2625, 2084, 2711, 2022, 26458, 7174, 10196, 1999, 3058, 2487, 2090, 1998, 4661, 2216, 2040, 3280, 2011, 2060, 2812, 2673, 2022, 2689, 4676, 2022, 2000, 2022, 2079, 2185, 2007, 1996, 2277, 2022, 2485, 1996, 2030, 18979, 2140, 2487, 2612, 1997, 3058, 2487, 16823, 2005, 2717, 2331, 2022, 2019, 10721, 3637, 2022, 16021, 26775, 20755, 2006, 1996, 2082, 1998, 3114, 5050, 2011, 1037, 4556, 2135, 5102, 2450, 2022, 4372, 2705, 20793, 1999, 1996, 5040, 1997, 10289, 1040, 1011, 4013, 2078, 1011, 2012, 1996, 2168, 2051, 1037, 2047, 3690, 2022, 1999, 15338, 1996, 1050, 2094, 1997, 3058, 2487, 2031, 2047, 2171, 1998, 1996, 26066, 5468, 1997, 3091, 3635, 1998, 3977, 2029, 2022, 2918, 2006, 1996, 10817, 1997, 1996, 8840, 2278, 2487, 2022, 2933, 2035, 2023, 2051, 2711, 2487, 2428, 4025, 2000, 2031, 2228, 1011, 4013, 2078, 1011, 1996, 27398, 1997, 1996, 2529, 2679, 2021, 2012, 2197, 1996, 2060, 2266, 1997, 1996, 4680, 2202, 8424, 2000, 7939, 23709, 3401, 1011, 4013, 2078, 1011, 1998, 1011, 4013, 2078, 1011, 2007, 7185, 2487, 2062, 2022, 6545, 1998, 4604, 2000, 1996, 26458, 7174, 10196, 1996, 2668, 15222, 12096, 2100, 9016, 2022, 2058, 8917, 2487, 2022, 16857, 1998, 2111, 7200, 2153, 1996, 14176, 1996, 2708, 3237, 2373, 2022, 2173, 1999, 1996, 2192, 1997, 1037, 14176, 8676, 1997, 2062, 8777, 2158, 1998, 1037, 2051, 1997, 2172, 14165, 2275, 1999, 2525, 1999, 1996, 2047, 6819, 3995, 3126, 4562, 1997, 1996, 2844, 7603, 1997, 1996, 2406, 1996, 2390, 2663, 2307, 3377, 2025, 2069, 16360, 2884, 1996, 4496, 2361, 2487, 1998, 1996, 12495, 18980, 2021, 15908, 14246, 2063, 2487, 2000, 8917, 2487, 1037, 4496, 2361, 2487, 2961, 2040, 2022, 2655, 2006, 2000, 4047, 1996, 14176, 2013, 2022, 2153, 2058, 10376, 2063, 2011, 1996, 11240, 2468, 1996, 2599, 4382, 1999, 14246, 2063, 2487, 2083, 1011, 4013, 2078, 1011, 4496, 2361, 2487, 3377, 1011, 4013, 2078, 1011, 16152, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 1998, 2486, 1996, 3750, 2000, 2292, 1011, 4013, 2078, 1011, 2468, 3072, 2104, 4496, 2361, 2487, 3860, 2036, 2000, 12897, 14246, 2063, 2487, 2000, 14246, 2063, 2487, 2011, 1996, 5036, 1997, 8917, 2487, 2059, 2191, 1037, 6934, 2006, 14246, 2063, 2487, 3246, 2000, 2886, 14246, 2063, 2487, 2013, 2008, 2217, 2021, 1011, 4013, 2078, 1011, 2022, 17910, 2011, 2711, 2487, 2040, 6033, 1011, 4013, 2078, 1011, 4170, 1999, 1996, 2645, 1997, 1996, 8840, 2278, 2487, 1998, 2909, 2711, 2487, 2040, 2907, 2041, 11712, 2487, 2114, 1011, 4013, 2078, 1011, 1011, 4013, 2078, 1011, 9241, 2188, 2000, 14246, 2063, 2487, 2006, 2424, 2008, 1996, 14176, 2031, 4088, 1037, 4840, 4496, 2361, 2487, 2162, 15126, 14246, 2063, 2487, 1998, 2486, 1011, 4013, 2078, 1011, 2000, 2507, 2039, 1011, 4013, 2078, 1011, 8813, 1998, 2468, 1037, 3072, 2006, 1011, 4013, 2078, 1011, 2944, 1998, 4287, 1996, 4831, 2125, 2046, 16187, 2035, 1996, 4496, 2361, 2487, 2373, 2031, 8917, 2487, 2114, 1011, 4013, 2078, 1011, 1998, 14246, 2063, 2487, 2031, 2022, 8980, 15897, 2011, 4496, 2361, 2487, 4681, 2061, 2008, 2711, 2487, 2006, 1996, 2598, 2008, 1037, 3842, 2012, 2162, 2342, 1037, 2625, 13988, 12618, 2271, 2231, 2084, 1037, 14176, 9530, 18886, 3726, 2000, 2131, 1011, 4013, 2078, 1011, 5454, 2030, 18979, 2140, 2487, 11801, 2007, 7185, 2487, 14092, 1999, 1996, 19972, 1037, 2307, 2607, 1997, 3377, 3582, 1999, 14246, 2063, 2487, 2073, 2711, 2487, 3094, 1999, 2711, 1998, 1999, 14246, 2063, 2487, 2104, 2711, 2487, 1998, 14246, 2063, 2487, 2022, 2486, 2000, 2191, 3521, 1998, 14246, 2063, 2487, 2022, 1996, 2069, 2406, 2008, 2145, 9507, 1011, 4013, 2078, 1011, 6229, 1037, 2236, 3521, 2022, 2191, 2012, 14246, 2063, 2487, 1999, 2021, 1011, 4013, 2078, 1011, 2069, 2197, 2005, 3058, 2487, 2005, 1996, 4496, 2361, 2487, 8246, 2000, 4685, 1996, 4650, 1998, 4088, 1996, 2162, 21358, 21898, 1999, 1996, 2812, 2051, 2711, 2487, 2031, 9239, 4676, 1998, 2344, 1998, 2061, 4498, 3040, 14246, 2063, 2487, 2008, 1999, 1011, 4013, 2078, 1011, 2022, 2583, 2000, 2433, 1996, 3072, 2046, 2019, 3400, 1998, 7461, 2000, 2022, 2178, 2711, 2487, 1011, 4013, 2078, 1011, 3426, 1996, 4831, 2000, 2360, 3742, 2012, 1011, 4013, 2078, 1011, 12773, 2295, 1011, 4013, 2078, 1011, 2404, 1996, 4410, 2006, 1011, 4013, 2078, 1011, 2219, 2132, 1037, 16557, 4017, 2007, 1996, 4831, 19222, 12259, 1996, 11646, 2021, 11477, 1996, 2407, 1997, 1996, 5801, 1998, 2404, 1996, 3387, 1998, 5011, 1999, 1996, 3477, 1997, 1996, 2110, 1996, 3400, 8917, 2487, 2000, 2023, 2047, 4496, 2361, 2487, 3400, 3426, 1037, 4840, 2162, 2007, 2035, 8840, 2278, 2487, 8917, 2487, 2174, 2022, 4154, 2012, 14246, 2063, 2487, 1998, 2711, 2487, 1996, 4496, 2361, 2487, 2022, 4498, 10188, 2012, 14246, 2063, 2487, 1998, 1996, 4496, 2361, 2487, 2954, 7185, 2487, 6659, 2021, 2471, 4009, 2645, 2012, 2711, 2487, 1998, 2711, 2487, 3521, 2022, 2059, 2191, 2007, 2035, 7185, 2487, 2012, 8917, 2487, 1999, 1996, 2744, 2811, 17003, 2135, 2524, 2588, 14246, 2063, 2487, 5679, 1997, 18445, 14246, 2063, 2487, 2022, 20432, 2011, 1996, 3750, 2021, 2022, 12532, 5897, 5339, 2011, 1996, 6215, 1997, 1996, 4496, 2361, 2487, 1998, 4496, 2361, 2487, 4170, 2011, 2711, 2487, 2012, 19817, 10354, 2389, 6843, 14246, 2063, 2487, 2022, 2059, 1999, 4707, 2007, 14246, 2063, 2487, 2021, 8917, 2487, 26648, 2135, 2131, 1996, 2548, 2155, 2046, 1011, 4013, 2078, 1011, 2192, 15126, 1011, 4013, 2078, 1011, 14246, 2063, 2487, 2191, 1011, 4013, 2078, 1011, 2567, 2711, 2487, 1011, 4013, 2078, 1011, 2332, 2021, 1996, 4496, 2361, 2487, 2052, 2025, 12040, 1998, 2655, 1999, 1996, 4496, 2361, 2487, 2000, 1011, 4013, 2078, 1011, 4681, 1996, 22682, 2162, 2765, 1999, 1037, 2186, 1997, 3377, 2006, 1996, 2112, 1997, 1996, 4496, 2361, 2487, 2104, 14246, 2063, 2487, 2096, 14246, 2063, 2487, 4088, 2178, 2162, 2022, 2153, 2061, 10188, 2008, 1996, 3750, 4241, 12096, 2025, 10214, 2000, 2507, 1011, 4013, 2078, 1011, 2684, 1999, 3510, 2000, 8917, 2487, 2174, 1999, 1996, 9187, 1997, 14246, 2063, 2487, 6011, 2019, 18077, 3458, 8917, 2487, 2022, 2373, 1011, 4013, 2078, 1011, 3362, 14246, 2063, 2487, 2007, 1011, 4013, 2078, 1011, 2882, 2390, 2021, 1996, 2103, 2022, 6402, 2091, 3202, 2044, 1011, 4013, 2078, 1011, 5508, 1998, 1011, 4013, 2078, 1011, 2031, 2053, 7713, 2030, 2812, 1997, 2490, 1011, 4013, 2078, 1011, 2022, 2486, 2000, 7822, 2083, 1037, 19725, 3058, 2487, 2302, 9347, 1998, 18820, 4757, 2011, 1996, 2522, 11488, 10603, 2040, 6865, 2006, 1996, 4373, 1998, 3013, 2125, 1996, 2358, 29181, 17420, 2061, 2008, 1011, 4013, 2078, 1011, 2878, 21459, 2390, 2031, 2468, 1037, 8210, 13736, 3338, 2358, 29181, 18483, 19614, 2011, 1996, 2051, 1996, 12084, 3362, 1996, 4496, 2361, 2487, 8880, 1011, 4013, 2078, 1011, 1011, 4013, 2078, 1011, 2031, 9241, 2067, 2000, 14246, 2063, 2487, 2004, 2574, 2004, 1011, 4013, 2078, 1011, 2424, 1011, 4013, 2078, 1011, 2553, 20625, 2000, 13621, 1011, 4013, 2078, 1011, 5012, 2000, 2035, 8840, 2278, 2487, 2005, 2296, 2406, 4125, 2114, 1011, 4013, 2078, 1011, 2006, 1011, 4013, 2078, 1011, 2030, 18979, 2140, 2487, 7071, 1998, 3058, 2487, 2022, 5247, 1999, 1037, 2186, 1997, 7143, 2645, 1999, 14246, 2063, 2487, 2090, 1011, 4013, 2078, 1011, 1998, 1996, 8917, 2487, 4204, 2711, 2487, 1998, 2711, 2487, 2022, 21888, 2021, 3058, 2487, 2645, 1997, 26947, 2361, 27586, 2022, 1037, 6659, 4154, 1999, 3058, 2487, 7185, 2487, 2390, 2216, 1997, 14246, 2063, 2487, 14246, 2063, 2487, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 4607, 14246, 2063, 2487, 2012, 2320, 1998, 2295, 8917, 2487, 9507, 3233, 9191, 2135, 1998, 8301, 10270, 18083, 2100, 1998, 5114, 2309, 2645, 2114, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 2071, 2025, 3233, 2114, 2035, 8840, 2278, 2487, 1999, 3058, 2487, 1996, 9698, 4607, 14246, 2063, 2487, 1998, 1011, 4013, 2078, 1011, 2022, 2486, 2000, 19935, 24695, 2022, 4604, 2104, 1037, 2844, 3457, 2000, 1996, 2210, 8840, 2278, 2487, 8842, 1997, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 2031, 12475, 14246, 2063, 2487, 1997, 2158, 2011, 1011, 4013, 2078, 1011, 5377, 2655, 2005, 5268, 2040, 2022, 4009, 2011, 26329, 2013, 1996, 2878, 2406, 6229, 2045, 2022, 2025, 2438, 2000, 2079, 1996, 2147, 1999, 1996, 2492, 1998, 3097, 7267, 2031, 2000, 2022, 12666, 2021, 1011, 4013, 2078, 1011, 2031, 9530, 7512, 2006, 1011, 4013, 2078, 1011, 7185, 2487, 2307, 5770, 1999, 1996, 2307, 3642, 1997, 2375, 2655, 1996, 3642, 18996, 4747, 2006, 2029, 2031, 2412, 2144, 3613, 1999, 2486, 14246, 2063, 2487, 2104, 8917, 2487, 1996, 2214, 2375, 1998, 7661, 8137, 1999, 2367, 2874, 2031, 2022, 11740, 2185, 2061, 2008, 1996, 2492, 2022, 3154, 1998, 1996, 2291, 1997, 2231, 2029, 8917, 2487, 14386, 3366, 2031, 3961, 8134, 15704, 2013, 2008, 2051, 2000, 2023, 2673, 2022, 2191, 2000, 12530, 2588, 1996, 2430, 2231, 1996, 2704, 1997, 4676, 1997, 3425, 1997, 2610, 1997, 2495, 4385, 2031, 1996, 7816, 1997, 2035, 4592, 6771, 1998, 16823, 2035, 2040, 2147, 2104, 1011, 4013, 2078, 1011, 2061, 2008, 6343, 4553, 2129, 2000, 2552, 2894, 1998, 2004, 1996, 2231, 2031, 2022, 1999, 2755, 2412, 2144, 7790, 2006, 1996, 2097, 1997, 1996, 2111, 1997, 14246, 2063, 2487, 1996, 2878, 2406, 2022, 24942, 1999, 1011, 4013, 2078, 1011, 2192, 1996, 2390, 2004, 1999, 2471, 2035, 3097, 3842, 2022, 5333, 2011, 26329, 2008, 2022, 2011, 4009, 2843, 2426, 1996, 2402, 2158, 20090, 2000, 3710, 1998, 2040, 2064, 2069, 4019, 2011, 3477, 1037, 7681, 2000, 3710, 1999, 1011, 4013, 2078, 1011, 26261, 4215, 1998, 2023, 2022, 3227, 1996, 2030, 18979, 2140, 2487, 4874, 1997, 1996, 7494, 1997, 1037, 2155, 2035, 16708, 4366, 2031, 2022, 2079, 2185, 2007, 1998, 2007, 1011, 4013, 2078, 1011, 1996, 2157, 1997, 26927, 5302, 6914, 4183, 5397, 1998, 5262, 1011, 4013, 2078, 1011, 2022, 2025, 2825, 2005, 1037, 3231, 8844, 2000, 4468, 2681, 1011, 4013, 2078, 1011, 3200, 2000, 2022, 3745, 2426, 1011, 4013, 2078, 1011, 2155, 2295, 1011, 4013, 2078, 1011, 2064, 2191, 2070, 2235, 4489, 1999, 1996, 3815, 2169, 4374, 1998, 2947, 3776, 2022, 14678, 20229, 11443, 1998, 2070, 4664, 2468, 2200, 2235, 5262, 4496, 2361, 2487, 14539, 2022, 2174, 2087, 9461, 2000, 2219, 2455, 1998, 2022, 2788, 2200, 10424, 16377, 2140, 17358, 1998, 3828, 1998, 1996, 2406, 2031, 2175, 2006, 3623, 1999, 14165, 1998, 7216, 1011, 4013, 2078, 1011, 2022, 2995, 2008, 2763, 2013, 1996, 2146, 10427, 1997, 19819, 2151, 7177, 1011, 4013, 2078, 1011, 2089, 10295, 1996, 4496, 2361, 2487, 7500, 1998, 14539, 2854, 2729, 2210, 2005, 4653, 2030, 2054, 1011, 4013, 2078, 1011, 2323, 2655, 7216, 1998, 2444, 5931, 2524, 2147, 2444, 2130, 2096, 2092, 2125, 1998, 2007, 2312, 7570, 4232, 1997, 7177, 2021, 1011, 4013, 2078, 1011, 4650, 2031, 2022, 6919, 2135, 2689, 2005, 1996, 2092, 2412, 2144, 1996, 4329, 2035, 2023, 2031, 3613, 2104, 1996, 3365, 2689, 2008, 2031, 2202, 2173, 1999, 1996, 2433, 1997, 2231, 10381, 14246, 2063, 2487, 2144, 1996, 4329, 1996, 6418, 1996, 9698, 2681, 1996, 2111, 1997, 14246, 2063, 2487, 2489, 2000, 5454, 1011, 4013, 2078, 1011, 2231, 1998, 1011, 4013, 2078, 1011, 5138, 1996, 2214, 2548, 2155, 2040, 2022, 2006, 1011, 4013, 2078, 1011, 3675, 26751, 1037, 9131, 1996, 2365, 1997, 2711, 2487, 2031, 2566, 4509, 1999, 1996, 2192, 1997, 1011, 4013, 2078, 1011, 7173, 2121, 1998, 2947, 1996, 2332, 2022, 2279, 2567, 2711, 2487, 9510, 2000, 1996, 6106, 3288, 2067, 1037, 2312, 12495, 18980, 3582, 2518, 2022, 2025, 7392, 2091, 2043, 8917, 2487, 1999, 1996, 3500, 1997, 4019, 2013, 8917, 2487, 6160, 1011, 4013, 2078, 1011, 2007, 12208, 1998, 14246, 2063, 2487, 2022, 2486, 2000, 10574, 2000, 14246, 2063, 2487, 2174, 1996, 9698, 3202, 4125, 1999, 2849, 1998, 1996, 10123, 1997, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 10188, 8917, 2487, 4498, 2012, 14246, 2063, 2487, 2006, 1996, 16215, 1997, 2238, 1011, 4013, 2078, 1011, 2022, 4604, 2000, 8917, 2487, 14246, 2063, 2487, 1999, 1996, 8840, 2278, 2487, 2043, 3401, 1011, 4013, 2078, 1011, 2071, 2025, 2153, 2709, 2000, 4390, 1996, 3521, 1997, 8840, 2278, 2487, 2045, 1011, 4013, 2078, 1011, 3280, 1999, 2711, 2487, 2022, 9239, 1998, 1037, 6111, 2022, 14386, 3366, 2011, 2029, 1037, 3132, 12078, 2022, 5323, 1037, 2332, 2012, 1996, 2132, 1998, 7185, 2487, 4574, 7185, 2487, 1997, 8152, 1996, 2060, 1997, 4112, 2021, 2007, 1037, 2200, 4867, 6329, 1011, 4013, 2078, 1011, 2079, 2025, 2174, 2147, 26445, 4757, 6229, 2044, 14246, 2063, 2487, 2022, 2331, 1999, 1011, 4013, 2078, 1011, 2567, 2711, 2487, 3046, 2000, 2991, 2067, 2006, 1996, 2214, 2291, 1011, 4013, 2078, 1011, 4638, 1996, 4071, 1997, 1996, 2811, 1998, 15115, 2007, 1996, 4071, 1997, 2602, 1996, 9509, 2022, 1037, 4840, 4329, 1999, 3058, 2487, 11361, 2007, 2210, 2668, 14740, 2021, 2029, 2486, 2711, 2487, 2000, 2175, 2046, 8340, 2007, 1011, 4013, 2078, 1011, 2882, 19339, 2711, 2487, 3005, 2269, 1996, 3804, 1997, 14246, 2063, 2487, 2031, 2022, 25683, 1999, 5853, 1997, 2711, 2487, 1996, 4574, 1997, 4112, 3749, 1996, 4410, 2000, 2711, 2487, 1997, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 2022, 18855, 2013, 1996, 11315, 1011, 4013, 2078, 1011, 2269, 2031, 2022, 7185, 2487, 1997, 8917, 2487, 1999, 1996, 4329, 1998, 2043, 2516, 2022, 11113, 20872, 2232, 2031, 2655, 1011, 4013, 2078, 1011, 2711, 2487, 9945, 2023, 2031, 2025, 3828, 1011, 4013, 2078, 1011, 2132, 2104, 1996, 5853, 1997, 7404, 1998, 1011, 4013, 2078, 1011, 2365, 2031, 2022, 27885, 3669, 3351, 2000, 10574, 1998, 2599, 1037, 17677, 2166, 2012, 7185, 2487, 2051, 5114, 1011, 4013, 2078, 1011, 24585, 2011, 6570, 8785, 14545, 4588, 2012, 1037, 2082, 1999, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 2031, 8980, 1011, 4013, 2078, 1011, 2155, 3776, 2012, 1996, 6418, 1998, 2004, 1996, 2132, 1997, 8917, 2487, 2022, 2200, 2759, 1011, 4013, 2078, 1011, 2022, 11322, 2332, 1997, 1996, 4496, 2361, 2487, 2025, 1997, 14246, 2063, 2487, 2007, 1037, 4574, 1997, 8152, 23388, 2005, 2166, 2069, 1998, 2178, 1997, 4112, 11322, 2011, 14303, 3005, 8263, 2022, 2769, 2487, 2030, 2809, 9044, 3058, 2487, 1011, 4013, 2078, 1011, 2079, 1011, 4013, 2078, 1011, 27917, 2000, 5114, 1996, 2204, 2097, 1997, 1996, 2111, 2444, 1037, 3722, 5379, 2155, 2166, 1998, 3046, 2000, 7857, 1996, 2744, 1997, 1996, 6926, 2332, 1998, 1999, 3058, 2487, 1997, 1011, 4013, 2078, 1011, 5853, 1011, 4013, 2078, 1011, 2022, 3144, 1996, 2406, 2022, 18241, 1998, 1037, 2307, 5701, 2022, 7392, 1999, 14246, 2063, 2487, 1998, 18094, 1037, 2146, 1998, 7143, 2162, 2007, 1996, 3748, 4496, 2361, 2487, 5917, 1037, 5701, 2022, 2036, 5323, 1999, 14246, 2063, 2487, 1999, 1996, 8840, 2278, 2487, 1998, 3535, 2022, 4287, 2041, 2000, 19079, 2947, 2005, 1996, 3279, 1997, 5336, 6664, 2029, 14246, 2063, 2487, 2031, 15770, 1999, 2162, 2007, 14246, 2063, 2487, 27648, 2174, 4088, 2000, 13368, 2006, 1996, 7185, 2487, 2192, 2013, 2216, 2040, 3342, 2069, 1996, 3112, 1997, 2711, 2487, 1998, 2025, 1996, 14624, 1011, 4013, 2078, 1011, 2031, 3426, 1998, 2006, 1996, 2060, 2013, 1996, 2147, 2465, 2040, 13520, 2008, 1996, 22846, 2030, 14279, 5051, 27469, 2031, 5114, 2673, 2011, 1996, 4329, 1997, 3058, 2487, 2021, 1011, 4013, 2078, 1011, 1011, 4013, 2078, 1011, 2498, 2711, 2487, 2079, 1011, 4013, 2078, 1011, 2204, 2000, 24665, 10450, 12031, 1998, 2572, 8557, 1996, 2111, 2011, 4604, 2005, 1996, 3961, 1997, 8917, 2487, 1998, 2507, 1011, 4013, 2078, 1011, 1037, 12047, 6715, 1998, 21459, 6104, 2426, 1011, 4013, 2078, 1011, 2214, 5268, 1996, 19528, 2229, 2021, 1011, 4013, 2078, 1011, 6217, 2022, 14071, 2063, 1999, 1011, 4013, 2078, 1011, 3449, 2094, 2365, 1996, 3804, 1997, 14246, 2063, 2487, 1037, 8837, 2007, 1996, 2111, 2022, 3102, 2011, 1037, 2991, 2013, 1011, 4013, 2078, 1011, 9118, 1998, 2023, 2022, 2178, 5213, 2000, 1011, 4013, 2078, 1011, 6106, 7185, 2487, 2402, 7631, 2022, 2681, 1998, 1996, 2332, 2031, 2036, 2195, 2365, 7185, 2487, 1997, 3183, 1996, 3804, 1997, 2711, 2487, 1011, 4013, 2078, 1011, 2507, 1999, 3510, 2000, 2711, 2487, 1996, 2905, 1998, 20020, 3653, 17421, 24971, 2000, 1996, 3035, 1997, 14246, 2063, 2487, 2295, 2011, 5036, 2007, 1996, 2060, 4496, 2361, 2487, 2373, 1011, 4013, 2078, 1011, 2031, 2022, 5993, 2008, 1011, 4013, 2078, 1011, 2323, 2025, 5914, 1037, 4496, 2361, 2487, 3159, 4983, 1996, 3035, 2031, 2775, 1997, 1011, 4013, 2078, 1011, 2219, 16290, 2005, 1011, 4013, 2078, 1011, 2155, 2022, 1037, 2307, 15226, 2000, 1011, 4013, 2078, 1011, 3395, 1998, 2012, 1996, 2168, 2051, 1037, 18487, 1996, 3804, 2711, 2487, 2040, 2031, 4028, 1011, 4013, 2078, 1011, 2564, 10797, 5920, 1999, 3827, 2000, 4468, 2270, 7781, 1998, 1996, 4496, 2361, 2487, 13520, 3251, 2074, 2135, 2030, 4895, 29427, 2135, 2008, 2023, 2031, 2022, 3499, 2738, 2084, 2292, 1037, 7015, 3280, 1037, 10768, 7811, 2022, 2331, 1996, 4329, 1997, 1999, 8741, 1997, 1996, 3623, 14165, 1997, 1996, 2406, 2045, 2022, 2236, 4487, 3736, 16020, 7542, 2045, 2022, 7185, 2487, 2283, 1996, 2030, 20898, 2923, 2040, 2907, 2011, 2711, 2487, 1998, 1011, 4013, 2078, 1011, 2704, 2711, 2487, 1998, 3005, 10780, 2022, 1996, 13012, 25778, 8162, 1996, 4496, 2361, 2487, 2040, 9279, 1011, 4013, 2078, 1011, 9721, 2000, 1996, 8340, 2711, 2487, 1998, 3005, 6454, 2022, 1996, 2317, 15477, 5210, 1996, 4496, 2361, 2487, 1998, 1996, 4496, 2361, 2487, 3005, 10780, 2022, 1996, 2417, 6178, 1998, 5210, 1037, 5157, 2005, 1037, 6329, 2008, 2323, 2421, 1996, 3742, 1997, 1996, 2111, 2022, 15454, 1998, 1996, 2236, 28606, 10364, 1011, 4013, 2078, 1011, 2041, 1999, 4613, 2012, 2576, 19032, 2019, 3535, 2000, 2644, 7185, 2487, 1997, 2122, 2599, 2000, 2019, 2039, 3217, 2906, 8917, 2487, 10214, 2000, 2543, 2006, 1996, 2111, 1998, 1011, 4013, 2078, 1011, 8111, 4125, 4895, 5403, 18141, 2061, 2008, 1996, 2332, 2228, 5012, 15784, 3696, 2019, 19935, 21261, 1998, 10574, 2000, 14246, 2063, 2487, 1999, 3058, 2487, 1037, 10864, 2231, 2022, 2433, 1998, 1037, 2047, 4552, 2022, 2000, 2022, 13621, 2021, 1996, 14246, 2063, 2487, 11240, 2040, 2424, 1011, 4013, 2078, 1011, 4650, 15704, 1998, 2428, 2215, 9945, 1997, 7177, 2025, 1997, 2157, 2191, 16915, 2153, 1998, 2153, 1998, 19820, 5555, 3207, 1996, 2395, 6229, 1011, 4013, 2078, 1011, 2022, 2633, 2404, 2091, 2011, 2236, 2711, 2487, 2096, 1996, 2717, 1997, 14246, 2063, 2487, 2022, 4498, 7790, 2006, 1996, 2097, 1997, 1996, 3007, 2044, 3058, 2487, 1037, 3072, 2022, 5646, 2006, 2029, 2022, 2000, 2031, 1037, 2343, 2012, 1011, 4013, 2078, 1011, 2132, 5454, 3058, 2487, 2011, 8917, 2487, 7833, 2000, 2711, 2487, 2022, 1996, 2030, 18979, 2140, 2487, 2343, 2947, 5454, 1998, 2044, 2070, 5998, 1011, 4013, 2078, 1011, 2025, 2069, 3040, 14246, 2063, 2487, 2021, 2011, 1996, 2393, 1997, 1996, 2390, 2029, 2022, 3262, 2711, 2487, 1011, 4013, 2078, 1011, 19776, 1996, 4574, 1997, 4112, 1998, 17727, 6935, 2239, 2030, 8340, 2035, 1996, 7116, 3183, 1996, 10123, 2031, 2025, 2404, 2000, 2331, 2006, 1996, 14865, 1997, 2019, 5987, 4125, 1997, 1996, 11240, 2023, 2022, 2655, 1037, 8648, 1040, 11937, 2102, 1998, 2711, 2487, 2022, 2059, 13520, 2343, 2005, 3058, 2487, 1996, 2030, 18979, 2140, 2487, 3400, 1999, 3058, 2487, 1996, 2343, 2202, 1996, 2516, 1997, 3750, 2655, 1011, 4013, 2078, 1011, 2711, 2487, 2004, 6332, 2000, 1996, 2402, 2365, 1997, 2711, 2487, 1011, 4013, 2078, 1011, 2562, 2039, 1037, 21459, 1998, 6450, 2457, 2191, 14246, 2063, 2487, 2062, 2084, 2412, 1996, 9121, 4497, 1997, 1996, 2088, 1998, 2079, 2172, 2000, 5335, 1011, 4013, 2078, 1011, 2011, 1996, 17973, 1997, 2395, 1998, 8208, 1997, 2214, 2311, 5036, 2022, 2191, 2029, 2172, 5335, 3119, 1998, 1996, 2406, 5083, 1999, 14165, 1996, 27788, 1997, 2231, 2022, 2174, 7371, 2907, 1998, 2498, 2022, 2061, 2172, 4468, 2004, 1996, 2292, 2158, 2228, 2030, 2552, 2005, 1011, 4013, 2078, 1011, 2096, 1011, 4013, 2078, 1011, 3239, 2022, 2000, 2022, 4830, 17644, 2007, 11867, 7770, 26797, 2099, 1998, 3377, 1999, 2043, 14246, 2063, 2487, 2022, 2886, 14246, 2063, 2487, 1996, 3750, 8917, 2487, 2007, 14246, 2063, 2487, 1999, 4559, 1998, 1996, 7185, 2487, 2390, 2362, 2022, 11741, 3351, 14246, 2063, 2487, 1998, 2954, 1996, 2645, 1997, 11346, 1998, 10710, 18689, 2078, 2202, 1996, 2103, 2044, 3058, 2487, 2022, 6859, 1998, 2059, 2191, 2054, 2022, 2113, 2004, 1996, 5036, 1997, 14246, 2063, 2487, 2029, 11302, 1996, 3808, 1997, 14246, 2063, 2487, 2061, 2146, 2004, 1996, 3395, 4496, 2361, 2487, 3842, 2022, 2025, 28616, 8557, 1999, 2711, 2487, 3693, 1999, 2019, 2886, 2006, 1996, 4496, 2361, 2487, 2373, 1999, 14246, 2063, 2487, 1998, 2362, 2007, 2711, 2487, 1997, 14246, 2063, 2487, 1998, 1996, 4496, 2361, 2487, 5114, 7185, 2487, 2307, 3377, 2012, 17454, 12380, 1998, 14017, 7512, 5740, 2021, 2191, 3521, 2004, 2574, 2004, 1011, 4013, 2078, 1011, 2022, 14057, 2000, 1011, 4013, 2078, 1011, 2302, 7634, 2000, 1011, 4013, 2078, 1011, 4872, 2000, 1996, 2332, 1997, 14246, 2063, 2487, 2040, 2022, 27885, 3669, 3351, 2000, 5309, 1011, 4013, 2078, 1011, 9619, 2000, 2468, 2332, 1997, 14246, 2063, 2487, 2011, 10750, 2039, 2000, 14246, 2063, 2487, 1011, 4013, 2078, 1011, 2214, 12839, 1997, 8917, 2487, 1998, 3835, 12507, 27648, 4088, 2000, 3500, 2039, 2012, 2188, 1998, 1996, 2417, 4496, 2361, 2487, 4382, 2022, 2147, 2006, 1996, 4121, 7280, 2191, 2011, 1996, 3144, 2069, 5587, 2000, 1996, 3168, 1997, 5688, 3595, 2554, 2022, 2012, 2147, 1998, 1996, 3750, 2044, 3058, 2487, 1997, 3112, 2514, 1011, 4013, 2078, 1011, 6217, 14071, 2063, 1996, 9341, 4496, 2361, 2487, 2162, 1999, 1996, 4496, 2361, 2487, 2040, 2031, 2139, 20688, 1011, 4013, 2078, 1011, 3035, 2711, 2487, 2462, 2191, 3601, 1997, 1037, 7189, 1997, 1996, 2332, 1997, 14246, 2063, 2487, 2004, 1011, 4013, 2078, 1011, 2332, 2045, 2031, 2146, 2022, 8618, 14225, 2090, 14246, 2063, 2487, 1998, 14246, 2063, 2487, 1998, 2295, 1996, 3159, 10214, 1996, 3749, 1997, 14246, 2063, 2487, 1996, 4496, 2361, 2487, 2265, 2107, 2019, 2058, 4783, 22397, 4382, 2008, 1037, 2162, 3338, 2041, 1996, 2613, 4792, 1997, 14246, 2063, 2487, 2022, 2000, 6855, 1996, 2172, 11821, 2102, 8880, 1997, 1996, 8840, 2278, 2487, 1998, 1996, 3750, 3684, 1011, 4013, 2078, 1011, 2390, 2007, 8945, 14083, 3993, 16413, 2029, 2022, 2021, 1996, 19508, 2000, 18704, 3993, 4154, 2012, 8917, 2487, 1998, 2005, 7693, 2012, 15134, 1996, 3750, 2022, 2486, 2000, 7806, 1011, 4013, 2078, 1011, 2004, 1037, 7267, 1998, 1996, 14841, 4667, 2053, 2574, 7180, 2012, 14246, 2063, 2487, 2084, 1996, 2878, 1997, 1996, 2111, 2735, 1011, 4013, 2078, 1011, 14532, 2006, 1011, 4013, 2078, 1011, 1998, 1011, 4013, 2078, 1011, 2155, 1011, 4013, 2078, 1011, 2564, 2711, 2487, 2031, 2000, 10574, 1037, 3072, 2022, 13520, 1998, 1996, 2103, 7374, 2000, 3233, 1037, 6859, 1996, 4496, 2361, 2487, 5083, 1998, 2404, 2091, 2035, 5012, 1999, 2060, 2112, 1997, 14246, 2063, 2487, 2307, 2112, 1997, 1996, 2390, 2031, 2022, 2191, 7267, 1998, 2295, 2045, 2022, 2172, 11655, 3567, 3527, 2045, 2022, 2210, 26261, 17190, 2791, 2030, 8424, 2681, 2426, 2216, 2040, 2085, 2202, 2039, 2849, 14246, 2063, 2487, 2029, 2022, 15823, 2044, 9015, 2172, 2013, 15625, 7806, 1999, 3058, 2487, 1998, 3521, 2022, 5309, 1999, 1037, 5036, 2011, 2029, 2307, 2112, 1997, 2711, 2487, 1998, 2711, 2487, 1998, 1996, 2103, 1997, 2711, 2487, 2022, 2507, 2067, 2000, 14246, 2063, 2487, 1996, 2203, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyguIeNSihp6"
      },
      "source": [
        "Отправляем подвыборку в BERT за эмбеддингами"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goMQil4Nigle",
        "outputId": "36d9a42a-73da-4e4b-cefb-9d4a11c5c320"
      },
      "source": [
        "# checking batch maker\n",
        "\n",
        "BATCH_SZ = 32\n",
        "\n",
        "word_cnt = Counter()\n",
        "\n",
        "total_word_cnt = 0\n",
        "for document in docs_sample[:10]:\n",
        "    words = document.split()\n",
        "    for i in range(0, len(words), BATCH_SZ):\n",
        "        batch = words[i:i + BATCH_SZ]\n",
        "        word_cnt.update(Counter(batch))\n",
        "        total_word_cnt += len(batch)\n",
        "total_word_cnt  # то же что в ячейке выше? да"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "312240"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6TCPDzVtng4",
        "outputId": "8a0c3d6d-78ec-4bc4-f63d-512315ccdc15"
      },
      "source": [
        "word_cnt.most_common(100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('-PRON-', 30366),\n",
              " ('the', 16371),\n",
              " ('be', 13802),\n",
              " ('and', 10069),\n",
              " ('to', 8804),\n",
              " ('PERSON1', 7930),\n",
              " ('of', 7508),\n",
              " ('a', 5900),\n",
              " ('have', 5365),\n",
              " ('in', 4688),\n",
              " ('that', 4648),\n",
              " ('i', 3971),\n",
              " ('not', 3539),\n",
              " ('as', 2737),\n",
              " ('with', 2510),\n",
              " ('do', 2388),\n",
              " ('on', 2169),\n",
              " ('say', 2124),\n",
              " ('but', 2070),\n",
              " ('for', 2068),\n",
              " ('mr', 1986),\n",
              " ('at', 1793),\n",
              " ('so', 1692),\n",
              " ('CARDINAL1', 1686),\n",
              " ('would', 1445),\n",
              " ('ORG1', 1406),\n",
              " ('all', 1354),\n",
              " ('GPE1', 1344),\n",
              " ('go', 1277),\n",
              " ('man', 1224),\n",
              " ('will', 1216),\n",
              " ('no', 1208),\n",
              " ('there', 1051),\n",
              " ('this', 1046),\n",
              " ('from', 1035),\n",
              " ('could', 1025),\n",
              " ('DATE1', 1009),\n",
              " ('which', 998),\n",
              " ('come', 966),\n",
              " ('by', 959),\n",
              " ('if', 940),\n",
              " ('slope', 934),\n",
              " ('make', 931),\n",
              " ('out', 927),\n",
              " ('when', 899),\n",
              " ('see', 888),\n",
              " ('then', 888),\n",
              " ('know', 837),\n",
              " ('think', 824),\n",
              " ('what', 819),\n",
              " ('very', 801),\n",
              " ('up', 775),\n",
              " ('who', 762),\n",
              " ('mrs', 756),\n",
              " ('well', 749),\n",
              " ('or', 739),\n",
              " ('one', 709),\n",
              " ('TIME1', 699),\n",
              " ('such', 688),\n",
              " ('now', 679),\n",
              " ('an', 667),\n",
              " ('little', 659),\n",
              " ('get', 658),\n",
              " ('may', 651),\n",
              " ('can', 609),\n",
              " ('take', 607),\n",
              " ('about', 595),\n",
              " ('should', 587),\n",
              " ('look', 585),\n",
              " ('some', 572),\n",
              " ('any', 560),\n",
              " ('old', 557),\n",
              " ('bishop', 555),\n",
              " ('tell', 546),\n",
              " ('into', 536),\n",
              " ('much', 515),\n",
              " ('down', 512),\n",
              " ('give', 503),\n",
              " ('good', 498),\n",
              " ('other', 497),\n",
              " ('more', 491),\n",
              " ('over', 481),\n",
              " ('eleanor', 478),\n",
              " ('own', 467),\n",
              " ('than', 458),\n",
              " ('like', 440),\n",
              " ('time', 425),\n",
              " ('how', 424),\n",
              " ('bully', 412),\n",
              " ('after', 411),\n",
              " ('before', 406),\n",
              " ('arabin', 406),\n",
              " ('never', 392),\n",
              " ('day', 390),\n",
              " ('boy', 381),\n",
              " ('oh', 380),\n",
              " ('archdeacon', 380),\n",
              " ('great', 374),\n",
              " ('woman', 373),\n",
              " ('ORDINAL1', 372)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "57ca74699e994dea970fbbd5575a9b34",
            "208114147c2744b2a7b0949eebcf6727",
            "c919d3a7b365437b9ea124587252367a",
            "d1179e63a23846208bf9007a2b88fd68",
            "fef476fb41d44d6ea7f7a72ca527865b",
            "683a948edd1c421098c464d7291c6243",
            "9d6732461d5e49cd9eb9b97a4baf0638",
            "5d69ecbad56f4d4e891a831789e398ab"
          ]
        },
        "id": "kTIIhGuvooYg",
        "outputId": "841c8718-cff4-4300-b0b9-5a27d715383c"
      },
      "source": [
        "embed_batches_sample = []  # эмбеддинги для текстов docs_sample\n",
        "\n",
        "total_word_cnt = 0\n",
        "for document in tqdm(docs_sample[:10]):\n",
        "    words = document.split()\n",
        "    for i in range(0, len(words), BATCH_SZ):\n",
        "        batch = words[i:i + BATCH_SZ]\n",
        "        embed_batches_sample.append(get_embeddings(' '.join(batch)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57ca74699e994dea970fbbd5575a9b34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B9OrwrMry8P",
        "outputId": "05aacd01-8457-4c69-81c8-6a681d449698"
      },
      "source": [
        "print('number of uncleaned batches', len(embed_batches_sample))\n",
        "print('total tokens in batches', sum(map(len, embed_batches_sample)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of uncleaned batches 9761\n",
            "total tokens in batches 439019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylTTDAUtbK3n",
        "outputId": "e284303f-4970-4a54-f8a3-0da0d739145b"
      },
      "source": [
        "embed_batches_sample[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(44, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDoGuYQf0aax"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import InputLayer, Dense\n",
        "from keras.models import Model, Sequential\n",
        "\n",
        "ORIGINAL_DIM = 768\n",
        "TARGET_DIM = 200\n",
        "\n",
        "encoder = Sequential([\n",
        "    InputLayer(input_shape=(ORIGINAL_DIM,)),\n",
        "    # Dense(90, activation='tanh'),\n",
        "    # Dense(32, activation='relu'),\n",
        "    Dense(TARGET_DIM, activation='tanh')\n",
        "])\n",
        "\n",
        "decoder = Sequential([\n",
        "    InputLayer(input_shape=(TARGET_DIM,)),\n",
        "    # Dense(32, activation='relu'),\n",
        "    # Dense(90, activation='tanh'),\n",
        "    Dense(ORIGINAL_DIM, activation=None)\n",
        "])\n",
        "\n",
        "autoencoder = Model(inputs=encoder.input, outputs=decoder(encoder.output))\n",
        "autoencoder.compile(loss='mse', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nr57XrF5zk-D",
        "outputId": "4cfb6649-dd83-4c6d-dc7c-bb7ba80321d2"
      },
      "source": [
        "autoencoder.count_params()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "308168"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF2n7t_sZMQy"
      },
      "source": [
        "# import numpy as np\n",
        "# train_data = np.random.rand(100_000, 768)\n",
        "# val_data = np.random.rand(30, 768)\n",
        "# test_data = np.random.rand(30, 768)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIX3AF_mD4oH",
        "outputId": "60c51eeb-de23-4d2b-ee76-e3472ed4bb90"
      },
      "source": [
        "train_data = np.vstack([batch for batch in embed_batches_sample])\n",
        "train_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(439019, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp4b_isBzniI",
        "outputId": "603496de-ee81-4678-850b-860c1b44c4ee"
      },
      "source": [
        "model_history = autoencoder.fit(train_data, train_data, epochs=6, batch_size=64)#, validation_data=(val_data, val_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "6860/6860 [==============================] - 47s 7ms/step - loss: 2.1663\n",
            "Epoch 2/6\n",
            "6860/6860 [==============================] - 37s 5ms/step - loss: 1.2326\n",
            "Epoch 3/6\n",
            "6860/6860 [==============================] - 37s 5ms/step - loss: 1.2072\n",
            "Epoch 4/6\n",
            "6860/6860 [==============================] - 37s 5ms/step - loss: 1.2027\n",
            "Epoch 5/6\n",
            "6860/6860 [==============================] - 37s 5ms/step - loss: 1.1992\n",
            "Epoch 6/6\n",
            "6860/6860 [==============================] - 37s 5ms/step - loss: 1.1986\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVZpWlIa7wUQ",
        "outputId": "5e7cf1c8-829c-40c9-e786-4c714d7af23c"
      },
      "source": [
        "model_history.history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [0.06289330869913101,\n",
              "  0.06211361289024353,\n",
              "  0.061545230448246,\n",
              "  0.06111234053969383,\n",
              "  0.06082377955317497,\n",
              "  0.06063752993941307],\n",
              " 'val_loss': [0.06272749602794647,\n",
              "  0.06248430907726288,\n",
              "  0.06267774105072021,\n",
              "  0.06290314346551895,\n",
              "  0.06296861916780472,\n",
              "  0.06302959471940994]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88W32CiJFp_Q"
      },
      "source": [
        "plt.plot(model_history.history[\"loss\"])\n",
        "plt.title(\"Loss vs. Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.grid(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yirJkFHvFtVC"
      },
      "source": [
        "import sys\n",
        "\n",
        "print('size of train_data of 430k tokens is', sys.getsizeof(locals()['train_data']) / 1024 / 1024, 'megabytes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1L3JC16WJ3nh"
      },
      "source": [
        "Todo:\n",
        "\n",
        "0*. Формализовать исследования, проверить каскадный спуск автоенкодерами (насколько лучше `768 -> 200 -> 50`, нежели `768 -> 50`)\n",
        "\n",
        "1. Выяснить, как сэплировать слова для обучения (частотные чаще или все слова уникальны в выборке)\n",
        "\n",
        "2. Пофиксить `train_data` (заменить токены от `spacy` на слова; убрать unknown (?))\n",
        "\n",
        "3. Обучить заново для нормальной `train_data`, которую и сохранить. Для русского тоже (мб потом?)\n",
        "\n",
        "4. Продолжить родительский пайплайн (тг)\n",
        "\n",
        "---\n",
        "\n",
        "родительский:\n",
        "\n",
        "План на ср-чт:\n",
        "1. Проделать заново их код со blob-ом: взять папку с текстами, предобработать, хранить не текст, а список предложений (длина не больше 512). Убрать 1/2, 3/4, ... Особые токены не заменять на entity tag, но числа можно объединить в одно (как и географические объекты, например, словом \"география\", и т. д.)\n",
        "2. Предложения из подвыборки текстов (сохранить взятые индексы!) поочерёдно прогонять через BERT (get_embeddings). Оставить текст как список эмбеддингов\n",
        "3. Использовать этот датасет как обучалку для автоенкодера (stacked версия). Обучить двойной автоенкодер, сохранить модель\n",
        "4. Прогнать весь датасет для кластеризации через автоенкодер\n",
        "5. Отправить вектора на кластеризацию (параметры взять те, которые уже попробовали остальные)\n",
        "\n",
        "---\n",
        "\n",
        "Глобальная цель:\n",
        "\n",
        "* сделать много автоенкодеров и получить много словарей для разных целевых длин эмбеддингов.\n",
        "* сделать такие же автоенкодеры для других языков (рус., англ)\n",
        "* сделать автоенкодеры для языков и для w2v\n",
        "* для всего этого добра запустить кластеризацию\n",
        "* сравнить с svd и с svd к hidden_state для каждой модели для каждого языка "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3twVGxRIfqq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzICIrUwH_Az"
      },
      "source": [
        "# misc\n",
        "\n",
        "import numpy as np \n",
        "\n",
        "zz = np.zeros((10**6, 768), dtype=float)\n",
        "zz.nbytes / 1024 / 1024, 'megabytes'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk0FpoaLF0SN"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq7rLyesgNUQ"
      },
      "source": [
        "from time import time\n",
        "\n",
        "st = time()\n",
        "num = 0\n",
        "for i in range(10 ** 7):\n",
        "    num += i % 3\n",
        "print(time() - st)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuxL_xQgGpEn"
      },
      "source": [
        "# Подготовка текста"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhdObuiNG1Ro",
        "outputId": "220e2aea-1238-485f-be65-080cf2214eef"
      },
      "source": [
        "# !pip install gensim\n",
        "# !pip install -U pip setuptools wheel\n",
        "# !pip install -U spacy\n",
        "!spacy download en_core_web_lg\n",
        "# !pip install natasha"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (56.1.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.4.3)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-cp37-none-any.whl size=829180944 sha256=68c281f78f8cdb3276b8c81b4ef845a0489d58c6d6f88a0a0cf552f430df45ea\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-23966crs/wheels/2a/c1/a6/fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJzGgGUBGwv0"
      },
      "source": [
        "import re\n",
        "import gensim\n",
        "import glob\n",
        "import en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t44X_COYGobz"
      },
      "source": [
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won['’‘`]t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can['’‘`]t\", \"can not\", phrase)\n",
        "    phrase = re.sub(r\"ain['’‘`]t\", \"am not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n['’‘`]t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"['’‘`]re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"['’‘`]s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"['’‘`]d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"['’‘`]ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"['’‘`]t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"['’‘`]ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"['’‘`]m\", \" am\", phrase)\n",
        "\n",
        "    #phrase = re.sub('([.;!?])', r' \\1 ', phrase)\n",
        "    phrase = re.sub(r'[^\\w.?!;]', ' ', phrase)\n",
        "    phrase = re.sub(' +', ' ', phrase)\n",
        "    sentences = re.split('([.;!?] *)', phrase)\n",
        "\n",
        "    return ' '.join([i.capitalize() for i in  sentences])\n",
        "\n",
        "\n",
        "def prepare_english_text(input_path, output_path):\n",
        "    nlp = en_core_web_lg.load(disable=['parser'])\n",
        "    nlp.max_length = 5000000\n",
        "    pos_dict = {'PROPN': 'PERSON1', 'PRON': 'PRON1', 'NUM': 'ORDINAL1'}\n",
        "    file_name =  input_path.split('/')[-1]\n",
        "    try:\n",
        "        with open(input_path, 'r') as lit_text:\n",
        "            raw_text = lit_text.read()\n",
        "    except Exception as ex:\n",
        "        return\n",
        "    \n",
        "    preprocessed_text = decontracted(raw_text)\n",
        "    #preprocessed_list = gensim.utils.simple_preprocess(raw_text, min_len=1, max_len=100, deacc = True)\n",
        "    #preprocessed_text  = ' '.join(preprocessed_list)\n",
        "\n",
        "    nlp_doc = nlp(preprocessed_text)\n",
        "    sorted_ents = sorted(nlp_doc.ents, key = lambda x: len(x), reverse =  True)\n",
        "\n",
        "\n",
        "    for ent in sorted_ents:\n",
        "        preprocessed_text = preprocessed_text.replace(' ' + ent.text + ' ', ' ' + ent.label_+ '1 ')\n",
        "        if not ent.text.islower():\n",
        "            preprocessed_text = preprocessed_text.replace(' ' + ent.text.lower() + ' ', ' ' + ent.label_+ '1 ')\n",
        "\n",
        "    new_nlp_doc = nlp(preprocessed_text)\n",
        "\n",
        "    with open(output_path + file_name, 'w') as prepared_text:\n",
        "        for token in new_nlp_doc:\n",
        "            if token.text[-1] != '1': \n",
        "                if token.pos_ in pos_dict:\n",
        "                    prepared_text.write(pos_dict[token.pos_])\n",
        "                    prepared_text.write('\\n')\n",
        "                    \n",
        "                elif token.pos_ != 'PUNCT':\n",
        "                    prepared_text.write(token.lemma_.lower())\n",
        "                    prepared_text.write('\\n')\n",
        "\n",
        "            else:\n",
        "                prepared_text.write(token.text)\n",
        "                prepared_text.write('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQQQkSnWBKWV"
      },
      "source": [
        "Выбор оптимального местоимения для замены: (получается `he`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrROXn9gBJOR",
        "outputId": "6729e701-7ddc-46fb-b082-a78ac03b4dcc"
      },
      "source": [
        "prons = ['they', 'it', 'i', 'he', 'she', 'someone', 'who', 'whoever', 'that', 'myself', 'my']\n",
        "\n",
        "prons = [get_embeddings(pron) for pron in prons]\n",
        "\n",
        "def vec_metrics(a, b):\n",
        "    return {\n",
        "        'rmse': np.mean((a - b) ** 2) ** .5,\n",
        "        'cosine': cosine(a, b)\n",
        "    }\n",
        "\n",
        "she, he = prons[4], prons[3]\n",
        "print(vec_metrics(she, he))\n",
        "\n",
        "avg_cosines = []\n",
        "for i, pron in enumerate(prons):\n",
        "    cur_cosines = [cosine(pron, pron2) for j, pron2 in enumerate(prons) if i != j]\n",
        "    avg_cosines.append(np.mean(cur_cosines))\n",
        "\n",
        "print(avg_cosines)\n",
        "print('best pron index:', np.argmin(avg_cosines))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'rmse': 0.8906200141097933, 'cosine': 0.06834912300109863}\n",
            "[0.23861536383628845, 0.24887626767158508, 0.23184401392936707, 0.21416858434677125, 0.2229945421218872, 0.23298413157463074, 0.31592010855674746, 0.35806862115859983, 0.3497359871864319, 0.3393749713897705, 0.28507325053215027]\n",
            "best pron index: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZDVFo9B9JwG"
      },
      "source": [
        "Смотрю на токены в spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3YTvWhxOcNd"
      },
      "source": [
        "nlp = en_core_web_lg.load(disable=['parser'])\n",
        "nlp.max_length = 5000000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNBENRK89JcV"
      },
      "source": [
        "spacy_ent2word = {\n",
        "    'CARDINAL': 'NUMBER',\n",
        "    'DATE': 'DATE',\n",
        "    'EVENT': 'EVENT',\n",
        "    'FAC': 'FACILITY',\n",
        "    'GPE': 'COUNTRY',\n",
        "    'LANGUAGE': 'LANGUAGE',\n",
        "    'LAW': 'LAW',\n",
        "    'LOC': 'LOCATION',\n",
        "    'MONEY': 'MONEY',\n",
        "    'NORP': 'NATION',\n",
        "    'ORDINAL': 'FIRST',\n",
        "    'ORG': 'COMPANY',\n",
        "    'PERCENT': 'PERCENT',\n",
        "    'PERSON': 'PERSON',\n",
        "    'PRODUCT': 'PRODUCT',\n",
        "    'QUANTITY': 'QUANTITY',\n",
        "    'TIME': 'TIME',\n",
        "    'WORK_OF_ART': 'ART',\n",
        "}\n",
        "    \n",
        "def prepare_english_text(text):\n",
        "    # nlp = en_core_web_lg.load(disable=['parser'])\n",
        "    # nlp.max_length = 5000000\n",
        "\n",
        "    pos_dict = {'PRON': 'HE', 'NUM': 'NUMBER'}\n",
        "\n",
        "    preprocessed_text = decontracted(text)\n",
        "\n",
        "    nlp_doc = nlp(preprocessed_text)\n",
        "\n",
        "    result_text = ''\n",
        "    for token in nlp_doc:\n",
        "        print('\\t', token, token.text, token.pos_, token.lemma_)\n",
        "        if token.ent_type_ != '':\n",
        "            result_text += token.text + ' '\n",
        "            continue\n",
        "        if token.pos_ == 'PUNCT':\n",
        "            continue\n",
        "        result_word = token.text\n",
        "        if token.pos_ in pos_dict:\n",
        "            result_word = pos_dict[token.pos_]\n",
        "        elif token.lemma_[1:-1] in pos_dict:  # например, \"my\" -> \"-PRON-\"\n",
        "            result_word = pos_dict[token.lemma_[1:-1]]\n",
        "        else:\n",
        "            if not token.lemma_.islower() and token.lemma_ != ' ':\n",
        "                print('preprocessing: token.lemma_ has upper case --- ', token, token.lemma_, file=sys.stderr)\n",
        "            result_word = token.lemma_.lower()\n",
        "        result_text += result_word + ' '\n",
        "\n",
        "    sorted_ents = sorted(nlp_doc.ents, key=len, reverse=True)\n",
        "    for ent in sorted_ents:\n",
        "        print(ent.text, ent.label_)\n",
        "        result_text = result_text.replace(' ' + ent.text + ' ', ' ' + spacy_ent2word[ent.label_] + ' ')\n",
        "        # if not ent.text.islower():\n",
        "        #     preprocessed_text = preprocessed_text.replace(' ' + ent.text.lower() + ' ', ' ' + spacy_ent2word[ent.label_] + ' ')\n",
        "\n",
        "    return re.sub(r'\\s\\s+', ' ', result_text).strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOENndDP-N9P"
      },
      "source": [
        "sample_text = \"\"\"I'm loving Mars and other goods.          These are my favourite     chocolate bars.\n",
        "\n",
        "I really love these!\n",
        "\n",
        "123\n",
        "And I come from Moscow.\"\"\"\n",
        "\n",
        "sample_text = \"\"\"I'm loving Mars and other goods.          These are my favourite     chocolate bars.\n",
        "\n",
        "I really love these!\n",
        "\n",
        "123\n",
        "And I come from Moscow.\n",
        "\n",
        "Ural Mountains are very high. Let's go to France maybe, and English language. They don't use US Dollars there though...\n",
        "\n",
        "Moscow, Saint Petersburg, London, England, UK, United States, Australia. \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxX_YkbC9I0X",
        "outputId": "f1eac87b-086e-4ef5-b38c-b57d73b291fd"
      },
      "source": [
        "preprocessed_text = prepare_english_text(sample_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t I I PRON -PRON-\n",
            "\t am am AUX be\n",
            "\t loving loving VERB love\n",
            "\t mars mars PROPN mars\n",
            "\t and and CCONJ and\n",
            "\t other other ADJ other\n",
            "\t goods goods NOUN good\n",
            "\t . . PUNCT .\n",
            "\t     SPACE  \n",
            "\t These These DET these\n",
            "\t are are AUX be\n",
            "\t my my DET -PRON-\n",
            "\t favourite favourite ADJ favourite\n",
            "\t chocolate chocolate NOUN chocolate\n",
            "\t bars bars NOUN bar\n",
            "\t . . PUNCT .\n",
            "\t     SPACE  \n",
            "\t I I PRON -PRON-\n",
            "\t really really ADV really\n",
            "\t love love VERB love\n",
            "\t these these DET these\n",
            "\t ! ! PUNCT !\n",
            "\t     SPACE  \n",
            "\t 123 123 NUM 123\n",
            "\t and and CCONJ and\n",
            "\t i i PRON i\n",
            "\t come come VERB come\n",
            "\t from from ADP from\n",
            "\t moscow moscow PROPN moscow\n",
            "\t . . PUNCT .\n",
            "\t     SPACE  \n",
            "\t Ural Ural PROPN Ural\n",
            "\t mountains mountains NOUN mountain\n",
            "\t are are AUX be\n",
            "\t very very ADV very\n",
            "\t high high ADJ high\n",
            "\t . . PUNCT .\n",
            "\t     SPACE  \n",
            "\t Let Let VERB let\n",
            "\t is is AUX be\n",
            "\t go go VERB go\n",
            "\t to to ADP to\n",
            "\t france france PROPN france\n",
            "\t maybe maybe ADV maybe\n",
            "\t and and CCONJ and\n",
            "\t english english PROPN english\n",
            "\t language language NOUN language\n",
            "\t . . PUNCT .\n",
            "\t     SPACE  \n",
            "\t They They PRON -PRON-\n",
            "\t do do AUX do\n",
            "\t not not PART not\n",
            "\t use use VERB use\n",
            "\t us us PROPN us\n",
            "\t dollars dollars NOUN dollar\n",
            "\t there there ADV there\n",
            "\t though though ADV though\n",
            "\t . . PUNCT .\n",
            "\t     SPACE  \n",
            "\t . . PUNCT .\n",
            "\t     SPACE  \n",
            "\t . . PUNCT .\n",
            "\t     SPACE  \n",
            "\t Moscow Moscow PROPN Moscow\n",
            "\t saint saint PROPN saint\n",
            "\t petersburg petersburg PROPN petersburg\n",
            "\t london london PROPN london\n",
            "\t england england PROPN england\n",
            "\t uk uk PROPN uk\n",
            "\t united united PROPN united\n",
            "\t states states PROPN states\n",
            "\t australia australia PROPN australia\n",
            "\t . . PUNCT .\n",
            "\t     SPACE  \n",
            "saint petersburg ORG\n",
            "united states GPE\n",
            "mars LOC\n",
            "123 CARDINAL\n",
            "moscow GPE\n",
            "Ural NORP\n",
            "france GPE\n",
            "english LANGUAGE\n",
            "Moscow GPE\n",
            "england GPE\n",
            "uk GPE\n",
            "australia GPE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "pDU2p1cFbqgK",
        "outputId": "f924503d-72a8-415c-f4cf-3c2266fb3558"
      },
      "source": [
        "preprocessed_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'HE be love LOCATION and other good these be HE favourite chocolate bar HE really love these NUMBER and HE come from COUNTRY NATION mountain be very high let be go to COUNTRY maybe and LANGUAGE language HE do not use us dollar there though COUNTRY COMPANY london COUNTRY COUNTRY COUNTRY COUNTRY'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvbKVUvgJCu7",
        "outputId": "69edca56-56d3-4e07-bcc5-ae9e3483e6ca"
      },
      "source": [
        "print(tokenizer.tokenize(' '.join(spacy_ent2word.values())))\n",
        "print(tokenizer.tokenize('London'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['number', 'date', 'event', 'facility', 'country', 'language', 'law', 'location', 'money', 'nation', 'first', 'company', 'percent', 'person', 'product', 'quantity', 'time', 'art']\n",
            "['london']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW74Bf9_DI3g"
      },
      "source": [
        "# Анализ данных для автоенкодера"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyGfumHuDK9r"
      },
      "source": [
        "sentence_lens = []\n",
        "with open('drive/MyDrive/Coursework2021/sticked_file.txt', 'r') as f:\n",
        "    for sentence in f:\n",
        "        sentence_lens.append(len(sentence.split()))\n",
        "\n",
        "sentence_lens = np.asarray(sentence_lens, dtype=int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "hHVEnPdPDksj",
        "outputId": "16f0d12a-2005-4efb-b5b5-db0ba56d83c6"
      },
      "source": [
        "plt.hist(sentence_lens, bins=100)\n",
        "plt.title('Распределение длин предложений (число слов) в выборке для автоенкодеров')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aaMKxo3154K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA2N0RdG1683"
      },
      "source": [
        "Заготовка для кода а/е на суперкомпьтере"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM9XcVcN15uE"
      },
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "from keras.callbacks import History\n",
        "from keras.layers import InputLayer, Dense\n",
        "from keras.models import Model, Sequential\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from typing import List, Optional, Sequence\n",
        "\n",
        "\n",
        "class Autoencoder(Model):\n",
        "    \"\"\"\n",
        "    Холдер для одного автоенкодера\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, output_dim: int, activation: Optional[str] = None) -> None:\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.activation = activation\n",
        "        self.build_autoencoder()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.autoencoder(inputs)\n",
        "\n",
        "    def build_autoencoder(self) -> None:\n",
        "        self.encoder = Sequential([\n",
        "            InputLayer(input_shape=(self.input_dim,)),\n",
        "            # Dense(90, activation='tanh'),\n",
        "            # Dense(32, activation='relu'),\n",
        "            Dense(self.output_dim, activation=self.activation)\n",
        "        ])\n",
        "        self.decoder = Sequential([\n",
        "            InputLayer(input_shape=(self.output_dim,)),\n",
        "            # Dense(32, activation='relu'),\n",
        "            # Dense(90, activation='tanh'),\n",
        "            Dense(self.input_dim, activation=None)\n",
        "        ])\n",
        "        self.autoencoder = Model(inputs=self.encoder.input, outputs=self.decoder(self.encoder.output))\n",
        "        self.autoencoder.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "\n",
        "class StackedAutoencoder:\n",
        "    \"\"\"\n",
        "    Ступенчатый автоенкодер: отдельно обучается каждый слой-encoder на основе результатов предыдущего слоя\n",
        "    \"\"\"\n",
        "    def __init__(self, dims: Sequence[int], activation: Optional[str] = 'tanh') -> None:\n",
        "        \"\"\":param dims: все размерности шагов (от input_dim до output_dim)\"\"\"\n",
        "        self.dims = dims\n",
        "        self.input_dim = dims[0]\n",
        "        self.output_dim = dims[-1]\n",
        "        self.autoencoders = [Autoencoder(dims[i - 1], dims[i], activation=activation) for i in range(1, len(dims))]\n",
        "\n",
        "    def fit(self, train_data: np.ndarray, val_data: Optional[np.ndarray] = None, verbose: bool = False, epochs: int = 10) -> List[History]:\n",
        "        if verbose:\n",
        "            print(f'\\nStackAutoencoder: fitting {len(self.autoencoders)} autoencoders with dimensions {self.dims}', flush=True)\n",
        "\n",
        "        train_data_reduced = train_data\n",
        "        val_data_reduced = val_data\n",
        "        self.model_histories = []\n",
        "        for i, autoencoder in enumerate(self.autoencoders):\n",
        "            autoencoder.compile(loss='mse', optimizer='adam')\n",
        "            if verbose:\n",
        "                print(f'\\tfitting autoencoder number {i} with dimensions {autoencoder.input_dim, autoencoder.output_dim}', file=sys.stderr)\n",
        "            model_history = autoencoder.fit(\n",
        "                x=train_data_reduced,\n",
        "                y=train_data_reduced,\n",
        "                epochs=epochs,\n",
        "                batch_size=64,\n",
        "                validation_data=(val_data_reduced, val_data_reduced),\n",
        "                shuffle=True,\n",
        "                verbose=verbose,\n",
        "                # callbacks=[tensorboard]\n",
        "            )\n",
        "            train_data_reduced = autoencoder.encoder(train_data_reduced)\n",
        "            val_data_reduced = autoencoder.encoder(val_data_reduced)\n",
        "            self.model_histories.append(model_history)\n",
        "            if verbose:\n",
        "                print(f'StackedAutoencoder: autoencoder {i} - model history\\n{model_history.history}', flush=True)\n",
        "\n",
        "        self.fine_tune(train_data, val_data, verbose, epochs)\n",
        "        return self.model_histories\n",
        "\n",
        "    def fine_tune(self, train_data: np.ndarray, val_data: Optional[np.ndarray] = None, verbose: bool = False, epochs: int = 10) -> None:\n",
        "        \"\"\"Дообучает всю цепь сразу\"\"\"\n",
        "        if verbose:\n",
        "            print(f'StackedAutoencoder: fine-tuning...')\n",
        "\n",
        "        self.autoencoder_sequence = Sequential()\n",
        "        for autoencoder in self.autoencoders:\n",
        "            self.autoencoder_sequence.add(autoencoder.encoder)\n",
        "        for autoencoder in self.autoencoders[::-1]:\n",
        "            self.autoencoder_sequence.add(autoencoder.decoder)\n",
        "\n",
        "        self.autoencoder_sequence.compile(loss='mse', optimizer='adam')\n",
        "        self.finetune_history = self.autoencoder_sequence.fit(\n",
        "            x=train_data,\n",
        "            y=train_data,\n",
        "            epochs=epochs,\n",
        "            batch_size=64,\n",
        "            validation_data=(val_data, val_data),\n",
        "            shuffle=True,\n",
        "            verbose=verbose\n",
        "        )\n",
        "        if verbose:\n",
        "            print(f'StackedAutoencoder: fine-tuning history model history\\n{self.finetune_history.history}')\n",
        "\n",
        "    def encode(self, data: np.ndarray) -> np.ndarray:\n",
        "        for autoencoder in self.autoencoders:\n",
        "            data = autoencoder.encoder(data)\n",
        "        return data\n",
        "\n",
        "    def decode(self, data: np.ndarray) -> np.ndarray:\n",
        "        for autoencoder in self.autoencoders[::-1]:\n",
        "            data = autoencoder.decoder(data)\n",
        "        return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "hU8VrcO3IQEp",
        "outputId": "ec43a149-e68a-4e1c-8ad2-bb701e0ecc8d"
      },
      "source": [
        "sa = StackedAutoencoder([768, 500, 100])\n",
        "sa.fit(train_data, val_data, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-93-9b0ca7f1d154>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    sa = StackedAutoencoder([768, 500, 100], ='sigmoid')\u001b[0m\n\u001b[0m                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmxp_CGgN5OW",
        "outputId": "c83d4df4-9fe8-48a0-9ee8-bc24f3811055"
      },
      "source": [
        "sa.model_histories"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.callbacks.History at 0x7f9ccfec7910>,\n",
              " <keras.callbacks.History at 0x7f9ccfd66c50>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j9afUH8QfJL",
        "outputId": "918d4d42-77cc-4bcb-cf7f-34208af8ace2"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error as mse\n",
        "\n",
        "\n",
        "train_data_ae = sa.decode(sa.encode(train_data))\n",
        "mse(train_data, train_data_ae)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0742634160434713"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gupEf8PgQpH"
      },
      "source": [
        "# sa\n",
        "\n",
        "train_data = np.load('drive/MyDrive/Coursework2021/autoencoder/test/embeddings.npy')  # switch to train further"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwnAiADmg0h0",
        "outputId": "b6bd076d-4b1e-476d-81fd-6a052f3194a2"
      },
      "source": [
        "sa = StackedAutoencoder([400, 300, 200, 100], activation='LeakyReLU')\n",
        "sa.fit(train_data[:75000, :400], train_data[75000:, :400], True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "StackAutoencoder: fitting 3 autoencoders with dimensions [400, 300, 200, 100]\n",
            "\tfitting autoencoder number 0 with dimensions (400, 300)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1172/1172 [==============================] - 60s 6ms/step - loss: 2.1924 - val_loss: 0.4157\n",
            "Epoch 2/10\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.3663 - val_loss: 0.3525\n",
            "Epoch 3/10\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.3219 - val_loss: 0.3323\n",
            "Epoch 4/10\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.3099 - val_loss: 0.3301\n",
            "Epoch 5/10\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.3050 - val_loss: 0.3233\n",
            "Epoch 6/10\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.3018 - val_loss: 0.3239\n",
            "Epoch 7/10\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.2997 - val_loss: 0.3226\n",
            "Epoch 8/10\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.2976 - val_loss: 0.3229\n",
            "Epoch 9/10\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.2966 - val_loss: 0.3205\n",
            "Epoch 10/10\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.2950 - val_loss: 0.3200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\tfitting autoencoder number 1 with dimensions (300, 200)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 2.1065 - val_loss: 0.6382\n",
            "Epoch 2/10\n",
            "1172/1172 [==============================] - 5s 5ms/step - loss: 0.5155 - val_loss: 0.3751\n",
            "Epoch 3/10\n",
            "1172/1172 [==============================] - 5s 5ms/step - loss: 0.3418 - val_loss: 0.3305\n",
            "Epoch 4/10\n",
            "1172/1172 [==============================] - 5s 5ms/step - loss: 0.3108 - val_loss: 0.3168\n",
            "Epoch 5/10\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.3005 - val_loss: 0.3137\n",
            "Epoch 6/10\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.2951 - val_loss: 0.3101\n",
            "Epoch 7/10\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.2917 - val_loss: 0.3078\n",
            "Epoch 8/10\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.2901 - val_loss: 0.3075\n",
            "Epoch 9/10\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.2896 - val_loss: 0.3066\n",
            "Epoch 10/10\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.2887 - val_loss: 0.3043\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\tfitting autoencoder number 2 with dimensions (200, 100)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1172/1172 [==============================] - 4s 3ms/step - loss: 1.5020 - val_loss: 0.6008\n",
            "Epoch 2/10\n",
            "1172/1172 [==============================] - 3s 3ms/step - loss: 0.5200 - val_loss: 0.4539\n",
            "Epoch 3/10\n",
            "1172/1172 [==============================] - 3s 3ms/step - loss: 0.4301 - val_loss: 0.4296\n",
            "Epoch 4/10\n",
            "1172/1172 [==============================] - 3s 3ms/step - loss: 0.4105 - val_loss: 0.4244\n",
            "Epoch 5/10\n",
            "1172/1172 [==============================] - 3s 3ms/step - loss: 0.4030 - val_loss: 0.4186\n",
            "Epoch 6/10\n",
            "1172/1172 [==============================] - 3s 3ms/step - loss: 0.3997 - val_loss: 0.4165\n",
            "Epoch 7/10\n",
            "1172/1172 [==============================] - 3s 3ms/step - loss: 0.3985 - val_loss: 0.4126\n",
            "Epoch 8/10\n",
            "1172/1172 [==============================] - 3s 3ms/step - loss: 0.3971 - val_loss: 0.4137\n",
            "Epoch 9/10\n",
            "1172/1172 [==============================] - 4s 3ms/step - loss: 0.3943 - val_loss: 0.4131\n",
            "Epoch 10/10\n",
            "1172/1172 [==============================] - 4s 3ms/step - loss: 0.3948 - val_loss: 0.4122\n",
            "StackedAutoencoder: autoencoder 0 - model history\n",
            "{'loss': [1.098940134048462, 0.3495142459869385, 0.3192394971847534, 0.3099696934223175, 0.30483970046043396, 0.30125680565834045, 0.29958590865135193, 0.29805782437324524, 0.2973875105381012, 0.29622095823287964], 'val_loss': [0.41573214530944824, 0.35245463252067566, 0.332292377948761, 0.33008167147636414, 0.32329636812210083, 0.3239075243473053, 0.32256826758384705, 0.3229196071624756, 0.3205215632915497, 0.31998950242996216]}\n",
            "StackedAutoencoder: autoencoder 1 - model history\n",
            "{'loss': [1.3012717962265015, 0.4494539499282837, 0.3319479525089264, 0.30818280577659607, 0.29906317591667175, 0.29495829343795776, 0.2923027276992798, 0.29069894552230835, 0.289929062128067, 0.2889997661113739], 'val_loss': [0.6381542682647705, 0.3750901520252228, 0.3304513692855835, 0.316776841878891, 0.3136577308177948, 0.3101053237915039, 0.307822048664093, 0.30750730633735657, 0.3066195845603943, 0.30433300137519836]}\n",
            "StackedAutoencoder: autoencoder 2 - model history\n",
            "{'loss': [0.9867383241653442, 0.4843645691871643, 0.4236336052417755, 0.408478707075119, 0.4024621844291687, 0.39929407835006714, 0.3974442183971405, 0.3962603807449341, 0.3955635130405426, 0.3950151801109314], 'val_loss': [0.6007615327835083, 0.45388180017471313, 0.4295690357685089, 0.42442554235458374, 0.4185788333415985, 0.41651543974876404, 0.4125533998012543, 0.41372278332710266, 0.413053959608078, 0.4122268259525299]}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.callbacks.History at 0x7f9ca289c1d0>,\n",
              " <keras.callbacks.History at 0x7f9cda61c3d0>,\n",
              " <keras.callbacks.History at 0x7f9cb181bc10>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLHJqDcGzo1D",
        "outputId": "d78b73df-7a68-4767-b87d-be74cf7216ec"
      },
      "source": [
        "mse(sa.decode(sa.encode(train_data[:75000, :400])), train_data[:75000, :400])  # (с просто автоенкодером было ~1.4-1.5 to 1.6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.8263515"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElZbcvD1uTCJ",
        "outputId": "7f0a3f11-a22f-49f4-9825-23f81aa2440a"
      },
      "source": [
        "# encoded_train = sa.autoencoders[0].encoder(train_data[:75000])\n",
        "encoded_train.numpy().mean(axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 5.04771695e-02,  3.14634293e-02,  2.19576657e-02,  1.86826326e-02,\n",
              "       -3.68635952e-02,  3.10710026e-03, -1.14733852e-01, -1.01467259e-01,\n",
              "       -7.76787251e-02,  5.78321740e-02,  4.37030904e-02, -3.19610769e-03,\n",
              "       -2.03715619e-02,  4.61229496e-02,  8.78743827e-01, -6.52614562e-03,\n",
              "        7.34758303e-02,  1.50829917e-02, -2.66951807e-02, -5.61798774e-02,\n",
              "       -2.23516803e-02, -1.61042418e-02, -2.35737916e-02,  8.53385866e-01,\n",
              "        1.00000000e+00, -6.39373728e-04,  1.37768835e-02,  1.27317756e-01,\n",
              "        6.95799419e-04,  1.40190134e-02,  9.09221351e-01, -1.68406665e-02,\n",
              "       -1.14607541e-02, -1.70303807e-01,  5.98439015e-02, -3.81856821e-02,\n",
              "       -1.27365720e-02, -5.43667786e-02, -1.19479848e-02,  1.49504831e-02,\n",
              "       -9.96744692e-01,  2.75003910e-02,  1.05099492e-02,  2.05947980e-02,\n",
              "       -7.04606771e-02, -4.45290748e-03, -8.96126851e-02,  6.31489754e-02,\n",
              "       -7.32826348e-03, -1.07630966e-02,  9.61513519e-01, -5.71018048e-02,\n",
              "       -7.12499619e-02,  7.38909617e-02,  4.22655158e-02, -4.15058024e-02,\n",
              "       -9.95485008e-01,  5.47062568e-02,  2.92611886e-02, -1.26438551e-02,\n",
              "       -1.25260770e-01,  4.82797176e-02, -2.64380984e-02,  1.06005333e-02,\n",
              "        5.53733185e-02, -3.23652327e-02,  9.01002139e-02, -4.93680350e-02,\n",
              "       -5.31298034e-02, -5.25018722e-02, -9.59130563e-03,  2.79089026e-02,\n",
              "       -1.73351727e-03,  2.63134167e-02, -7.08206231e-03,  9.01790336e-02,\n",
              "       -9.74522680e-02, -7.00353831e-02, -2.20041480e-02,  5.65803722e-02,\n",
              "        9.96691465e-01, -9.68393326e-01,  1.56771950e-02, -4.36227061e-02,\n",
              "        4.26695980e-02, -5.89283742e-02, -8.26903209e-02, -9.87707496e-01,\n",
              "       -6.14926219e-02,  3.88836302e-03,  6.48563132e-02, -8.51889104e-02,\n",
              "       -6.03888743e-03, -9.94696021e-01, -2.66485345e-02, -9.99996781e-01,\n",
              "        1.01916790e-02, -1.00468308e-01, -2.06469446e-02,  4.51464355e-02,\n",
              "        4.54053730e-02, -3.84191424e-02,  6.91722408e-02,  8.17632452e-02,\n",
              "        5.23674749e-02,  9.95155513e-01,  6.99501038e-02,  8.41158722e-03,\n",
              "       -1.64723381e-01, -2.89253742e-02, -5.40957078e-02,  7.57891983e-02,\n",
              "        7.90625811e-02,  4.52929828e-03, -1.74165666e-02,  8.22411850e-02,\n",
              "       -2.77281477e-04, -9.96173561e-01,  2.36110855e-02,  2.14202944e-02,\n",
              "        4.76146564e-02, -6.40163049e-02, -1.73012260e-02,  1.13065511e-01,\n",
              "       -2.58143302e-02, -8.01819786e-02,  1.78001300e-02,  2.13208664e-02,\n",
              "       -6.50143102e-02,  3.61523665e-02, -7.44638145e-02, -6.55959696e-02,\n",
              "        7.88557902e-03, -1.86750926e-02, -3.07261888e-02,  1.85720883e-02,\n",
              "       -9.50040400e-01,  1.50422901e-02,  1.13021053e-01,  2.43708249e-02,\n",
              "        5.66762835e-02,  1.59433652e-02, -6.29698159e-04,  9.95550811e-01,\n",
              "       -3.72212473e-03,  4.96273860e-04,  6.19160086e-02, -5.01557328e-02,\n",
              "        1.70490921e-01,  5.74029349e-02,  9.93473351e-01, -8.83133650e-01,\n",
              "        5.88630326e-02,  7.74150155e-03,  9.53514874e-02,  5.39223813e-02,\n",
              "        2.42789462e-02,  3.93253155e-02,  1.43994438e-02, -2.95868758e-02,\n",
              "        8.34253505e-02,  7.60278665e-03,  8.89497101e-02, -7.08214939e-02,\n",
              "        4.29822542e-02,  2.67900396e-02,  6.07970580e-02, -1.01881891e-01,\n",
              "       -4.93455045e-02, -1.08692711e-02, -1.44172823e-02, -5.77698238e-02,\n",
              "       -1.39875859e-02,  2.26804428e-02, -7.23486673e-03, -5.81589178e-04,\n",
              "        3.28040458e-02, -6.77214116e-02,  1.95479438e-01,  7.86483809e-02,\n",
              "        1.41595164e-02,  2.36201342e-02,  3.09798084e-02,  2.39991881e-02,\n",
              "       -2.52371151e-02,  8.02331232e-03,  3.24191153e-02,  5.70730343e-02,\n",
              "        3.11137340e-03, -3.18171568e-02,  5.19063622e-02,  2.41981540e-02,\n",
              "        1.29359439e-01,  4.05607093e-03, -5.80827817e-02, -6.23362488e-04,\n",
              "        4.32089120e-02,  2.77982093e-02,  5.50489724e-02, -1.43563300e-01,\n",
              "       -7.42783025e-03,  1.00000000e+00, -5.42111509e-02, -7.84018356e-03,\n",
              "       -9.96113002e-01, -1.27695256e-03,  6.52417019e-02,  2.43885163e-02,\n",
              "        1.11550167e-01,  1.37632824e-02, -7.38196522e-02, -9.96096015e-01,\n",
              "        2.01651789e-02,  5.75168803e-02, -7.77588785e-02,  7.88737983e-02,\n",
              "       -2.27194596e-02, -8.82634744e-02, -5.95557550e-03, -3.15634944e-02,\n",
              "       -3.11600808e-02,  5.36718057e-04,  5.68943433e-02,  5.89039773e-02,\n",
              "        9.90378261e-02, -2.36029383e-02, -5.12976870e-02,  8.78416821e-02,\n",
              "        2.90251616e-03, -1.73793361e-02, -6.08363032e-01,  2.95171645e-02,\n",
              "       -5.46488203e-02, -4.77641309e-03, -9.65963155e-02, -3.21087763e-02,\n",
              "       -1.28522934e-02, -5.90995848e-02, -1.93577092e-02,  5.01603857e-02,\n",
              "        2.49927212e-02, -2.52027065e-03, -5.03460206e-02, -9.93580185e-03,\n",
              "        1.49773387e-02,  6.96442649e-02, -9.03739333e-02, -7.52854198e-02,\n",
              "        9.99942303e-01, -6.13409653e-02,  4.19547632e-02,  1.81055870e-02,\n",
              "       -4.86469865e-02,  1.22839166e-02, -2.32173931e-02,  3.10004950e-02,\n",
              "       -1.80488278e-03, -4.91846502e-02, -9.48086143e-01, -3.88568826e-02,\n",
              "       -3.47666107e-02,  5.42070232e-02,  1.13625685e-02, -4.97986451e-02,\n",
              "        1.03915166e-02, -1.24574348e-01,  4.00625318e-02, -9.85633850e-01,\n",
              "       -2.53396053e-02, -1.41883744e-02,  2.20132656e-02,  6.14737310e-02,\n",
              "        2.77948696e-02, -9.39761996e-02,  1.97799616e-02,  5.76217137e-02,\n",
              "       -4.01084349e-02,  6.39439598e-02,  1.88266691e-02,  1.76997446e-02,\n",
              "        8.63619968e-02, -9.03589189e-01,  1.45944403e-02,  1.18686706e-01,\n",
              "        1.33660994e-02, -2.41874177e-02, -8.10131803e-02, -1.42087797e-02,\n",
              "        6.73127472e-02,  4.78832237e-02,  4.05564308e-02, -8.51252303e-02,\n",
              "        3.30619951e-04, -5.50159439e-02, -9.99884784e-01,  9.99553978e-01,\n",
              "       -1.97238345e-02, -6.61647692e-02, -7.26771951e-02,  7.99419582e-02,\n",
              "        9.99893010e-01, -5.81711121e-02, -1.49318520e-02, -1.16551956e-02,\n",
              "       -4.59310412e-02,  7.18682781e-02, -1.02026395e-01, -4.57380787e-02,\n",
              "       -9.40683126e-01,  9.54660296e-01,  5.56127727e-02, -9.95683014e-01,\n",
              "        4.44594771e-02, -1.07128307e-01,  5.34231029e-02,  3.00023630e-02,\n",
              "       -2.51094606e-02,  3.68174841e-03, -4.33173999e-02,  7.72181153e-02,\n",
              "       -2.16219071e-02,  5.79130873e-02,  6.58886798e-04, -3.32935601e-02,\n",
              "        3.80858853e-02, -2.32101064e-02, -2.96463314e-02,  1.60285253e-02,\n",
              "       -5.04149422e-02,  1.28773525e-02, -3.92735414e-02,  6.96375743e-02,\n",
              "       -3.23963426e-02,  5.06023578e-02, -2.32723188e-02,  5.36233839e-03,\n",
              "        4.86354865e-02, -2.66275480e-02, -2.10482292e-02, -2.05351170e-02,\n",
              "       -1.68998130e-02,  5.13071492e-02, -1.09663688e-01, -8.10084939e-02,\n",
              "       -7.31077716e-02, -6.42724661e-03, -1.11532591e-01, -9.23547074e-02,\n",
              "       -4.24768366e-02,  1.94723401e-02, -5.61340619e-03, -2.09157560e-02,\n",
              "        3.70925441e-02, -7.62514621e-02,  9.95957792e-01,  6.91956505e-02,\n",
              "        1.04108267e-01,  3.82289663e-02, -1.25459537e-01, -8.67112204e-02,\n",
              "       -2.23208964e-02, -5.35157211e-02,  9.68964323e-02,  5.92446188e-03,\n",
              "        1.82952117e-02,  3.84121872e-02,  2.67668795e-02,  3.37130353e-02,\n",
              "       -6.17904495e-03, -4.14505713e-02, -2.18470395e-02,  8.46744627e-02,\n",
              "       -1.08868601e-02, -2.63937446e-03,  8.06161389e-03, -4.53696400e-02,\n",
              "       -9.99749184e-01,  3.79456393e-02, -6.53520972e-02,  6.67077973e-02,\n",
              "       -6.84522316e-02, -8.55975449e-02, -5.21629117e-03, -4.11775634e-02,\n",
              "       -9.15319979e-01,  1.60766710e-02, -1.48482576e-01, -1.89525113e-02,\n",
              "       -9.16952491e-01,  5.05143739e-02,  5.15049919e-02, -1.18965320e-02,\n",
              "        1.30654164e-02, -9.50498879e-01, -9.96050715e-01,  7.66643733e-02,\n",
              "        3.36173810e-02, -5.96137233e-02,  9.98544276e-01, -4.27640341e-02,\n",
              "       -9.96899903e-01,  7.02246130e-02,  2.38176435e-02,  4.09970107e-03,\n",
              "       -4.39429656e-02, -9.71055124e-03, -1.25047881e-02, -2.46645063e-02,\n",
              "       -7.07492828e-02,  8.56958702e-03, -1.00844786e-01,  2.49230899e-02,\n",
              "       -1.59444381e-02, -3.18103544e-02, -4.91910204e-02,  8.40221569e-02,\n",
              "        6.89929724e-02,  5.93968481e-02,  4.99910451e-02,  1.11138791e-01,\n",
              "        4.61799317e-07, -5.61626889e-02,  9.40123387e-03,  2.96859965e-02,\n",
              "        5.93323186e-02,  6.31630700e-03,  5.53561628e-01,  1.41371891e-01,\n",
              "       -3.75596583e-02,  1.71098940e-03, -8.72453824e-02, -7.58408755e-03,\n",
              "       -4.52182628e-02,  8.95220786e-02,  8.94850399e-03, -9.43509676e-03,\n",
              "        3.37581784e-02,  3.13656181e-02,  2.69632600e-02,  5.84185794e-02,\n",
              "       -8.04248303e-02,  1.16640300e-01,  3.86039764e-02, -3.66097800e-02,\n",
              "       -2.39586476e-02, -3.45493965e-02,  2.52475683e-02,  4.07917798e-02,\n",
              "        3.02746780e-02,  2.75415629e-02, -1.42392322e-01, -8.28271136e-02,\n",
              "        1.08633652e-01,  2.65462231e-02,  1.15201809e-02,  7.49551207e-02,\n",
              "        9.42752603e-03, -2.49937959e-02,  3.85487601e-02,  4.88778092e-02,\n",
              "       -7.09207030e-03, -2.33527776e-02, -2.58046500e-02,  2.97315437e-02,\n",
              "        6.67503923e-02, -6.27261819e-03, -1.98246888e-03, -4.54084091e-02,\n",
              "        1.14810755e-02, -1.92269627e-02, -2.99482290e-02, -1.01404838e-01,\n",
              "       -9.87018123e-02, -5.03242873e-02, -8.80907327e-02,  1.65822674e-02,\n",
              "        1.52548522e-01, -2.45994441e-02, -6.94657341e-02,  3.29569317e-02,\n",
              "        5.69519401e-02,  1.58181936e-02, -4.58939606e-03,  1.54150277e-01,\n",
              "        2.85329204e-02,  8.46260190e-02, -4.29903902e-02, -6.50226399e-02,\n",
              "       -3.48248929e-02, -9.99455333e-01,  7.16256164e-03,  9.84483119e-03,\n",
              "       -1.41314669e-02,  2.45530773e-02,  2.70316228e-02, -2.81597171e-02,\n",
              "        2.41010226e-02,  2.54866257e-02,  5.06290048e-02, -2.32706275e-02,\n",
              "       -7.88612105e-03,  6.13495298e-02, -2.66160574e-02,  1.09457867e-02,\n",
              "        3.99409095e-03,  1.46120656e-02, -3.40608396e-02,  2.88144816e-02,\n",
              "        5.72353639e-02, -2.27150451e-02,  3.06453388e-02, -4.64741364e-02,\n",
              "       -7.56977797e-02,  1.78660844e-02,  5.95328249e-02,  1.72200739e-01,\n",
              "        6.92773191e-03,  1.52154965e-02,  7.29177799e-03, -8.70996621e-03,\n",
              "        1.08277621e-02, -4.92887534e-02, -2.62154695e-02, -6.34997040e-02,\n",
              "        1.26675600e-02, -1.36414692e-02,  1.04773290e-01,  1.22531533e-01,\n",
              "       -1.45328958e-02, -7.84061942e-03,  5.04389144e-02, -1.08337374e-02,\n",
              "       -8.56274739e-02,  1.61840990e-02, -3.16077098e-02,  5.08076251e-02,\n",
              "        2.76168846e-02, -9.99364138e-01,  1.39983837e-02,  3.78607698e-02,\n",
              "       -4.21199501e-02, -1.30019844e-01,  4.73116748e-02, -2.32269820e-02,\n",
              "       -6.58139260e-03, -1.33486539e-01,  3.05428673e-02,  1.37865290e-01,\n",
              "        1.81902703e-02,  3.90058681e-02, -2.49757729e-02, -9.97665644e-01,\n",
              "       -6.13422785e-03,  1.38446972e-01, -1.09407781e-02, -8.04662034e-02,\n",
              "       -2.66987849e-02, -1.06863752e-02,  2.02011988e-02, -6.41569495e-02,\n",
              "        1.64114591e-02,  2.00059060e-02, -9.99946058e-01, -2.07154155e-02,\n",
              "       -6.39328808e-02,  4.75851074e-02, -4.42348346e-02,  1.81346461e-02,\n",
              "       -4.81280424e-02, -1.15268759e-01,  3.38889062e-02, -3.01688015e-02,\n",
              "       -7.87173118e-03, -5.47965690e-02,  7.03059509e-03, -4.41390686e-02,\n",
              "       -8.55057407e-03,  2.13456210e-02,  1.61859803e-02,  3.94968167e-02,\n",
              "       -1.42682912e-02, -8.21211413e-02,  6.08335920e-02,  5.54927737e-02,\n",
              "        2.21190546e-02,  2.46353522e-02,  4.92516346e-02,  3.76837738e-02,\n",
              "        1.95374135e-02,  8.28766543e-03, -5.21956980e-02,  4.90132533e-02,\n",
              "       -3.01502179e-02, -3.25916074e-02, -7.68968090e-03, -1.45756574e-02,\n",
              "        1.04601691e-02,  4.91392147e-03,  7.53781050e-02,  8.24403297e-03,\n",
              "       -2.67712381e-02,  3.50246057e-02,  8.10564496e-03, -4.36940156e-02,\n",
              "        2.30424348e-02,  3.37122604e-02, -4.22440320e-02, -1.70239061e-02,\n",
              "        5.97397313e-02, -2.17295892e-04,  3.00637726e-02,  3.15601751e-02,\n",
              "       -4.47779447e-02, -4.78320196e-02,  1.69089690e-01,  8.77656043e-01,\n",
              "        3.99756171e-02, -2.20332090e-02, -5.34604117e-02,  1.55154886e-02,\n",
              "        1.06305042e-02,  6.02296740e-03, -2.60696858e-02,  6.49887025e-02,\n",
              "       -5.32840379e-02,  9.95747924e-01, -3.28168273e-02, -9.99914765e-01,\n",
              "        4.06694449e-02, -7.98613392e-03,  9.97602403e-01, -2.29381565e-02,\n",
              "       -3.78766842e-02,  5.79560399e-02, -1.47324130e-01,  3.78316343e-02,\n",
              "       -2.55213007e-02, -3.62637118e-02,  4.05690372e-02,  3.89763415e-02,\n",
              "        3.88422608e-02, -2.35522371e-02, -2.15404928e-02,  5.17971329e-02,\n",
              "       -4.63936217e-02, -9.70536619e-02, -2.17470952e-04, -1.65801924e-02,\n",
              "        5.62371500e-02,  9.43302438e-02, -9.65141356e-02, -1.86624274e-01,\n",
              "        6.11303598e-02, -3.97125306e-03,  1.76988281e-02,  2.55299732e-02,\n",
              "        8.74808505e-02,  4.87401383e-03, -1.50379268e-02,  3.74400765e-02,\n",
              "       -8.26747564e-04, -2.81660003e-03,  8.38113725e-02, -4.00413126e-02,\n",
              "       -2.49854159e-02,  7.75016397e-02,  1.82476267e-02, -2.77862996e-02,\n",
              "       -1.46681722e-02,  7.60125294e-02, -2.12984383e-02,  5.65774506e-03,\n",
              "        4.51017097e-02,  7.75433751e-03,  9.96208251e-01, -2.09423564e-02,\n",
              "       -2.86351591e-02, -9.95684147e-01,  8.83700773e-02, -4.35471386e-02,\n",
              "        4.19709943e-02,  3.31333168e-02, -3.86077873e-02, -7.33920326e-03,\n",
              "       -1.03033796e-01, -1.46728665e-01, -2.84220632e-02,  2.39882413e-02,\n",
              "        9.20656323e-02, -1.06858239e-01, -8.79111290e-02, -5.74864373e-02,\n",
              "       -4.23733890e-02,  7.04606026e-02, -5.91624528e-02, -3.40128504e-02,\n",
              "       -6.24783151e-02, -2.58548968e-02, -9.02963579e-02,  5.86990872e-03,\n",
              "       -1.87397208e-02,  7.33659565e-02,  1.54703021e-01,  1.52500877e-02,\n",
              "       -9.99594569e-01,  9.24578309e-02,  3.18414308e-02, -8.21421389e-03,\n",
              "       -8.75691473e-02, -3.36041674e-03,  7.47545883e-02, -5.32659180e-02,\n",
              "        7.02065369e-03, -4.01184522e-02, -6.17337711e-02, -9.99955416e-01,\n",
              "        4.03626077e-03, -5.29472046e-02,  4.09173258e-02,  5.09110689e-02,\n",
              "       -8.46892521e-02,  6.14944787e-04, -7.52738565e-02, -4.60324399e-02,\n",
              "        2.19619405e-02,  2.00695246e-02, -7.06599876e-02,  9.19052884e-02,\n",
              "        2.77665388e-02,  6.15858519e-03,  1.74094327e-02,  4.10242043e-02,\n",
              "        9.55099426e-03,  5.23828454e-02, -4.06888910e-02, -4.20940705e-02,\n",
              "        1.81802586e-02,  4.15765457e-02,  1.09493891e-02,  8.47990531e-03,\n",
              "        3.46173830e-02,  8.09931010e-02, -4.00135946e-03, -4.33035055e-03,\n",
              "        9.98275340e-01,  1.64474584e-02, -3.09447553e-02,  1.04640173e-02,\n",
              "       -2.90954206e-03, -4.52599162e-03, -7.82649368e-02, -6.64144307e-02,\n",
              "       -4.97498363e-02,  1.61248576e-02,  8.79236497e-03, -3.85479443e-02,\n",
              "        1.14902761e-02, -1.67571511e-02,  6.03784561e-01,  2.34051533e-02,\n",
              "        1.63293835e-02, -9.99268115e-01,  2.89360229e-02, -5.10534719e-02,\n",
              "        4.10089642e-02,  9.99190390e-01, -1.76804885e-02, -8.17499124e-03],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "id": "aoYcGWxdhCgp",
        "outputId": "8be80930-9f6b-408d-8678-32125f3e5978"
      },
      "source": [
        "# autoencoder.compile('adam', 'mse')\n",
        "autoencoder.fit(train_data, train_data, batch_size=64, epochs=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1307/1307 [==============================] - 9s 7ms/step - loss: 1.3027\n",
            "Epoch 2/20\n",
            "1307/1307 [==============================] - 8s 6ms/step - loss: 1.3019\n",
            "Epoch 3/20\n",
            "1307/1307 [==============================] - 7s 6ms/step - loss: 1.2995\n",
            "Epoch 4/20\n",
            "1307/1307 [==============================] - 7s 6ms/step - loss: 1.2993\n",
            "Epoch 5/20\n",
            "1307/1307 [==============================] - 7s 6ms/step - loss: 1.2964\n",
            "Epoch 6/20\n",
            "1307/1307 [==============================] - 8s 6ms/step - loss: 1.2961\n",
            "Epoch 7/20\n",
            "1307/1307 [==============================] - 7s 6ms/step - loss: 1.2962\n",
            "Epoch 8/20\n",
            "1307/1307 [==============================] - 7s 6ms/step - loss: 1.2947\n",
            "Epoch 9/20\n",
            "1307/1307 [==============================] - 7s 6ms/step - loss: 1.2929\n",
            "Epoch 10/20\n",
            " 766/1307 [================>.............] - ETA: 3s - loss: 1.2935"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-d69c338a979b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# autoencoder.compile('adam', 'mse')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 _r=1):\n\u001b[1;32m   1157\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZOGdWXf2C2S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iVf0NfN2FCT"
      },
      "source": [
        "### сравниваю с PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW93wwbwiWR4",
        "outputId": "8b115d08-8164-460f-c73f-f9cb3eca2885"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=100)\n",
        "train_data_pca = pca.fit_transform(train_data[:75000, :400])\n",
        "\n",
        "train_data_pca_reconstructed = pca.inverse_transform(train_data_pca)\n",
        "mse(train_data[:75000, :400], train_data_pca_reconstructed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.4661937"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPk5B43VB4Fu",
        "outputId": "94395c43-e923-4b68-9a2c-b098d7af2504"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=100)\n",
        "train_data_pca = pca.fit_transform(train_data)\n",
        "\n",
        "train_data_pca_reconstructed = pca.inverse_transform(train_data_pca)\n",
        "mse(train_data, train_data_pca_reconstructed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.7946674"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjoKKTeaQjud"
      },
      "source": [
        "### Автоенкодер на вокабуляре без контекста"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KngVWIe0QnF1"
      },
      "source": [
        "bert_embeddings = np.load('drive/MyDrive/Coursework2021/bert_base_uncased_vocab_embeddings.npy')\n",
        "with open('drive/MyDrive/Coursework2021/bert_base_uncased_vocab.txt', 'r') as f:\n",
        "    bert_tokens = f.read().split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3odknCDRQ0Gv",
        "outputId": "7d9a5924-eecb-49a5-f1e0-c28bd8c8c689"
      },
      "source": [
        "bert_embeddings.shape, len(bert_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((30522, 768), 30523)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT91MkWwWlUC"
      },
      "source": [
        "#### Анализ сходимости"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRscCDTVQ6ZE",
        "outputId": "e85c6196-2d33-475b-ee5f-398fb07ccc64"
      },
      "source": [
        "sae = StackedAutoencoder([768, 64])\n",
        "sae.fit(bert_embeddings, bert_embeddings, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "StackAutoencoder: fitting 1 autoencoders with dimensions [768, 64]\n",
            "\tfitting autoencoder number 0 with dimensions (768, 64)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "477/477 [==============================] - 3s 5ms/step - loss: 2.9718 - val_loss: 1.5777\n",
            "Epoch 2/10\n",
            "477/477 [==============================] - 3s 6ms/step - loss: 1.4985 - val_loss: 1.3510\n",
            "Epoch 3/10\n",
            "477/477 [==============================] - 2s 4ms/step - loss: 1.3186 - val_loss: 1.2486\n",
            "Epoch 4/10\n",
            "477/477 [==============================] - 3s 6ms/step - loss: 1.2295 - val_loss: 1.1792\n",
            "Epoch 5/10\n",
            "477/477 [==============================] - 2s 5ms/step - loss: 1.1671 - val_loss: 1.1231\n",
            "Epoch 6/10\n",
            "477/477 [==============================] - 3s 6ms/step - loss: 1.1126 - val_loss: 1.0808\n",
            "Epoch 7/10\n",
            "477/477 [==============================] - 3s 6ms/step - loss: 1.0784 - val_loss: 1.0505\n",
            "Epoch 8/10\n",
            "477/477 [==============================] - 2s 5ms/step - loss: 1.0474 - val_loss: 1.0249\n",
            "Epoch 9/10\n",
            "477/477 [==============================] - 3s 6ms/step - loss: 1.0136 - val_loss: 1.0052\n",
            "Epoch 10/10\n",
            "477/477 [==============================] - 2s 4ms/step - loss: 0.9941 - val_loss: 0.9843\n",
            "StackedAutoencoder: autoencoder 0 - model history\n",
            "{'loss': [2.214223861694336, 1.4449907541275024, 1.2987651824951172, 1.2138054370880127, 1.1518361568450928, 1.1048563718795776, 1.0673623085021973, 1.039108157157898, 1.0149952173233032, 0.9948530793190002], 'val_loss': [1.577706217765808, 1.3509759902954102, 1.248580813407898, 1.1792117357254028, 1.1231237649917603, 1.08076810836792, 1.0505017042160034, 1.0249497890472412, 1.0051982402801514, 0.9842550754547119]}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.callbacks.History at 0x7efb7f0ad810>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4woRIFOvsnh",
        "outputId": "fa863b03-5f53-4962-9ac3-7543d35b37b5"
      },
      "source": [
        "# full reconstruction\n",
        "\n",
        "sae = StackedAutoencoder([768, 768])\n",
        "sae.fit(bert_embeddings[:, :768], bert_embeddings[:, :768], verbose=True, epochs=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1202 - val_loss: 0.1131\n",
            "Epoch 15/50\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.1180 - val_loss: 0.1129\n",
            "Epoch 16/50\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1133 - val_loss: 0.1104\n",
            "Epoch 17/50\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1113 - val_loss: 0.1173\n",
            "Epoch 18/50\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.1093 - val_loss: 0.1039\n",
            "Epoch 19/50\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.1074 - val_loss: 0.1192\n",
            "Epoch 20/50\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1077 - val_loss: 0.1083\n",
            "Epoch 21/50\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.1036 - val_loss: 0.0975\n",
            "Epoch 22/50\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.1016 - val_loss: 0.0997\n",
            "Epoch 23/50\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.1025 - val_loss: 0.1040\n",
            "Epoch 24/50\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.1032 - val_loss: 0.0936\n",
            "Epoch 25/50\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0997 - val_loss: 0.0981\n",
            "Epoch 26/50\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1010 - val_loss: 0.0987\n",
            "Epoch 27/50\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0974 - val_loss: 0.0942\n",
            "Epoch 28/50\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0966 - val_loss: 0.0929\n",
            "Epoch 29/50\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0990 - val_loss: 0.0974\n",
            "Epoch 30/50\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0983 - val_loss: 0.0915\n",
            "Epoch 31/50\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0954 - val_loss: 0.0913\n",
            "Epoch 32/50\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0956 - val_loss: 0.1005\n",
            "Epoch 33/50\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0931 - val_loss: 0.0921\n",
            "Epoch 34/50\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0950 - val_loss: 0.0900\n",
            "Epoch 35/50\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0938 - val_loss: 0.0918\n",
            "Epoch 36/50\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0946 - val_loss: 0.0891\n",
            "Epoch 37/50\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0934 - val_loss: 0.0860\n",
            "Epoch 38/50\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0919 - val_loss: 0.0929\n",
            "Epoch 39/50\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0955 - val_loss: 0.0907\n",
            "Epoch 40/50\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0908 - val_loss: 0.1004\n",
            "Epoch 41/50\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0935 - val_loss: 0.1004\n",
            "Epoch 42/50\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0936 - val_loss: 0.0955\n",
            "Epoch 43/50\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0909 - val_loss: 0.1027\n",
            "Epoch 44/50\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0921 - val_loss: 0.0860\n",
            "Epoch 45/50\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0917 - val_loss: 0.0848\n",
            "Epoch 46/50\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0927 - val_loss: 0.0888\n",
            "Epoch 47/50\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0892 - val_loss: 0.0898\n",
            "Epoch 48/50\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0910 - val_loss: 0.0828\n",
            "Epoch 49/50\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0893 - val_loss: 0.0917\n",
            "Epoch 50/50\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0899 - val_loss: 0.0956\n",
            "StackedAutoencoder: autoencoder 0 - model history\n",
            "{'loss': [0.9499152898788452, 0.4237971901893616, 0.3114783465862274, 0.25465285778045654, 0.21648752689361572, 0.19169765710830688, 0.17353299260139465, 0.1608220934867859, 0.1493903398513794, 0.1409216672182083, 0.13474182784557343, 0.13069963455200195, 0.12410818785429001, 0.11980908364057541, 0.11835259199142456, 0.11362095922231674, 0.11253956705331802, 0.10969290882349014, 0.10874446481466293, 0.10800474882125854, 0.10426269471645355, 0.10385330766439438, 0.10265348106622696, 0.10312626510858536, 0.09987787157297134, 0.10235755145549774, 0.09874149411916733, 0.0966070145368576, 0.10036646574735641, 0.09736465662717819, 0.09605854749679565, 0.09697102010250092, 0.09471002966165543, 0.09512756019830704, 0.09515246748924255, 0.09418687969446182, 0.09418537467718124, 0.09330452978610992, 0.09385894238948822, 0.09292928129434586, 0.09319419413805008, 0.09406817704439163, 0.09218675643205643, 0.09243855625391006, 0.09199105948209763, 0.09304158389568329, 0.09053843468427658, 0.09083274006843567, 0.09154053032398224, 0.09089293330907822], 'val_loss': [0.5179778337478638, 0.3429090976715088, 0.2757438123226166, 0.22794799506664276, 0.19650022685527802, 0.17810991406440735, 0.16241560876369476, 0.14867697656154633, 0.13900570571422577, 0.1342698633670807, 0.12704826891422272, 0.12667715549468994, 0.12346088886260986, 0.11307676136493683, 0.11287649720907211, 0.11044403165578842, 0.11727086454629898, 0.10385242849588394, 0.11920018494129181, 0.10832900553941727, 0.09746567904949188, 0.09969829022884369, 0.10404022783041, 0.09361252933740616, 0.09806953370571136, 0.09868845343589783, 0.09415890276432037, 0.09292957931756973, 0.09739094227552414, 0.09148530662059784, 0.09128383547067642, 0.10049202293157578, 0.09211070090532303, 0.08998873084783554, 0.09182511270046234, 0.08909076452255249, 0.08601953089237213, 0.09287256002426147, 0.09072839468717575, 0.10042161494493484, 0.10035848617553711, 0.0954669639468193, 0.10273190587759018, 0.08596906810998917, 0.0847824290394783, 0.08878273516893387, 0.08984042704105377, 0.0828445628285408, 0.09169194847345352, 0.09559248387813568]}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.callbacks.History at 0x7efb7f1432d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C7MFkFcxIVT"
      },
      "source": [
        "# reconstruction no activation\n",
        "\n",
        "sae = StackedAutoencoder([768, 768], activation=None)\n",
        "sae.fit(bert_embeddings, bert_embeddings, verbose=True, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz_TNDclwmrI",
        "outputId": "271b4b61-baff-40bf-eb22-c9d3303f93cd"
      },
      "source": [
        "# shrinked vectors\n",
        "\n",
        "sae = StackedAutoencoder([100, 80])\n",
        "sae.fit(bert_embeddings[:, :100], bert_embeddings[:, :100], verbose=True, epochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "StackAutoencoder: fitting 1 autoencoders with dimensions [100, 80]\n",
            "\tfitting autoencoder number 0 with dimensions (100, 80)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "477/477 [==============================] - 2s 3ms/step - loss: 2.2031 - val_loss: 0.8363\n",
            "Epoch 2/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.7280 - val_loss: 0.4948\n",
            "Epoch 3/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.4485 - val_loss: 0.3407\n",
            "Epoch 4/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.3183 - val_loss: 0.2613\n",
            "Epoch 5/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.2507 - val_loss: 0.2172\n",
            "Epoch 6/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.2096 - val_loss: 0.1858\n",
            "Epoch 7/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1817 - val_loss: 0.1670\n",
            "Epoch 8/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1648 - val_loss: 0.1547\n",
            "Epoch 9/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1541 - val_loss: 0.1464\n",
            "Epoch 10/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1443 - val_loss: 0.1381\n",
            "Epoch 11/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1381 - val_loss: 0.1345\n",
            "Epoch 12/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1324 - val_loss: 0.1286\n",
            "Epoch 13/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1286 - val_loss: 0.1263\n",
            "Epoch 14/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1252 - val_loss: 0.1233\n",
            "Epoch 15/100\n",
            "477/477 [==============================] - 1s 2ms/step - loss: 0.1226 - val_loss: 0.1223\n",
            "Epoch 16/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1217 - val_loss: 0.1201\n",
            "Epoch 17/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1198 - val_loss: 0.1186\n",
            "Epoch 18/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1189 - val_loss: 0.1168\n",
            "Epoch 19/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1175 - val_loss: 0.1160\n",
            "Epoch 20/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1161 - val_loss: 0.1155\n",
            "Epoch 21/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1159 - val_loss: 0.1127\n",
            "Epoch 22/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1130 - val_loss: 0.1115\n",
            "Epoch 23/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1122 - val_loss: 0.1101\n",
            "Epoch 24/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1111 - val_loss: 0.1095\n",
            "Epoch 25/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1105 - val_loss: 0.1087\n",
            "Epoch 26/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1088 - val_loss: 0.1098\n",
            "Epoch 27/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1089 - val_loss: 0.1074\n",
            "Epoch 28/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1087 - val_loss: 0.1082\n",
            "Epoch 29/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1081 - val_loss: 0.1086\n",
            "Epoch 30/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1075 - val_loss: 0.1064\n",
            "Epoch 31/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1075 - val_loss: 0.1064\n",
            "Epoch 32/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1076 - val_loss: 0.1087\n",
            "Epoch 33/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1067 - val_loss: 0.1078\n",
            "Epoch 34/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1057 - val_loss: 0.1075\n",
            "Epoch 35/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1060 - val_loss: 0.1050\n",
            "Epoch 36/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1060 - val_loss: 0.1048\n",
            "Epoch 37/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1056 - val_loss: 0.1085\n",
            "Epoch 38/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1072 - val_loss: 0.1055\n",
            "Epoch 39/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1057 - val_loss: 0.1114\n",
            "Epoch 40/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1059 - val_loss: 0.1050\n",
            "Epoch 41/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1050 - val_loss: 0.1053\n",
            "Epoch 42/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1058 - val_loss: 0.1042\n",
            "Epoch 43/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1055 - val_loss: 0.1046\n",
            "Epoch 44/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1048 - val_loss: 0.1044\n",
            "Epoch 45/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1057 - val_loss: 0.1052\n",
            "Epoch 46/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1054 - val_loss: 0.1044\n",
            "Epoch 47/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1041 - val_loss: 0.1039\n",
            "Epoch 48/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1049 - val_loss: 0.1043\n",
            "Epoch 49/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1046 - val_loss: 0.1061\n",
            "Epoch 50/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1047 - val_loss: 0.1044\n",
            "Epoch 51/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1043 - val_loss: 0.1041\n",
            "Epoch 52/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1045 - val_loss: 0.1039\n",
            "Epoch 53/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1040 - val_loss: 0.1061\n",
            "Epoch 54/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1049 - val_loss: 0.1051\n",
            "Epoch 55/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1046 - val_loss: 0.1038\n",
            "Epoch 56/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1038 - val_loss: 0.1045\n",
            "Epoch 57/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1046 - val_loss: 0.1045\n",
            "Epoch 58/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1046 - val_loss: 0.1036\n",
            "Epoch 59/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1037 - val_loss: 0.1039\n",
            "Epoch 60/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1044 - val_loss: 0.1034\n",
            "Epoch 61/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1052 - val_loss: 0.1034\n",
            "Epoch 62/100\n",
            "477/477 [==============================] - 1s 2ms/step - loss: 0.1039 - val_loss: 0.1045\n",
            "Epoch 63/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1056 - val_loss: 0.1033\n",
            "Epoch 64/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1041 - val_loss: 0.1032\n",
            "Epoch 65/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1037 - val_loss: 0.1049\n",
            "Epoch 66/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1047 - val_loss: 0.1031\n",
            "Epoch 67/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1039 - val_loss: 0.1068\n",
            "Epoch 68/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1047 - val_loss: 0.1031\n",
            "Epoch 69/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1033 - val_loss: 0.1041\n",
            "Epoch 70/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1039 - val_loss: 0.1028\n",
            "Epoch 71/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1039 - val_loss: 0.1038\n",
            "Epoch 72/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1042 - val_loss: 0.1035\n",
            "Epoch 73/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1045 - val_loss: 0.1032\n",
            "Epoch 74/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1034 - val_loss: 0.1031\n",
            "Epoch 75/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1035 - val_loss: 0.1011\n",
            "Epoch 76/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1017 - val_loss: 0.1010\n",
            "Epoch 77/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1009 - val_loss: 0.1008\n",
            "Epoch 78/100\n",
            "477/477 [==============================] - 1s 2ms/step - loss: 0.1008 - val_loss: 0.1024\n",
            "Epoch 79/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1006 - val_loss: 0.0996\n",
            "Epoch 80/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1002 - val_loss: 0.0996\n",
            "Epoch 81/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0997 - val_loss: 0.0983\n",
            "Epoch 82/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0997 - val_loss: 0.0999\n",
            "Epoch 83/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0989 - val_loss: 0.0988\n",
            "Epoch 84/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0995 - val_loss: 0.0981\n",
            "Epoch 85/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0988 - val_loss: 0.0992\n",
            "Epoch 86/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0990 - val_loss: 0.1003\n",
            "Epoch 87/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0992 - val_loss: 0.0990\n",
            "Epoch 88/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0984 - val_loss: 0.0983\n",
            "Epoch 89/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0994 - val_loss: 0.0983\n",
            "Epoch 90/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0981 - val_loss: 0.0989\n",
            "Epoch 91/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0993 - val_loss: 0.0974\n",
            "Epoch 92/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0989 - val_loss: 0.0982\n",
            "Epoch 93/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0987 - val_loss: 0.0975\n",
            "Epoch 94/100\n",
            "477/477 [==============================] - 2s 3ms/step - loss: 0.0982 - val_loss: 0.0980\n",
            "Epoch 95/100\n",
            "477/477 [==============================] - 2s 3ms/step - loss: 0.0982 - val_loss: 0.0978\n",
            "Epoch 96/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0977 - val_loss: 0.0978\n",
            "Epoch 97/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0982 - val_loss: 0.0973\n",
            "Epoch 98/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0977 - val_loss: 0.1008\n",
            "Epoch 99/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0977 - val_loss: 0.0975\n",
            "Epoch 100/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0986 - val_loss: 0.0982\n",
            "StackedAutoencoder: autoencoder 0 - model history\n",
            "{'loss': [1.4624005556106567, 0.6410015225410461, 0.4106590449810028, 0.29888421297073364, 0.23969806730747223, 0.2020190954208374, 0.1771657019853592, 0.16198010742664337, 0.15168435871601105, 0.1425384134054184, 0.1359683722257614, 0.13128823041915894, 0.12810267508029938, 0.12508076429367065, 0.12271042168140411, 0.12133637070655823, 0.11975004523992538, 0.1183791533112526, 0.11766016483306885, 0.1164872869849205, 0.11535343527793884, 0.11264713108539581, 0.11160027980804443, 0.11043865978717804, 0.10963394492864609, 0.1090526357293129, 0.1085389107465744, 0.10833627730607986, 0.10784342139959335, 0.10780472308397293, 0.10681360214948654, 0.10696059465408325, 0.10705062001943588, 0.10651463270187378, 0.10617834329605103, 0.10615712404251099, 0.10649881511926651, 0.1058870255947113, 0.10592386871576309, 0.10555774718523026, 0.10536167770624161, 0.10571195930242538, 0.105369433760643, 0.10521642863750458, 0.1056714877486229, 0.10513017326593399, 0.10493148863315582, 0.1048140600323677, 0.10506442189216614, 0.10508720576763153, 0.10481593012809753, 0.1048048585653305, 0.10465448349714279, 0.10486780852079391, 0.10467449575662613, 0.1043960228562355, 0.10448607802391052, 0.10458171367645264, 0.10435090959072113, 0.10438575595617294, 0.10463985055685043, 0.10441489517688751, 0.10459425300359726, 0.1040973886847496, 0.10417427867650986, 0.10434780269861221, 0.10401277244091034, 0.10426776111125946, 0.10402996838092804, 0.10422299802303314, 0.10411273688077927, 0.10404694825410843, 0.10437149554491043, 0.1040339320898056, 0.1028466746211052, 0.10150449723005295, 0.10104700177907944, 0.100741446018219, 0.1006241962313652, 0.10011104494333267, 0.09994831681251526, 0.09977488964796066, 0.09992329031229019, 0.09926018118858337, 0.09935396164655685, 0.09955724328756332, 0.09929102659225464, 0.09928258508443832, 0.09898698329925537, 0.09899198263883591, 0.09891397505998611, 0.09887877106666565, 0.09870248287916183, 0.09858880192041397, 0.09888514876365662, 0.09859167784452438, 0.09891002625226974, 0.09831834584474564, 0.09837368875741959, 0.09891557693481445], 'val_loss': [0.8363040685653687, 0.4947846531867981, 0.3407312333583832, 0.2613474428653717, 0.21723929047584534, 0.18581505119800568, 0.16704604029655457, 0.15471377968788147, 0.14643408358097076, 0.13808833062648773, 0.1345173567533493, 0.1286371946334839, 0.1262606829404831, 0.12331333011388779, 0.12231984734535217, 0.12010431289672852, 0.11855083703994751, 0.11677861213684082, 0.1160450428724289, 0.11552903801202774, 0.11266351491212845, 0.11145239323377609, 0.11012579500675201, 0.10947845876216888, 0.10868016630411148, 0.10983553528785706, 0.10738128423690796, 0.10822059959173203, 0.1086384728550911, 0.10638251155614853, 0.10642731189727783, 0.10873780399560928, 0.1077856719493866, 0.10754545778036118, 0.10496574640274048, 0.10478657484054565, 0.1085316389799118, 0.10553313791751862, 0.11137949675321579, 0.10498702526092529, 0.1053108349442482, 0.10416646301746368, 0.10460828989744186, 0.10435617715120316, 0.10515673458576202, 0.10437457263469696, 0.1038738340139389, 0.10427618026733398, 0.10605423897504807, 0.10442915558815002, 0.10407108068466187, 0.10386070609092712, 0.1060677096247673, 0.10510080307722092, 0.10375808924436569, 0.10446058958768845, 0.10448518395423889, 0.1035664975643158, 0.10385292023420334, 0.10340108722448349, 0.10336312651634216, 0.10450038313865662, 0.10328109562397003, 0.10320323705673218, 0.10489363223314285, 0.10314564406871796, 0.1067524179816246, 0.10309441387653351, 0.1041097342967987, 0.10276332497596741, 0.10383529216051102, 0.10346312075853348, 0.10315829515457153, 0.10313080251216888, 0.1011052206158638, 0.10103283077478409, 0.10078052431344986, 0.10243815183639526, 0.09964162856340408, 0.09958412498235703, 0.09826425462961197, 0.09987574070692062, 0.09882520139217377, 0.09814192354679108, 0.09922482818365097, 0.10025317966938019, 0.09904840588569641, 0.09826599061489105, 0.09827707707881927, 0.09891793131828308, 0.09744095802307129, 0.0981869027018547, 0.09753181040287018, 0.09801720082759857, 0.09776066988706589, 0.09776125103235245, 0.0972895547747612, 0.1008385568857193, 0.09753109514713287, 0.09822579473257065]}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.callbacks.History at 0x7efb7f750cd0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gIjI65bYw5d3",
        "outputId": "2aad3d07-dca6-4d44-885f-171d9d237307"
      },
      "source": [
        "# shrinked vectors no activation\n",
        "\n",
        "sae = StackedAutoencoder([100, 80], activation=None)\n",
        "sae.fit(bert_embeddings[:, :100], bert_embeddings[:, :100], verbose=True, epochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "StackAutoencoder: fitting 1 autoencoders with dimensions [100, 80]\n",
            "\tfitting autoencoder number 0 with dimensions (100, 80)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "477/477 [==============================] - 2s 3ms/step - loss: 2.4144 - val_loss: 0.3788\n",
            "Epoch 2/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.3100 - val_loss: 0.1789\n",
            "Epoch 3/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1614 - val_loss: 0.1269\n",
            "Epoch 4/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1231 - val_loss: 0.1124\n",
            "Epoch 5/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1108 - val_loss: 0.1057\n",
            "Epoch 6/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1053 - val_loss: 0.1018\n",
            "Epoch 7/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.1019 - val_loss: 0.0995\n",
            "Epoch 8/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0993 - val_loss: 0.0979\n",
            "Epoch 9/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0990 - val_loss: 0.0976\n",
            "Epoch 10/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0974 - val_loss: 0.0965\n",
            "Epoch 11/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0971 - val_loss: 0.0958\n",
            "Epoch 12/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0957 - val_loss: 0.0962\n",
            "Epoch 13/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0963 - val_loss: 0.0957\n",
            "Epoch 14/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0966 - val_loss: 0.0953\n",
            "Epoch 15/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0950 - val_loss: 0.0956\n",
            "Epoch 16/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0961 - val_loss: 0.0948\n",
            "Epoch 17/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0959 - val_loss: 0.0950\n",
            "Epoch 18/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0956 - val_loss: 0.0949\n",
            "Epoch 19/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0955 - val_loss: 0.0949\n",
            "Epoch 20/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0960 - val_loss: 0.0948\n",
            "Epoch 21/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0950 - val_loss: 0.0947\n",
            "Epoch 22/100\n",
            "477/477 [==============================] - 1s 2ms/step - loss: 0.0959 - val_loss: 0.0947\n",
            "Epoch 23/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0951 - val_loss: 0.0947\n",
            "Epoch 24/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0953 - val_loss: 0.0946\n",
            "Epoch 25/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0953 - val_loss: 0.0961\n",
            "Epoch 26/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0957 - val_loss: 0.0943\n",
            "Epoch 27/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0946 - val_loss: 0.0944\n",
            "Epoch 28/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0953 - val_loss: 0.0942\n",
            "Epoch 29/100\n",
            "477/477 [==============================] - 1s 2ms/step - loss: 0.0951 - val_loss: 0.0947\n",
            "Epoch 30/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0953 - val_loss: 0.0944\n",
            "Epoch 31/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0945 - val_loss: 0.0943\n",
            "Epoch 32/100\n",
            "477/477 [==============================] - 1s 2ms/step - loss: 0.0948 - val_loss: 0.0951\n",
            "Epoch 33/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0951 - val_loss: 0.0947\n",
            "Epoch 34/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0952 - val_loss: 0.0942\n",
            "Epoch 35/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0948 - val_loss: 0.0942\n",
            "Epoch 36/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0943 - val_loss: 0.0942\n",
            "Epoch 37/100\n",
            "477/477 [==============================] - 1s 2ms/step - loss: 0.0948 - val_loss: 0.0940\n",
            "Epoch 38/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0942 - val_loss: 0.0946\n",
            "Epoch 39/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0958 - val_loss: 0.0943\n",
            "Epoch 40/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0949 - val_loss: 0.0944\n",
            "Epoch 41/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0940 - val_loss: 0.0941\n",
            "Epoch 42/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0952 - val_loss: 0.0943\n",
            "Epoch 43/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0951 - val_loss: 0.0943\n",
            "Epoch 44/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0942 - val_loss: 0.0948\n",
            "Epoch 45/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0948 - val_loss: 0.0944\n",
            "Epoch 46/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0951 - val_loss: 0.0941\n",
            "Epoch 47/100\n",
            "477/477 [==============================] - 1s 2ms/step - loss: 0.0950 - val_loss: 0.0943\n",
            "Epoch 48/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0949 - val_loss: 0.0950\n",
            "Epoch 49/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0952 - val_loss: 0.0938\n",
            "Epoch 50/100\n",
            "477/477 [==============================] - 1s 3ms/step - loss: 0.0940 - val_loss: 0.0940\n",
            "Epoch 51/100\n",
            "277/477 [================>.............] - ETA: 0s - loss: 0.0940"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-e3c3870e3531>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStackedAutoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-88-224d6c8d7d30>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, val_data, verbose, epochs)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data_reduced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_reduced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                 \u001b[0;31m# callbacks=[tensorboard]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 _r=1):\n\u001b[1;32m   1157\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNUz1MKWxuVG",
        "outputId": "b61000f4-d259-40cb-d0e4-ae61b508bb18"
      },
      "source": [
        "# full reconstruction long\n",
        "\n",
        "sae = StackedAutoencoder([768, 768])\n",
        "sae.fit(bert_embeddings[:, :768], bert_embeddings[:, :768], verbose=True, epochs=400)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "StackAutoencoder: fitting 1 autoencoders with dimensions [768, 768]\n",
            "\tfitting autoencoder number 0 with dimensions (768, 768)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 1.4496 - val_loss: 0.5284\n",
            "Epoch 2/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.4743 - val_loss: 0.3570\n",
            "Epoch 3/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.3372 - val_loss: 0.2778\n",
            "Epoch 4/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.2671 - val_loss: 0.2318\n",
            "Epoch 5/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.2269 - val_loss: 0.2045\n",
            "Epoch 6/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1993 - val_loss: 0.1808\n",
            "Epoch 7/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1767 - val_loss: 0.1661\n",
            "Epoch 8/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1632 - val_loss: 0.1565\n",
            "Epoch 9/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1536 - val_loss: 0.1388\n",
            "Epoch 10/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1429 - val_loss: 0.1323\n",
            "Epoch 11/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1339 - val_loss: 0.1316\n",
            "Epoch 12/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1283 - val_loss: 0.1219\n",
            "Epoch 13/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1253 - val_loss: 0.1198\n",
            "Epoch 14/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1203 - val_loss: 0.1143\n",
            "Epoch 15/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1191 - val_loss: 0.1078\n",
            "Epoch 16/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.1133 - val_loss: 0.1060\n",
            "Epoch 17/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.1120 - val_loss: 0.1299\n",
            "Epoch 18/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1139 - val_loss: 0.1042\n",
            "Epoch 19/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.1092 - val_loss: 0.1051\n",
            "Epoch 20/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1074 - val_loss: 0.1051\n",
            "Epoch 21/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1054 - val_loss: 0.1175\n",
            "Epoch 22/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1072 - val_loss: 0.1046\n",
            "Epoch 23/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1046 - val_loss: 0.1003\n",
            "Epoch 24/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1006 - val_loss: 0.0994\n",
            "Epoch 25/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1033 - val_loss: 0.0987\n",
            "Epoch 26/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1010 - val_loss: 0.1069\n",
            "Epoch 27/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1016 - val_loss: 0.0961\n",
            "Epoch 28/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0979 - val_loss: 0.0997\n",
            "Epoch 29/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1034 - val_loss: 0.0976\n",
            "Epoch 30/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0970 - val_loss: 0.0980\n",
            "Epoch 31/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0988 - val_loss: 0.0927\n",
            "Epoch 32/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0960 - val_loss: 0.0947\n",
            "Epoch 33/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0978 - val_loss: 0.0924\n",
            "Epoch 34/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0969 - val_loss: 0.0920\n",
            "Epoch 35/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0963 - val_loss: 0.0976\n",
            "Epoch 36/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0961 - val_loss: 0.0939\n",
            "Epoch 37/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0993 - val_loss: 0.0906\n",
            "Epoch 38/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0947 - val_loss: 0.0981\n",
            "Epoch 39/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0968 - val_loss: 0.1022\n",
            "Epoch 40/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0953 - val_loss: 0.0923\n",
            "Epoch 41/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0951 - val_loss: 0.0940\n",
            "Epoch 42/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0942 - val_loss: 0.1178\n",
            "Epoch 43/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0967 - val_loss: 0.0882\n",
            "Epoch 44/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0929 - val_loss: 0.0937\n",
            "Epoch 45/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0997 - val_loss: 0.1069\n",
            "Epoch 46/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0949 - val_loss: 0.0943\n",
            "Epoch 47/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0934 - val_loss: 0.0949\n",
            "Epoch 48/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0923 - val_loss: 0.0861\n",
            "Epoch 49/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0924 - val_loss: 0.0946\n",
            "Epoch 50/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0938 - val_loss: 0.0889\n",
            "Epoch 51/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0921 - val_loss: 0.0927\n",
            "Epoch 52/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0927 - val_loss: 0.0876\n",
            "Epoch 53/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0931 - val_loss: 0.0863\n",
            "Epoch 54/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0910 - val_loss: 0.0858\n",
            "Epoch 55/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0908 - val_loss: 0.0879\n",
            "Epoch 56/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0922 - val_loss: 0.0890\n",
            "Epoch 57/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0938 - val_loss: 0.0880\n",
            "Epoch 58/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0916 - val_loss: 0.1084\n",
            "Epoch 59/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0951 - val_loss: 0.0895\n",
            "Epoch 60/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0918 - val_loss: 0.1055\n",
            "Epoch 61/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0947 - val_loss: 0.0826\n",
            "Epoch 62/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0891 - val_loss: 0.0996\n",
            "Epoch 63/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0926 - val_loss: 0.0834\n",
            "Epoch 64/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0926 - val_loss: 0.0895\n",
            "Epoch 65/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0914 - val_loss: 0.0829\n",
            "Epoch 66/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0894 - val_loss: 0.0972\n",
            "Epoch 67/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0902 - val_loss: 0.0834\n",
            "Epoch 68/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0896 - val_loss: 0.0873\n",
            "Epoch 69/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0912 - val_loss: 0.0910\n",
            "Epoch 70/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0911 - val_loss: 0.0925\n",
            "Epoch 71/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0894 - val_loss: 0.0882\n",
            "Epoch 72/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0931 - val_loss: 0.0915\n",
            "Epoch 73/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0904 - val_loss: 0.0876\n",
            "Epoch 74/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0885 - val_loss: 0.0828\n",
            "Epoch 75/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0907 - val_loss: 0.0879\n",
            "Epoch 76/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0894 - val_loss: 0.0894\n",
            "Epoch 77/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0898 - val_loss: 0.0917\n",
            "Epoch 78/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0894 - val_loss: 0.0808\n",
            "Epoch 79/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0886 - val_loss: 0.0899\n",
            "Epoch 80/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0897 - val_loss: 0.0829\n",
            "Epoch 81/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0876 - val_loss: 0.0820\n",
            "Epoch 82/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0900 - val_loss: 0.0873\n",
            "Epoch 83/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0892 - val_loss: 0.0875\n",
            "Epoch 84/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0885 - val_loss: 0.0912\n",
            "Epoch 85/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0891 - val_loss: 0.0988\n",
            "Epoch 86/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0902 - val_loss: 0.0828\n",
            "Epoch 87/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0882 - val_loss: 0.0851\n",
            "Epoch 88/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0887 - val_loss: 0.0925\n",
            "Epoch 89/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0905 - val_loss: 0.0863\n",
            "Epoch 90/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0873 - val_loss: 0.0963\n",
            "Epoch 91/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0896 - val_loss: 0.0918\n",
            "Epoch 92/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0886 - val_loss: 0.1101\n",
            "Epoch 93/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0900 - val_loss: 0.0843\n",
            "Epoch 94/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0886 - val_loss: 0.0844\n",
            "Epoch 95/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0868 - val_loss: 0.0849\n",
            "Epoch 96/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0901 - val_loss: 0.0896\n",
            "Epoch 97/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0874 - val_loss: 0.0845\n",
            "Epoch 98/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0897 - val_loss: 0.0856\n",
            "Epoch 99/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0862 - val_loss: 0.0941\n",
            "Epoch 100/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0902 - val_loss: 0.0861\n",
            "Epoch 101/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0863 - val_loss: 0.0844\n",
            "Epoch 102/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0865 - val_loss: 0.0821\n",
            "Epoch 103/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0875 - val_loss: 0.0949\n",
            "Epoch 104/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0864 - val_loss: 0.0877\n",
            "Epoch 105/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0881 - val_loss: 0.0921\n",
            "Epoch 106/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0876 - val_loss: 0.0869\n",
            "Epoch 107/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0876 - val_loss: 0.0897\n",
            "Epoch 108/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0874 - val_loss: 0.0887\n",
            "Epoch 109/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0876 - val_loss: 0.0832\n",
            "Epoch 110/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0870 - val_loss: 0.0901\n",
            "Epoch 111/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0898 - val_loss: 0.0854\n",
            "Epoch 112/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0889 - val_loss: 0.0849\n",
            "Epoch 113/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0866 - val_loss: 0.0888\n",
            "Epoch 114/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0877 - val_loss: 0.0900\n",
            "Epoch 115/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0895 - val_loss: 0.0881\n",
            "Epoch 116/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0881 - val_loss: 0.0871\n",
            "Epoch 117/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0866 - val_loss: 0.0851\n",
            "Epoch 118/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0853 - val_loss: 0.0824\n",
            "Epoch 119/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0873 - val_loss: 0.0827\n",
            "Epoch 120/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0860 - val_loss: 0.0845\n",
            "Epoch 121/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0878 - val_loss: 0.0853\n",
            "Epoch 122/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0871 - val_loss: 0.0841\n",
            "Epoch 123/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0879 - val_loss: 0.0820\n",
            "Epoch 124/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0871 - val_loss: 0.0867\n",
            "Epoch 125/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0871 - val_loss: 0.0834\n",
            "Epoch 126/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0887 - val_loss: 0.0806\n",
            "Epoch 127/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0859 - val_loss: 0.0860\n",
            "Epoch 128/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0863 - val_loss: 0.0823\n",
            "Epoch 129/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0852 - val_loss: 0.0877\n",
            "Epoch 130/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0882 - val_loss: 0.0802\n",
            "Epoch 131/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0868 - val_loss: 0.0781\n",
            "Epoch 132/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0860 - val_loss: 0.0836\n",
            "Epoch 133/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0896 - val_loss: 0.0873\n",
            "Epoch 134/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0879 - val_loss: 0.0892\n",
            "Epoch 135/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0869 - val_loss: 0.0803\n",
            "Epoch 136/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0870 - val_loss: 0.0799\n",
            "Epoch 137/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0869 - val_loss: 0.0867\n",
            "Epoch 138/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0877 - val_loss: 0.0863\n",
            "Epoch 139/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0858 - val_loss: 0.0924\n",
            "Epoch 140/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0873 - val_loss: 0.0893\n",
            "Epoch 141/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0892 - val_loss: 0.0802\n",
            "Epoch 142/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0858 - val_loss: 0.0826\n",
            "Epoch 143/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0883 - val_loss: 0.0875\n",
            "Epoch 144/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0879 - val_loss: 0.0888\n",
            "Epoch 145/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0879 - val_loss: 0.0845\n",
            "Epoch 146/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0872 - val_loss: 0.0855\n",
            "Epoch 147/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0863 - val_loss: 0.0847\n",
            "Epoch 148/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0870 - val_loss: 0.0825\n",
            "Epoch 149/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0870 - val_loss: 0.0861\n",
            "Epoch 150/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0870 - val_loss: 0.0839\n",
            "Epoch 151/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0880 - val_loss: 0.0854\n",
            "Epoch 152/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0866 - val_loss: 0.0969\n",
            "Epoch 153/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0868 - val_loss: 0.0811\n",
            "Epoch 154/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0851 - val_loss: 0.0853\n",
            "Epoch 155/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0886 - val_loss: 0.0855\n",
            "Epoch 156/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0849 - val_loss: 0.0846\n",
            "Epoch 157/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0879 - val_loss: 0.0888\n",
            "Epoch 158/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0874 - val_loss: 0.0915\n",
            "Epoch 159/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0884 - val_loss: 0.0848\n",
            "Epoch 160/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0872 - val_loss: 0.0862\n",
            "Epoch 161/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0861 - val_loss: 0.0843\n",
            "Epoch 162/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0884 - val_loss: 0.0825\n",
            "Epoch 163/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0840 - val_loss: 0.0915\n",
            "Epoch 164/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0862 - val_loss: 0.0835\n",
            "Epoch 165/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0863 - val_loss: 0.0858\n",
            "Epoch 166/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0879 - val_loss: 0.0867\n",
            "Epoch 167/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0861 - val_loss: 0.0866\n",
            "Epoch 168/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0858 - val_loss: 0.0930\n",
            "Epoch 169/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0868 - val_loss: 0.0830\n",
            "Epoch 170/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0855 - val_loss: 0.0815\n",
            "Epoch 171/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0839 - val_loss: 0.0862\n",
            "Epoch 172/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0869 - val_loss: 0.0827\n",
            "Epoch 173/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0844 - val_loss: 0.0883\n",
            "Epoch 174/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0864 - val_loss: 0.0798\n",
            "Epoch 175/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0849 - val_loss: 0.0844\n",
            "Epoch 176/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0857 - val_loss: 0.0807\n",
            "Epoch 177/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0840 - val_loss: 0.0846\n",
            "Epoch 178/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0881 - val_loss: 0.0856\n",
            "Epoch 179/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0870 - val_loss: 0.0838\n",
            "Epoch 180/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0880 - val_loss: 0.0854\n",
            "Epoch 181/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0873 - val_loss: 0.0810\n",
            "Epoch 182/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0830 - val_loss: 0.0888\n",
            "Epoch 183/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0878 - val_loss: 0.0906\n",
            "Epoch 184/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0853 - val_loss: 0.0874\n",
            "Epoch 185/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0871 - val_loss: 0.0876\n",
            "Epoch 186/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0873 - val_loss: 0.0848\n",
            "Epoch 187/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0861 - val_loss: 0.0833\n",
            "Epoch 188/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0861 - val_loss: 0.0894\n",
            "Epoch 189/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0860 - val_loss: 0.0918\n",
            "Epoch 190/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0857 - val_loss: 0.0819\n",
            "Epoch 191/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0862 - val_loss: 0.0775\n",
            "Epoch 192/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0864 - val_loss: 0.0802\n",
            "Epoch 193/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0861 - val_loss: 0.0811\n",
            "Epoch 194/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0845 - val_loss: 0.0806\n",
            "Epoch 195/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0838 - val_loss: 0.0858\n",
            "Epoch 196/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0847 - val_loss: 0.0940\n",
            "Epoch 197/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0866 - val_loss: 0.0836\n",
            "Epoch 198/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0871 - val_loss: 0.0829\n",
            "Epoch 199/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0860 - val_loss: 0.0915\n",
            "Epoch 200/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0861 - val_loss: 0.0845\n",
            "Epoch 201/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0845 - val_loss: 0.0828\n",
            "Epoch 202/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0844 - val_loss: 0.0831\n",
            "Epoch 203/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0836 - val_loss: 0.0839\n",
            "Epoch 204/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0849 - val_loss: 0.0817\n",
            "Epoch 205/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0851 - val_loss: 0.0901\n",
            "Epoch 206/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0859 - val_loss: 0.0814\n",
            "Epoch 207/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0869 - val_loss: 0.0886\n",
            "Epoch 208/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0876 - val_loss: 0.0780\n",
            "Epoch 209/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0839 - val_loss: 0.0902\n",
            "Epoch 210/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0839 - val_loss: 0.0835\n",
            "Epoch 211/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0855 - val_loss: 0.1235\n",
            "Epoch 212/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0924 - val_loss: 0.0798\n",
            "Epoch 213/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0847 - val_loss: 0.0810\n",
            "Epoch 214/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0853 - val_loss: 0.0839\n",
            "Epoch 215/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0840 - val_loss: 0.0809\n",
            "Epoch 216/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0854 - val_loss: 0.0826\n",
            "Epoch 217/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0846 - val_loss: 0.0865\n",
            "Epoch 218/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0836 - val_loss: 0.0888\n",
            "Epoch 219/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0857 - val_loss: 0.0813\n",
            "Epoch 220/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0853 - val_loss: 0.0807\n",
            "Epoch 221/400\n",
            "477/477 [==============================] - 12s 26ms/step - loss: 0.0882 - val_loss: 0.0795\n",
            "Epoch 222/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0848 - val_loss: 0.0860\n",
            "Epoch 223/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0846 - val_loss: 0.0860\n",
            "Epoch 224/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0865 - val_loss: 0.0820\n",
            "Epoch 225/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0855 - val_loss: 0.0906\n",
            "Epoch 226/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0861 - val_loss: 0.0856\n",
            "Epoch 227/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0861 - val_loss: 0.0801\n",
            "Epoch 228/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0845 - val_loss: 0.0867\n",
            "Epoch 229/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0848 - val_loss: 0.0803\n",
            "Epoch 230/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0868 - val_loss: 0.0798\n",
            "Epoch 231/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0861 - val_loss: 0.0896\n",
            "Epoch 232/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0847 - val_loss: 0.0798\n",
            "Epoch 233/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0839 - val_loss: 0.0829\n",
            "Epoch 234/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0861 - val_loss: 0.0804\n",
            "Epoch 235/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0863 - val_loss: 0.0919\n",
            "Epoch 236/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0868 - val_loss: 0.0822\n",
            "Epoch 237/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0843 - val_loss: 0.0802\n",
            "Epoch 238/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0866 - val_loss: 0.0899\n",
            "Epoch 239/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0864 - val_loss: 0.0972\n",
            "Epoch 240/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0868 - val_loss: 0.0805\n",
            "Epoch 241/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0859 - val_loss: 0.0802\n",
            "Epoch 242/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0839 - val_loss: 0.0854\n",
            "Epoch 243/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0848 - val_loss: 0.0784\n",
            "Epoch 244/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0875 - val_loss: 0.0815\n",
            "Epoch 245/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0839 - val_loss: 0.0860\n",
            "Epoch 246/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0843 - val_loss: 0.0825\n",
            "Epoch 247/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0867 - val_loss: 0.0801\n",
            "Epoch 248/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0856 - val_loss: 0.0833\n",
            "Epoch 249/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0852 - val_loss: 0.0867\n",
            "Epoch 250/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0844 - val_loss: 0.0783\n",
            "Epoch 251/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0844 - val_loss: 0.0862\n",
            "Epoch 252/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0840 - val_loss: 0.0896\n",
            "Epoch 253/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0838 - val_loss: 0.0838\n",
            "Epoch 254/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0839 - val_loss: 0.0827\n",
            "Epoch 255/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0853 - val_loss: 0.0879\n",
            "Epoch 256/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0874 - val_loss: 0.0857\n",
            "Epoch 257/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0858 - val_loss: 0.0763\n",
            "Epoch 258/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0841 - val_loss: 0.0827\n",
            "Epoch 259/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0876 - val_loss: 0.0822\n",
            "Epoch 260/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0852 - val_loss: 0.0866\n",
            "Epoch 261/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0854 - val_loss: 0.0928\n",
            "Epoch 262/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0856 - val_loss: 0.0803\n",
            "Epoch 263/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0857 - val_loss: 0.0846\n",
            "Epoch 264/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0841 - val_loss: 0.0806\n",
            "Epoch 265/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0846 - val_loss: 0.0844\n",
            "Epoch 266/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0863 - val_loss: 0.0800\n",
            "Epoch 267/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0830 - val_loss: 0.0804\n",
            "Epoch 268/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0848 - val_loss: 0.0913\n",
            "Epoch 269/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0846 - val_loss: 0.0850\n",
            "Epoch 270/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0840 - val_loss: 0.0791\n",
            "Epoch 271/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0866 - val_loss: 0.0807\n",
            "Epoch 272/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0829 - val_loss: 0.0846\n",
            "Epoch 273/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0851 - val_loss: 0.0836\n",
            "Epoch 274/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0858 - val_loss: 0.0838\n",
            "Epoch 275/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0823 - val_loss: 0.0794\n",
            "Epoch 276/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0846 - val_loss: 0.0903\n",
            "Epoch 277/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0878 - val_loss: 0.0786\n",
            "Epoch 278/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0840 - val_loss: 0.0803\n",
            "Epoch 279/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0857 - val_loss: 0.0793\n",
            "Epoch 280/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0847 - val_loss: 0.0838\n",
            "Epoch 281/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0846 - val_loss: 0.0817\n",
            "Epoch 282/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0847 - val_loss: 0.0817\n",
            "Epoch 283/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0841 - val_loss: 0.0941\n",
            "Epoch 284/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0864 - val_loss: 0.0779\n",
            "Epoch 285/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0845 - val_loss: 0.0900\n",
            "Epoch 286/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0859 - val_loss: 0.0805\n",
            "Epoch 287/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0863 - val_loss: 0.0822\n",
            "Epoch 288/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0838 - val_loss: 0.0837\n",
            "Epoch 289/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0850 - val_loss: 0.0805\n",
            "Epoch 290/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0836 - val_loss: 0.0862\n",
            "Epoch 291/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0853 - val_loss: 0.0803\n",
            "Epoch 292/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0835 - val_loss: 0.0826\n",
            "Epoch 293/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0830 - val_loss: 0.0837\n",
            "Epoch 294/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0841 - val_loss: 0.0910\n",
            "Epoch 295/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0868 - val_loss: 0.0837\n",
            "Epoch 296/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0830 - val_loss: 0.0923\n",
            "Epoch 297/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0880 - val_loss: 0.0901\n",
            "Epoch 298/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0847 - val_loss: 0.0807\n",
            "Epoch 299/400\n",
            "477/477 [==============================] - 10s 22ms/step - loss: 0.0830 - val_loss: 0.0785\n",
            "Epoch 300/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0859 - val_loss: 0.0806\n",
            "Epoch 301/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0857 - val_loss: 0.0860\n",
            "Epoch 302/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0846 - val_loss: 0.0797\n",
            "Epoch 303/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0852 - val_loss: 0.0773\n",
            "Epoch 304/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0878 - val_loss: 0.0795\n",
            "Epoch 305/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0875 - val_loss: 0.0873\n",
            "Epoch 306/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0853 - val_loss: 0.0922\n",
            "Epoch 307/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0841 - val_loss: 0.0857\n",
            "Epoch 308/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0843 - val_loss: 0.0764\n",
            "Epoch 309/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0840 - val_loss: 0.0776\n",
            "Epoch 310/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0847 - val_loss: 0.0870\n",
            "Epoch 311/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0836 - val_loss: 0.0864\n",
            "Epoch 312/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0847 - val_loss: 0.0868\n",
            "Epoch 313/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0842 - val_loss: 0.0904\n",
            "Epoch 314/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0841 - val_loss: 0.0806\n",
            "Epoch 315/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0860 - val_loss: 0.0809\n",
            "Epoch 316/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0832 - val_loss: 0.0823\n",
            "Epoch 317/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0846 - val_loss: 0.0858\n",
            "Epoch 318/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0851 - val_loss: 0.0828\n",
            "Epoch 319/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0853 - val_loss: 0.0830\n",
            "Epoch 320/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0858 - val_loss: 0.0854\n",
            "Epoch 321/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0842 - val_loss: 0.0881\n",
            "Epoch 322/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0830 - val_loss: 0.0851\n",
            "Epoch 323/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0842 - val_loss: 0.0848\n",
            "Epoch 324/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0846 - val_loss: 0.0779\n",
            "Epoch 325/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0829 - val_loss: 0.0828\n",
            "Epoch 326/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0839 - val_loss: 0.0827\n",
            "Epoch 327/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0851 - val_loss: 0.0906\n",
            "Epoch 328/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0858 - val_loss: 0.0889\n",
            "Epoch 329/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0852 - val_loss: 0.0766\n",
            "Epoch 330/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0842 - val_loss: 0.0817\n",
            "Epoch 331/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0831 - val_loss: 0.0830\n",
            "Epoch 332/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0826 - val_loss: 0.0814\n",
            "Epoch 333/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0837 - val_loss: 0.0885\n",
            "Epoch 334/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0855 - val_loss: 0.0821\n",
            "Epoch 335/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0833 - val_loss: 0.0774\n",
            "Epoch 336/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0830 - val_loss: 0.0833\n",
            "Epoch 337/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0833 - val_loss: 0.0853\n",
            "Epoch 338/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0847 - val_loss: 0.0807\n",
            "Epoch 339/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0836 - val_loss: 0.0948\n",
            "Epoch 340/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0860 - val_loss: 0.0866\n",
            "Epoch 341/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0840 - val_loss: 0.0856\n",
            "Epoch 342/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0848 - val_loss: 0.0782\n",
            "Epoch 343/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0838 - val_loss: 0.0839\n",
            "Epoch 344/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0830 - val_loss: 0.0803\n",
            "Epoch 345/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0840 - val_loss: 0.0850\n",
            "Epoch 346/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0836 - val_loss: 0.0813\n",
            "Epoch 347/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0835 - val_loss: 0.0874\n",
            "Epoch 348/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0835 - val_loss: 0.0814\n",
            "Epoch 349/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0842 - val_loss: 0.0794\n",
            "Epoch 350/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0829 - val_loss: 0.0829\n",
            "Epoch 351/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0844 - val_loss: 0.0791\n",
            "Epoch 352/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0830 - val_loss: 0.0852\n",
            "Epoch 353/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0829 - val_loss: 0.0912\n",
            "Epoch 354/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0847 - val_loss: 0.1135\n",
            "Epoch 355/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0870 - val_loss: 0.0891\n",
            "Epoch 356/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0836 - val_loss: 0.0853\n",
            "Epoch 357/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0864 - val_loss: 0.0833\n",
            "Epoch 358/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0837 - val_loss: 0.0878\n",
            "Epoch 359/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0833 - val_loss: 0.0819\n",
            "Epoch 360/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0848 - val_loss: 0.0810\n",
            "Epoch 361/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0843 - val_loss: 0.0808\n",
            "Epoch 362/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0825 - val_loss: 0.0799\n",
            "Epoch 363/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0843 - val_loss: 0.0820\n",
            "Epoch 364/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0842 - val_loss: 0.0895\n",
            "Epoch 365/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0839 - val_loss: 0.0859\n",
            "Epoch 366/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0847 - val_loss: 0.0819\n",
            "Epoch 367/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0862 - val_loss: 0.0800\n",
            "Epoch 368/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0842 - val_loss: 0.0883\n",
            "Epoch 369/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0834 - val_loss: 0.0823\n",
            "Epoch 370/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0823 - val_loss: 0.0805\n",
            "Epoch 371/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0827 - val_loss: 0.0799\n",
            "Epoch 372/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0835 - val_loss: 0.0881\n",
            "Epoch 373/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0860 - val_loss: 0.0864\n",
            "Epoch 374/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0875 - val_loss: 0.0807\n",
            "Epoch 375/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0834 - val_loss: 0.0865\n",
            "Epoch 376/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0863 - val_loss: 0.0799\n",
            "Epoch 377/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0841 - val_loss: 0.0823\n",
            "Epoch 378/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0837 - val_loss: 0.0792\n",
            "Epoch 379/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0826 - val_loss: 0.0842\n",
            "Epoch 380/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0842 - val_loss: 0.0826\n",
            "Epoch 381/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0835 - val_loss: 0.0791\n",
            "Epoch 382/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0834 - val_loss: 0.0866\n",
            "Epoch 383/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0836 - val_loss: 0.0795\n",
            "Epoch 384/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0829 - val_loss: 0.0938\n",
            "Epoch 385/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0850 - val_loss: 0.0802\n",
            "Epoch 386/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0837 - val_loss: 0.0799\n",
            "Epoch 387/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0841 - val_loss: 0.0798\n",
            "Epoch 388/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0824 - val_loss: 0.0800\n",
            "Epoch 389/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0858 - val_loss: 0.0782\n",
            "Epoch 390/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0832 - val_loss: 0.0780\n",
            "Epoch 391/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0848 - val_loss: 0.0796\n",
            "Epoch 392/400\n",
            "477/477 [==============================] - 10s 22ms/step - loss: 0.0824 - val_loss: 0.0858\n",
            "Epoch 393/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0831 - val_loss: 0.0798\n",
            "Epoch 394/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0844 - val_loss: 0.0799\n",
            "Epoch 395/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0827 - val_loss: 0.0767\n",
            "Epoch 396/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0841 - val_loss: 0.0823\n",
            "Epoch 397/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0856 - val_loss: 0.0840\n",
            "Epoch 398/400\n",
            "477/477 [==============================] - 10s 22ms/step - loss: 0.0829 - val_loss: 0.0775\n",
            "Epoch 399/400\n",
            "477/477 [==============================] - 10s 22ms/step - loss: 0.0828 - val_loss: 0.0817\n",
            "Epoch 400/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0839 - val_loss: 0.0833\n",
            "StackedAutoencoder: autoencoder 0 - model history\n",
            "{'loss': [0.9586986303329468, 0.43353161215782166, 0.3195570111274719, 0.2590569257736206, 0.22160126268863678, 0.19420692324638367, 0.17569619417190552, 0.16103816032409668, 0.15031276643276215, 0.14215132594108582, 0.13529103994369507, 0.12872116267681122, 0.125270813703537, 0.12099774181842804, 0.11792266368865967, 0.11577246338129044, 0.1131264716386795, 0.11118420958518982, 0.11061935871839523, 0.10858958214521408, 0.10743873566389084, 0.10577709972858429, 0.10584712773561478, 0.10210639238357544, 0.10364451259374619, 0.10217873007059097, 0.10097365081310272, 0.09988094121217728, 0.10288704186677933, 0.09845668822526932, 0.100652314722538, 0.09895968437194824, 0.09830562770366669, 0.09730803966522217, 0.09829114377498627, 0.09615201503038406, 0.09838876128196716, 0.09667046368122101, 0.09782020002603531, 0.0953652560710907, 0.09604618698358536, 0.09541135281324387, 0.09663500636816025, 0.09355748444795609, 0.09730250388383865, 0.09467247128486633, 0.09590204805135727, 0.09288399666547775, 0.09333600848913193, 0.09514912217855453, 0.09522522985935211, 0.09358666092157364, 0.09341463446617126, 0.09350266307592392, 0.09194551408290863, 0.09142656624317169, 0.09273786097764969, 0.09288147836923599, 0.09348301589488983, 0.09256128966808319, 0.09293673932552338, 0.09173718839883804, 0.09211432933807373, 0.09282424300909042, 0.09112008661031723, 0.09147202968597412, 0.09097378700971603, 0.09097114205360413, 0.09195538610219955, 0.0911974161863327, 0.09077931940555573, 0.09194424003362656, 0.09062846750020981, 0.09062885493040085, 0.09070976078510284, 0.0909118726849556, 0.08937622606754303, 0.09039540588855743, 0.09051500260829926, 0.09056791663169861, 0.08897455781698227, 0.09049133956432343, 0.09049096703529358, 0.08919520676136017, 0.08910651504993439, 0.08972568064928055, 0.0895351842045784, 0.09027991443872452, 0.09051697701215744, 0.08852218836545944, 0.0900588259100914, 0.08879053592681885, 0.08960654586553574, 0.09020227193832397, 0.08861307054758072, 0.09097357839345932, 0.08845634013414383, 0.09053544700145721, 0.08763916790485382, 0.09011487662792206, 0.08751481771469116, 0.0875958502292633, 0.08904796093702316, 0.08710254728794098, 0.08957763016223907, 0.08810971677303314, 0.08946086466312408, 0.08877275884151459, 0.08897435665130615, 0.08806172758340836, 0.08829905092716217, 0.08999251574277878, 0.08797238767147064, 0.08741503953933716, 0.08809265494346619, 0.08942955732345581, 0.08797800540924072, 0.08741030097007751, 0.08879584074020386, 0.08771966397762299, 0.08841510862112045, 0.08733615279197693, 0.08917319774627686, 0.08797359466552734, 0.0880647823214531, 0.08907441049814224, 0.08770206570625305, 0.08787254989147186, 0.08822266757488251, 0.08835956454277039, 0.08726038783788681, 0.0882701426744461, 0.08947068452835083, 0.08812277764081955, 0.08758266270160675, 0.08763312548398972, 0.08777201175689697, 0.0898839458823204, 0.08676330000162125, 0.0876646563410759, 0.0891658216714859, 0.0867709070444107, 0.08865778148174286, 0.08800072968006134, 0.08786961436271667, 0.08786161988973618, 0.08751966804265976, 0.08736851066350937, 0.08683665841817856, 0.087593212723732, 0.08859558403491974, 0.08674057573080063, 0.08722309023141861, 0.08671912550926208, 0.08859501034021378, 0.08557792007923126, 0.08789804577827454, 0.08684717863798141, 0.08992864191532135, 0.08692219853401184, 0.08621479570865631, 0.08872312307357788, 0.08612819015979767, 0.08748090267181396, 0.08763915300369263, 0.08760840445756912, 0.08649677038192749, 0.08641554415225983, 0.08714021742343903, 0.0867820680141449, 0.08722744137048721, 0.08782374113798141, 0.08574111759662628, 0.08602724969387054, 0.08711310476064682, 0.08720386028289795, 0.0853528305888176, 0.08697418868541718, 0.087603859603405, 0.08770070225000381, 0.08855781704187393, 0.08465974777936935, 0.08887092024087906, 0.08589544147253036, 0.08716905117034912, 0.08765533566474915, 0.08661343902349472, 0.08656973391771317, 0.08658365905284882, 0.0868721529841423, 0.08706780523061752, 0.08671920746564865, 0.0870128870010376, 0.08631406724452972, 0.08667602390050888, 0.08603240549564362, 0.08628983795642853, 0.08719083666801453, 0.08750800043344498, 0.0862942710518837, 0.08575137704610825, 0.08597405254840851, 0.08642416447401047, 0.0872037336230278, 0.08543498814105988, 0.08589605242013931, 0.08760730177164078, 0.08663434535264969, 0.08598802983760834, 0.0840596854686737, 0.08703669160604477, 0.08795090764760971, 0.08608491718769073, 0.08606290072202682, 0.08583188056945801, 0.08677255362272263, 0.08588922768831253, 0.0857013389468193, 0.08550342917442322, 0.08613698929548264, 0.08745764195919037, 0.08635392040014267, 0.08495718240737915, 0.08668015152215958, 0.08640707284212112, 0.08703801780939102, 0.08763004094362259, 0.08531951904296875, 0.08482679724693298, 0.08554453402757645, 0.08611414581537247, 0.08516846597194672, 0.08645924925804138, 0.08597524464130402, 0.08651689440011978, 0.08706668764352798, 0.08470233529806137, 0.087318055331707, 0.08627144247293472, 0.08477377891540527, 0.08651044219732285, 0.08480264991521835, 0.0855325236916542, 0.08855527639389038, 0.0849231407046318, 0.08505740016698837, 0.08614455163478851, 0.0866873636841774, 0.08679568022489548, 0.08454859256744385, 0.08587659895420074, 0.08516929298639297, 0.08519414812326431, 0.08516912907361984, 0.08533874899148941, 0.08628489077091217, 0.0852513387799263, 0.08513115346431732, 0.08744651079177856, 0.08550406247377396, 0.08526261150836945, 0.08591361343860626, 0.08562049269676208, 0.08548539876937866, 0.0849330946803093, 0.08702880889177322, 0.08429249376058578, 0.08630100637674332, 0.08534389734268188, 0.0839092954993248, 0.08699601888656616, 0.0850234255194664, 0.0850779265165329, 0.08729737997055054, 0.0839894488453865, 0.08573145419359207, 0.08715493232011795, 0.08571527898311615, 0.08592300862073898, 0.08410023152828217, 0.08516237884759903, 0.08529357612133026, 0.08514440059661865, 0.08633476495742798, 0.08592205494642258, 0.08554740250110626, 0.08542873710393906, 0.08397164940834045, 0.08731015771627426, 0.08500063419342041, 0.08599667251110077, 0.08486568182706833, 0.08475091308355331, 0.08480759710073471, 0.0861637219786644, 0.08431536704301834, 0.08610573410987854, 0.08515781909227371, 0.08500465750694275, 0.08554860949516296, 0.08533630520105362, 0.08411449939012527, 0.084400475025177, 0.0864952877163887, 0.08793887495994568, 0.08529076725244522, 0.08462478965520859, 0.08431189507246017, 0.08476986736059189, 0.08629045635461807, 0.08491216599941254, 0.08518626540899277, 0.08461207896471024, 0.08411301672458649, 0.08636967837810516, 0.08484992384910583, 0.08637873083353043, 0.08449852466583252, 0.08429350703954697, 0.08692337572574615, 0.0862249881029129, 0.08313266187906265, 0.08466700464487076, 0.08481882512569427, 0.08452284336090088, 0.08486541360616684, 0.08440041542053223, 0.08571627736091614, 0.08486978709697723, 0.08605415374040604, 0.0843004435300827, 0.08476083725690842, 0.08414245396852493, 0.08639971166849136, 0.08447093516588211, 0.08394938707351685, 0.083304762840271, 0.0845944732427597, 0.08556228876113892, 0.08531829714775085, 0.08349531888961792, 0.08490747958421707, 0.08486034721136093, 0.08514397591352463, 0.08554167300462723, 0.08424724638462067, 0.08557956665754318, 0.08406063914299011, 0.08631140738725662, 0.08479298651218414, 0.08564387261867523, 0.08429919183254242, 0.0837484747171402, 0.08499655872583389, 0.08449658006429672, 0.08379147201776505, 0.08627433329820633, 0.08503538370132446, 0.08426205813884735, 0.0849418193101883, 0.08491673320531845, 0.08346062153577805, 0.08517301827669144, 0.08419057726860046, 0.08361531049013138, 0.08503791689872742, 0.08619508147239685, 0.08468525856733322, 0.08480461686849594, 0.08428138494491577, 0.08423557132482529, 0.08351042121648788, 0.08534862846136093, 0.08581862598657608, 0.08364125341176987, 0.08575272560119629, 0.08554742485284805, 0.08457528799772263, 0.08488945662975311, 0.08482159674167633, 0.08538378030061722, 0.08456714451313019, 0.08377153426408768, 0.08371666818857193, 0.08515405654907227, 0.08486996591091156, 0.08502116799354553, 0.08377247303724289, 0.08616989105939865, 0.0849648267030716, 0.08454789966344833, 0.08368653059005737, 0.08417486399412155, 0.08467843383550644, 0.08407529443502426, 0.08465275168418884, 0.08564651012420654, 0.0844469740986824, 0.08489792793989182, 0.08380085974931717], 'val_loss': [0.5283607840538025, 0.35704681277275085, 0.2777908742427826, 0.23180142045021057, 0.2044873833656311, 0.18082356452941895, 0.1660519689321518, 0.15645116567611694, 0.13879366219043732, 0.13232064247131348, 0.13162045180797577, 0.12192060053348541, 0.11975368112325668, 0.11426326632499695, 0.10777474194765091, 0.10595909506082535, 0.12987130880355835, 0.10420771688222885, 0.10509130358695984, 0.10506805777549744, 0.11745181679725647, 0.10457494854927063, 0.10029761493206024, 0.09935405105352402, 0.09871864318847656, 0.10694433748722076, 0.09606219083070755, 0.09968053549528122, 0.09762227535247803, 0.09802369028329849, 0.09270346909761429, 0.09471188485622406, 0.09244020283222198, 0.09199320524930954, 0.09756030142307281, 0.09387420862913132, 0.09058330208063126, 0.09811113029718399, 0.10222643613815308, 0.0923149511218071, 0.09402476996183395, 0.11777966469526291, 0.0881553441286087, 0.09366844594478607, 0.1069318950176239, 0.09433655440807343, 0.0949174165725708, 0.08606638759374619, 0.09460026770830154, 0.08886246383190155, 0.09270527213811874, 0.08759966492652893, 0.08628026396036148, 0.0857648253440857, 0.08793134987354279, 0.08903120458126068, 0.08799231797456741, 0.10839518159627914, 0.0894949734210968, 0.10547386109828949, 0.08264932781457901, 0.09957367181777954, 0.08336378633975983, 0.08950679004192352, 0.08294489234685898, 0.09717590361833572, 0.08344712853431702, 0.08726156502962112, 0.09102050960063934, 0.09248188138008118, 0.0881505087018013, 0.09154007583856583, 0.08755838871002197, 0.08284751325845718, 0.08791575580835342, 0.08944262564182281, 0.09171511232852936, 0.08075509965419769, 0.08988165110349655, 0.08291251212358475, 0.0820125937461853, 0.08727432042360306, 0.08750763535499573, 0.09116914123296738, 0.09876944124698639, 0.08276994526386261, 0.08505235612392426, 0.09252448379993439, 0.08632391691207886, 0.09632665663957596, 0.09184244275093079, 0.1101401075720787, 0.08425933867692947, 0.08439117670059204, 0.08490312099456787, 0.08959788084030151, 0.08451627194881439, 0.08562181144952774, 0.09407110512256622, 0.0861055850982666, 0.08444176614284515, 0.08210112899541855, 0.0949474647641182, 0.08766216039657593, 0.09207058697938919, 0.08685624599456787, 0.0897015631198883, 0.08872433006763458, 0.08321129530668259, 0.09007442742586136, 0.08536271750926971, 0.0848863422870636, 0.08880119770765305, 0.09003972262144089, 0.08807089924812317, 0.08709213137626648, 0.08514836430549622, 0.08244754374027252, 0.08269061893224716, 0.08448845148086548, 0.08531208336353302, 0.08409338444471359, 0.08197959512472153, 0.08666515350341797, 0.08335500210523605, 0.08061355352401733, 0.08599099516868591, 0.08234460651874542, 0.08771912753582001, 0.08017368614673615, 0.07812253385782242, 0.0836438238620758, 0.08731458336114883, 0.08918129652738571, 0.08030707389116287, 0.07991794496774673, 0.08668933808803558, 0.08631204813718796, 0.09238258004188538, 0.08925556391477585, 0.08021287620067596, 0.08257734030485153, 0.08747456222772598, 0.08875437080860138, 0.0844552144408226, 0.08549278974533081, 0.08471712470054626, 0.08251508325338364, 0.0861058160662651, 0.08393765985965729, 0.08537376672029495, 0.09692792594432831, 0.08105739206075668, 0.08527196943759918, 0.08547339588403702, 0.08464523404836655, 0.0887722298502922, 0.09149961173534393, 0.08477833122015, 0.08622436225414276, 0.0843074694275856, 0.0825236514210701, 0.09149326384067535, 0.08351054787635803, 0.0857672393321991, 0.0867386907339096, 0.08655643463134766, 0.09301578998565674, 0.08297129720449448, 0.08152635395526886, 0.08620390295982361, 0.08270606398582458, 0.0882687196135521, 0.0797845646739006, 0.08443848043680191, 0.08070351928472519, 0.08457472920417786, 0.08560296893119812, 0.08380448818206787, 0.08540575206279755, 0.0809580609202385, 0.08879268169403076, 0.09060043096542358, 0.08736725151538849, 0.08760365098714828, 0.0847802683711052, 0.08326328545808792, 0.08935355395078659, 0.09182678163051605, 0.08192247152328491, 0.07754702121019363, 0.08020583540201187, 0.08110052347183228, 0.08059993386268616, 0.08580038696527481, 0.0939994752407074, 0.08362512290477753, 0.082941934466362, 0.0914769396185875, 0.0844666138291359, 0.08278445899486542, 0.08305046707391739, 0.08393028378486633, 0.08171838521957397, 0.09006178379058838, 0.08136386424303055, 0.08862410485744476, 0.07802290469408035, 0.09022275358438492, 0.08352714031934738, 0.12349127978086472, 0.07978417724370956, 0.08096113055944443, 0.0838681012392044, 0.08085288107395172, 0.0826086550951004, 0.08652632683515549, 0.08879732340574265, 0.08129161596298218, 0.08067674189805984, 0.07945485413074493, 0.08596789836883545, 0.08599726110696793, 0.08203424513339996, 0.0905921682715416, 0.08558537811040878, 0.08008221536874771, 0.08670015633106232, 0.08031326532363892, 0.07978973537683487, 0.08963535726070404, 0.07982351630926132, 0.08288555592298508, 0.0803956612944603, 0.09186267107725143, 0.08224021643400192, 0.08015257865190506, 0.08991764485836029, 0.0972292497754097, 0.0805170089006424, 0.08016569912433624, 0.08535399287939072, 0.0783972442150116, 0.08150144666433334, 0.0859820619225502, 0.08246464282274246, 0.08013463765382767, 0.08331049233675003, 0.08668249100446701, 0.07829996943473816, 0.08621405065059662, 0.08958154916763306, 0.08377348631620407, 0.0827125757932663, 0.08793657273054123, 0.08572271466255188, 0.07625342160463333, 0.08267387002706528, 0.08223645389080048, 0.08657097071409225, 0.0927780345082283, 0.08031690120697021, 0.08462074398994446, 0.08063895255327225, 0.08444652706384659, 0.08000205457210541, 0.08042185008525848, 0.0912940502166748, 0.0849946066737175, 0.07906060665845871, 0.08069201558828354, 0.08464127033948898, 0.08360650390386581, 0.08379489928483963, 0.07938620448112488, 0.0903070941567421, 0.07857372611761093, 0.08031711727380753, 0.0792786180973053, 0.08383417129516602, 0.0816962942481041, 0.08167251199483871, 0.09414999186992645, 0.07788323611021042, 0.08998080343008041, 0.0805165246129036, 0.08224654942750931, 0.08369504660367966, 0.08048481494188309, 0.08621976524591446, 0.08030027151107788, 0.08259639143943787, 0.08373405784368515, 0.09104286879301071, 0.08372843265533447, 0.09228260815143585, 0.09010794013738632, 0.08066794276237488, 0.07851038128137589, 0.08064039796590805, 0.0859585627913475, 0.07974052429199219, 0.07725738734006882, 0.07954690605401993, 0.08727242052555084, 0.09220349788665771, 0.0856822058558464, 0.0764395147562027, 0.07756195962429047, 0.08695080131292343, 0.08642733842134476, 0.08684969693422318, 0.09041072428226471, 0.08057748526334763, 0.08093384653329849, 0.08226759731769562, 0.08580062538385391, 0.0828024297952652, 0.08302294462919235, 0.08538727462291718, 0.08805994689464569, 0.08507920056581497, 0.08476820588111877, 0.07789111882448196, 0.08279009163379669, 0.08272577822208405, 0.09062409400939941, 0.08886614441871643, 0.07664469629526138, 0.08174654841423035, 0.08304113894701004, 0.08139743655920029, 0.08849399536848068, 0.08211226761341095, 0.07744520157575607, 0.08329042047262192, 0.08534735441207886, 0.0806899294257164, 0.0947694405913353, 0.08663441240787506, 0.08557908236980438, 0.07818471640348434, 0.08393809199333191, 0.08025164902210236, 0.08498747646808624, 0.08129274100065231, 0.0873914510011673, 0.0813664048910141, 0.07939352095127106, 0.0829392671585083, 0.07912489026784897, 0.08524003624916077, 0.09120839089155197, 0.1134997084736824, 0.08908813446760178, 0.08531311899423599, 0.08333821594715118, 0.08783648163080215, 0.08190828561782837, 0.08103536814451218, 0.08082659542560577, 0.07985524833202362, 0.08195072412490845, 0.08946388959884644, 0.08587183803319931, 0.08194251358509064, 0.07995419204235077, 0.08827076852321625, 0.08225194364786148, 0.08048165589570999, 0.07986819744110107, 0.08811020106077194, 0.08638656139373779, 0.0807381346821785, 0.08653193712234497, 0.07994315773248672, 0.08227887004613876, 0.0791918933391571, 0.08417480438947678, 0.08261821419000626, 0.07914599776268005, 0.08658833801746368, 0.07945334166288376, 0.09380161762237549, 0.08018223941326141, 0.07990378886461258, 0.07980791479349136, 0.07997407019138336, 0.07815082371234894, 0.0780019462108612, 0.07960189133882523, 0.0857756957411766, 0.07975131273269653, 0.079898901283741, 0.07674264907836914, 0.08229820430278778, 0.08398478478193283, 0.07747642695903778, 0.08172468841075897, 0.08330237120389938]}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.callbacks.History at 0x7efb7e945290>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjS6VyvtzBCf",
        "outputId": "7f95a474-3144-4e6c-eb2e-00db5bffee10"
      },
      "source": [
        "# full reconstruction long\n",
        "\n",
        "sae = StackedAutoencoder([768, 768], activation='sigmoid')\n",
        "sae.fit(bert_embeddings[:, :768], bert_embeddings[:, :768], verbose=True, epochs=400)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "StackAutoencoder: fitting 1 autoencoders with dimensions [768, 768]\n",
            "\tfitting autoencoder number 0 with dimensions (768, 768)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 1.6330 - val_loss: 0.6175\n",
            "Epoch 2/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.5379 - val_loss: 0.3626\n",
            "Epoch 3/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.3373 - val_loss: 0.2603\n",
            "Epoch 4/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.2484 - val_loss: 0.2082\n",
            "Epoch 5/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1992 - val_loss: 0.1698\n",
            "Epoch 6/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.1661 - val_loss: 0.1454\n",
            "Epoch 7/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1441 - val_loss: 0.1293\n",
            "Epoch 8/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1253 - val_loss: 0.1142\n",
            "Epoch 9/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1109 - val_loss: 0.0983\n",
            "Epoch 10/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0991 - val_loss: 0.0915\n",
            "Epoch 11/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0893 - val_loss: 0.0827\n",
            "Epoch 12/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0825 - val_loss: 0.0763\n",
            "Epoch 13/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0757 - val_loss: 0.0701\n",
            "Epoch 14/400\n",
            "477/477 [==============================] - 11s 23ms/step - loss: 0.0696 - val_loss: 0.0629\n",
            "Epoch 15/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0652 - val_loss: 0.0634\n",
            "Epoch 16/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0624 - val_loss: 0.0578\n",
            "Epoch 17/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0582 - val_loss: 0.0523\n",
            "Epoch 18/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0550 - val_loss: 0.0513\n",
            "Epoch 19/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0520 - val_loss: 0.0504\n",
            "Epoch 20/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0511 - val_loss: 0.0484\n",
            "Epoch 21/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0482 - val_loss: 0.0460\n",
            "Epoch 22/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0454 - val_loss: 0.0462\n",
            "Epoch 23/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0436 - val_loss: 0.0408\n",
            "Epoch 24/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0415 - val_loss: 0.0413\n",
            "Epoch 25/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0414 - val_loss: 0.0399\n",
            "Epoch 26/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0395 - val_loss: 0.0362\n",
            "Epoch 27/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0380 - val_loss: 0.0403\n",
            "Epoch 28/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0368 - val_loss: 0.0330\n",
            "Epoch 29/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0367 - val_loss: 0.0339\n",
            "Epoch 30/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0351 - val_loss: 0.0319\n",
            "Epoch 31/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0353 - val_loss: 0.0385\n",
            "Epoch 32/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0338 - val_loss: 0.0340\n",
            "Epoch 33/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0349 - val_loss: 0.0301\n",
            "Epoch 34/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0315 - val_loss: 0.0301\n",
            "Epoch 35/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0325 - val_loss: 0.0304\n",
            "Epoch 36/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0313 - val_loss: 0.0282\n",
            "Epoch 37/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0307 - val_loss: 0.0305\n",
            "Epoch 38/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0294 - val_loss: 0.0312\n",
            "Epoch 39/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0314 - val_loss: 0.0302\n",
            "Epoch 40/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0295 - val_loss: 0.0273\n",
            "Epoch 41/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0281 - val_loss: 0.0261\n",
            "Epoch 42/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0293 - val_loss: 0.0288\n",
            "Epoch 43/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0281 - val_loss: 0.0260\n",
            "Epoch 44/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0272 - val_loss: 0.0282\n",
            "Epoch 45/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0284 - val_loss: 0.0266\n",
            "Epoch 46/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0283 - val_loss: 0.0333\n",
            "Epoch 47/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0295 - val_loss: 0.0250\n",
            "Epoch 48/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0283 - val_loss: 0.0229\n",
            "Epoch 49/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0259 - val_loss: 0.0256\n",
            "Epoch 50/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0273 - val_loss: 0.0280\n",
            "Epoch 51/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0265 - val_loss: 0.0263\n",
            "Epoch 52/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0267 - val_loss: 0.0230\n",
            "Epoch 53/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0256 - val_loss: 0.0245\n",
            "Epoch 54/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0260 - val_loss: 0.0240\n",
            "Epoch 55/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0266 - val_loss: 0.0258\n",
            "Epoch 56/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0250 - val_loss: 0.0252\n",
            "Epoch 57/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0254 - val_loss: 0.0262\n",
            "Epoch 58/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0254 - val_loss: 0.0243\n",
            "Epoch 59/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0252 - val_loss: 0.0265\n",
            "Epoch 60/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0256 - val_loss: 0.0234\n",
            "Epoch 61/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0236 - val_loss: 0.0219\n",
            "Epoch 62/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0243 - val_loss: 0.0278\n",
            "Epoch 63/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0250 - val_loss: 0.0216\n",
            "Epoch 64/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0230 - val_loss: 0.0229\n",
            "Epoch 65/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0249 - val_loss: 0.0271\n",
            "Epoch 66/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0236 - val_loss: 0.0216\n",
            "Epoch 67/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0237 - val_loss: 0.0241\n",
            "Epoch 68/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0253 - val_loss: 0.0208\n",
            "Epoch 69/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0218 - val_loss: 0.0236\n",
            "Epoch 70/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0240 - val_loss: 0.0207\n",
            "Epoch 71/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0243 - val_loss: 0.0218\n",
            "Epoch 72/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0236 - val_loss: 0.0207\n",
            "Epoch 73/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0234 - val_loss: 0.0234\n",
            "Epoch 74/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0224 - val_loss: 0.0218\n",
            "Epoch 75/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0227 - val_loss: 0.0226\n",
            "Epoch 76/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0244 - val_loss: 0.0224\n",
            "Epoch 77/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0233 - val_loss: 0.0208\n",
            "Epoch 78/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0237 - val_loss: 0.0219\n",
            "Epoch 79/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0242 - val_loss: 0.0199\n",
            "Epoch 80/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0218 - val_loss: 0.0214\n",
            "Epoch 81/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0232 - val_loss: 0.0196\n",
            "Epoch 82/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0229 - val_loss: 0.0207\n",
            "Epoch 83/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0227 - val_loss: 0.0214\n",
            "Epoch 84/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0233 - val_loss: 0.0211\n",
            "Epoch 85/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0225 - val_loss: 0.0203\n",
            "Epoch 86/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0211 - val_loss: 0.0216\n",
            "Epoch 87/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0258 - val_loss: 0.0199\n",
            "Epoch 88/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0217 - val_loss: 0.0221\n",
            "Epoch 89/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0235 - val_loss: 0.0209\n",
            "Epoch 90/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0211 - val_loss: 0.0206\n",
            "Epoch 91/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0228 - val_loss: 0.0249\n",
            "Epoch 92/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0218 - val_loss: 0.0226\n",
            "Epoch 93/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0236 - val_loss: 0.0231\n",
            "Epoch 94/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0210 - val_loss: 0.0229\n",
            "Epoch 95/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0206 - val_loss: 0.0237\n",
            "Epoch 96/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0225 - val_loss: 0.0239\n",
            "Epoch 97/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0208 - val_loss: 0.0209\n",
            "Epoch 98/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0229 - val_loss: 0.0206\n",
            "Epoch 99/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0229 - val_loss: 0.0208\n",
            "Epoch 100/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0206 - val_loss: 0.0280\n",
            "Epoch 101/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0247 - val_loss: 0.0206\n",
            "Epoch 102/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0242 - val_loss: 0.0195\n",
            "Epoch 103/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0211 - val_loss: 0.0233\n",
            "Epoch 104/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0227 - val_loss: 0.0194\n",
            "Epoch 105/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0212 - val_loss: 0.0210\n",
            "Epoch 106/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0210 - val_loss: 0.0232\n",
            "Epoch 107/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0209 - val_loss: 0.0251\n",
            "Epoch 108/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0234 - val_loss: 0.0242\n",
            "Epoch 109/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0232 - val_loss: 0.0182\n",
            "Epoch 110/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0195 - val_loss: 0.0210\n",
            "Epoch 111/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0231 - val_loss: 0.0224\n",
            "Epoch 112/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0232 - val_loss: 0.0179\n",
            "Epoch 113/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0200 - val_loss: 0.0211\n",
            "Epoch 114/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0240 - val_loss: 0.0194\n",
            "Epoch 115/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0204 - val_loss: 0.0194\n",
            "Epoch 116/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0234 - val_loss: 0.0190\n",
            "Epoch 117/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0203 - val_loss: 0.0237\n",
            "Epoch 118/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0227 - val_loss: 0.0286\n",
            "Epoch 119/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0220 - val_loss: 0.0202\n",
            "Epoch 120/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0194 - val_loss: 0.0241\n",
            "Epoch 121/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0207 - val_loss: 0.0211\n",
            "Epoch 122/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0208 - val_loss: 0.0198\n",
            "Epoch 123/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0225 - val_loss: 0.0176\n",
            "Epoch 124/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0223 - val_loss: 0.0195\n",
            "Epoch 125/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0210 - val_loss: 0.0204\n",
            "Epoch 126/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0211 - val_loss: 0.0199\n",
            "Epoch 127/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0209 - val_loss: 0.0217\n",
            "Epoch 128/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0204 - val_loss: 0.0177\n",
            "Epoch 129/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0214 - val_loss: 0.0201\n",
            "Epoch 130/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0209 - val_loss: 0.0246\n",
            "Epoch 131/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0205 - val_loss: 0.0201\n",
            "Epoch 132/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0205 - val_loss: 0.0254\n",
            "Epoch 133/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0208 - val_loss: 0.0215\n",
            "Epoch 134/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0200 - val_loss: 0.0177\n",
            "Epoch 135/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0204 - val_loss: 0.0225\n",
            "Epoch 136/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0208 - val_loss: 0.0185\n",
            "Epoch 137/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0206 - val_loss: 0.0181\n",
            "Epoch 138/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0210 - val_loss: 0.0189\n",
            "Epoch 139/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0208 - val_loss: 0.0175\n",
            "Epoch 140/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0206 - val_loss: 0.0240\n",
            "Epoch 141/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0205 - val_loss: 0.0259\n",
            "Epoch 142/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0214 - val_loss: 0.0217\n",
            "Epoch 143/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0203 - val_loss: 0.0191\n",
            "Epoch 144/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0210 - val_loss: 0.0171\n",
            "Epoch 145/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0194 - val_loss: 0.0173\n",
            "Epoch 146/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0194 - val_loss: 0.0279\n",
            "Epoch 147/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0215 - val_loss: 0.0183\n",
            "Epoch 148/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0212 - val_loss: 0.0191\n",
            "Epoch 149/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0200 - val_loss: 0.0197\n",
            "Epoch 150/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0216 - val_loss: 0.0238\n",
            "Epoch 151/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0202 - val_loss: 0.0202\n",
            "Epoch 152/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0214 - val_loss: 0.0231\n",
            "Epoch 153/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0213 - val_loss: 0.0217\n",
            "Epoch 154/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0231 - val_loss: 0.0176\n",
            "Epoch 155/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0204 - val_loss: 0.0351\n",
            "Epoch 156/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0226 - val_loss: 0.0276\n",
            "Epoch 157/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0211 - val_loss: 0.0192\n",
            "Epoch 158/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0188 - val_loss: 0.0199\n",
            "Epoch 159/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0215 - val_loss: 0.0204\n",
            "Epoch 160/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0210 - val_loss: 0.0199\n",
            "Epoch 161/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0200 - val_loss: 0.0285\n",
            "Epoch 162/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0224 - val_loss: 0.0157\n",
            "Epoch 163/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0195 - val_loss: 0.0173\n",
            "Epoch 164/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0204 - val_loss: 0.0159\n",
            "Epoch 165/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0195 - val_loss: 0.0198\n",
            "Epoch 166/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0221 - val_loss: 0.0178\n",
            "Epoch 167/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0196 - val_loss: 0.0208\n",
            "Epoch 168/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0198 - val_loss: 0.0179\n",
            "Epoch 169/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0197 - val_loss: 0.0201\n",
            "Epoch 170/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0202 - val_loss: 0.0197\n",
            "Epoch 171/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0207 - val_loss: 0.0197\n",
            "Epoch 172/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0195 - val_loss: 0.0193\n",
            "Epoch 173/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0204 - val_loss: 0.0179\n",
            "Epoch 174/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0203 - val_loss: 0.0203\n",
            "Epoch 175/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0194 - val_loss: 0.0237\n",
            "Epoch 176/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0208 - val_loss: 0.0169\n",
            "Epoch 177/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0204 - val_loss: 0.0194\n",
            "Epoch 178/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0192 - val_loss: 0.0178\n",
            "Epoch 179/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0204 - val_loss: 0.0224\n",
            "Epoch 180/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0200 - val_loss: 0.0237\n",
            "Epoch 181/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0206 - val_loss: 0.0214\n",
            "Epoch 182/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0198 - val_loss: 0.0168\n",
            "Epoch 183/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0201 - val_loss: 0.0188\n",
            "Epoch 184/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0186 - val_loss: 0.0183\n",
            "Epoch 185/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0216 - val_loss: 0.0161\n",
            "Epoch 186/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0200 - val_loss: 0.0192\n",
            "Epoch 187/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0189 - val_loss: 0.0193\n",
            "Epoch 188/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0204 - val_loss: 0.0164\n",
            "Epoch 189/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0201 - val_loss: 0.0213\n",
            "Epoch 190/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0202 - val_loss: 0.0184\n",
            "Epoch 191/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0203 - val_loss: 0.0216\n",
            "Epoch 192/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0195 - val_loss: 0.0188\n",
            "Epoch 193/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0193 - val_loss: 0.0218\n",
            "Epoch 194/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0209 - val_loss: 0.0221\n",
            "Epoch 195/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0188 - val_loss: 0.0175\n",
            "Epoch 196/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0202 - val_loss: 0.0185\n",
            "Epoch 197/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0206 - val_loss: 0.0164\n",
            "Epoch 198/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0191 - val_loss: 0.0193\n",
            "Epoch 199/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0191 - val_loss: 0.0195\n",
            "Epoch 200/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0200 - val_loss: 0.0256\n",
            "Epoch 201/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0208 - val_loss: 0.0173\n",
            "Epoch 202/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0204 - val_loss: 0.0183\n",
            "Epoch 203/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0196 - val_loss: 0.0210\n",
            "Epoch 204/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0198 - val_loss: 0.0188\n",
            "Epoch 205/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0199 - val_loss: 0.0317\n",
            "Epoch 206/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0208 - val_loss: 0.0277\n",
            "Epoch 207/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0195 - val_loss: 0.0198\n",
            "Epoch 208/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0187 - val_loss: 0.0224\n",
            "Epoch 209/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0193 - val_loss: 0.0172\n",
            "Epoch 210/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0192 - val_loss: 0.0192\n",
            "Epoch 211/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0209 - val_loss: 0.0176\n",
            "Epoch 212/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0188 - val_loss: 0.0180\n",
            "Epoch 213/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0188 - val_loss: 0.0178\n",
            "Epoch 214/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0191 - val_loss: 0.0157\n",
            "Epoch 215/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0197 - val_loss: 0.0198\n",
            "Epoch 216/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0213 - val_loss: 0.0200\n",
            "Epoch 217/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0191 - val_loss: 0.0213\n",
            "Epoch 218/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0200 - val_loss: 0.0187\n",
            "Epoch 219/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0193 - val_loss: 0.0311\n",
            "Epoch 220/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0195 - val_loss: 0.0164\n",
            "Epoch 221/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0197 - val_loss: 0.0148\n",
            "Epoch 222/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0181 - val_loss: 0.0179\n",
            "Epoch 223/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0180 - val_loss: 0.0200\n",
            "Epoch 224/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0198 - val_loss: 0.0204\n",
            "Epoch 225/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0194 - val_loss: 0.0281\n",
            "Epoch 226/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0208 - val_loss: 0.0186\n",
            "Epoch 227/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0184 - val_loss: 0.0238\n",
            "Epoch 228/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0200 - val_loss: 0.0193\n",
            "Epoch 229/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0194 - val_loss: 0.0279\n",
            "Epoch 230/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0202 - val_loss: 0.0193\n",
            "Epoch 231/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0179 - val_loss: 0.0175\n",
            "Epoch 232/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0202 - val_loss: 0.0178\n",
            "Epoch 233/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0183 - val_loss: 0.0223\n",
            "Epoch 234/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0207 - val_loss: 0.0204\n",
            "Epoch 235/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0205 - val_loss: 0.0163\n",
            "Epoch 236/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0186 - val_loss: 0.0197\n",
            "Epoch 237/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0191 - val_loss: 0.0197\n",
            "Epoch 238/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0190 - val_loss: 0.0214\n",
            "Epoch 239/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0201 - val_loss: 0.0185\n",
            "Epoch 240/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0194 - val_loss: 0.0329\n",
            "Epoch 241/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0203 - val_loss: 0.0231\n",
            "Epoch 242/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0203 - val_loss: 0.0205\n",
            "Epoch 243/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0209 - val_loss: 0.0233\n",
            "Epoch 244/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0203 - val_loss: 0.0193\n",
            "Epoch 245/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0190 - val_loss: 0.0204\n",
            "Epoch 246/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0198 - val_loss: 0.0204\n",
            "Epoch 247/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0187 - val_loss: 0.0185\n",
            "Epoch 248/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0183 - val_loss: 0.0196\n",
            "Epoch 249/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0206 - val_loss: 0.0173\n",
            "Epoch 250/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0189 - val_loss: 0.0181\n",
            "Epoch 251/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0194 - val_loss: 0.0201\n",
            "Epoch 252/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0214 - val_loss: 0.0335\n",
            "Epoch 253/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0197 - val_loss: 0.0183\n",
            "Epoch 254/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0181 - val_loss: 0.0188\n",
            "Epoch 255/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0206 - val_loss: 0.0164\n",
            "Epoch 256/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0200 - val_loss: 0.0165\n",
            "Epoch 257/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0179 - val_loss: 0.0191\n",
            "Epoch 258/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0185 - val_loss: 0.0169\n",
            "Epoch 259/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0195 - val_loss: 0.0156\n",
            "Epoch 260/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0205 - val_loss: 0.0206\n",
            "Epoch 261/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0188 - val_loss: 0.0198\n",
            "Epoch 262/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0188 - val_loss: 0.0209\n",
            "Epoch 263/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0188 - val_loss: 0.0177\n",
            "Epoch 264/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0202 - val_loss: 0.0174\n",
            "Epoch 265/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0183 - val_loss: 0.0205\n",
            "Epoch 266/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0191 - val_loss: 0.0224\n",
            "Epoch 267/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0198 - val_loss: 0.0202\n",
            "Epoch 268/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0201 - val_loss: 0.0229\n",
            "Epoch 269/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0199 - val_loss: 0.0190\n",
            "Epoch 270/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0188 - val_loss: 0.0159\n",
            "Epoch 271/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0185 - val_loss: 0.0204\n",
            "Epoch 272/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0204 - val_loss: 0.0189\n",
            "Epoch 273/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0198 - val_loss: 0.0179\n",
            "Epoch 274/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0199 - val_loss: 0.0161\n",
            "Epoch 275/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0173 - val_loss: 0.0264\n",
            "Epoch 276/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0199 - val_loss: 0.0188\n",
            "Epoch 277/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0202 - val_loss: 0.0198\n",
            "Epoch 278/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0188 - val_loss: 0.0178\n",
            "Epoch 279/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0191 - val_loss: 0.0193\n",
            "Epoch 280/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0186 - val_loss: 0.0197\n",
            "Epoch 281/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0214 - val_loss: 0.0193\n",
            "Epoch 282/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0191 - val_loss: 0.0181\n",
            "Epoch 283/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0191 - val_loss: 0.0206\n",
            "Epoch 284/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0183 - val_loss: 0.0173\n",
            "Epoch 285/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0196 - val_loss: 0.0160\n",
            "Epoch 286/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0188 - val_loss: 0.0184\n",
            "Epoch 287/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0185 - val_loss: 0.0200\n",
            "Epoch 288/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0214 - val_loss: 0.0195\n",
            "Epoch 289/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0194 - val_loss: 0.0187\n",
            "Epoch 290/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0192 - val_loss: 0.0242\n",
            "Epoch 291/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0185 - val_loss: 0.0159\n",
            "Epoch 292/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0196 - val_loss: 0.0205\n",
            "Epoch 293/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0197 - val_loss: 0.0154\n",
            "Epoch 294/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0195 - val_loss: 0.0215\n",
            "Epoch 295/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0205 - val_loss: 0.0177\n",
            "Epoch 296/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0188 - val_loss: 0.0159\n",
            "Epoch 297/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0191 - val_loss: 0.0168\n",
            "Epoch 298/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0200 - val_loss: 0.0157\n",
            "Epoch 299/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0181 - val_loss: 0.0160\n",
            "Epoch 300/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0188 - val_loss: 0.0198\n",
            "Epoch 301/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0186 - val_loss: 0.0182\n",
            "Epoch 302/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0187 - val_loss: 0.0202\n",
            "Epoch 303/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0186 - val_loss: 0.0196\n",
            "Epoch 304/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0181 - val_loss: 0.0191\n",
            "Epoch 305/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0202 - val_loss: 0.0204\n",
            "Epoch 306/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0209 - val_loss: 0.0164\n",
            "Epoch 307/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0182 - val_loss: 0.0176\n",
            "Epoch 308/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0192 - val_loss: 0.0183\n",
            "Epoch 309/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0181 - val_loss: 0.0191\n",
            "Epoch 310/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0187 - val_loss: 0.0175\n",
            "Epoch 311/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0198 - val_loss: 0.0220\n",
            "Epoch 312/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0191 - val_loss: 0.0206\n",
            "Epoch 313/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0188 - val_loss: 0.0200\n",
            "Epoch 314/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0186 - val_loss: 0.0163\n",
            "Epoch 315/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0183 - val_loss: 0.0221\n",
            "Epoch 316/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0196 - val_loss: 0.0195\n",
            "Epoch 317/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0193 - val_loss: 0.0168\n",
            "Epoch 318/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0174 - val_loss: 0.0222\n",
            "Epoch 319/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0199 - val_loss: 0.0155\n",
            "Epoch 320/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0185 - val_loss: 0.0182\n",
            "Epoch 321/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0194 - val_loss: 0.0176\n",
            "Epoch 322/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0193 - val_loss: 0.0151\n",
            "Epoch 323/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0181 - val_loss: 0.0226\n",
            "Epoch 324/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0187 - val_loss: 0.0213\n",
            "Epoch 325/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0181 - val_loss: 0.0175\n",
            "Epoch 326/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0187 - val_loss: 0.0175\n",
            "Epoch 327/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0187 - val_loss: 0.0176\n",
            "Epoch 328/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0194 - val_loss: 0.0161\n",
            "Epoch 329/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0185 - val_loss: 0.0231\n",
            "Epoch 330/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0188 - val_loss: 0.0174\n",
            "Epoch 331/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0178 - val_loss: 0.0169\n",
            "Epoch 332/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0185 - val_loss: 0.0180\n",
            "Epoch 333/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0187 - val_loss: 0.0178\n",
            "Epoch 334/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0175 - val_loss: 0.0213\n",
            "Epoch 335/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0184 - val_loss: 0.0176\n",
            "Epoch 336/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0186 - val_loss: 0.0171\n",
            "Epoch 337/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0199 - val_loss: 0.0202\n",
            "Epoch 338/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0182 - val_loss: 0.0181\n",
            "Epoch 339/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0201 - val_loss: 0.0189\n",
            "Epoch 340/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0193 - val_loss: 0.0170\n",
            "Epoch 341/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0181 - val_loss: 0.0181\n",
            "Epoch 342/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0186 - val_loss: 0.0159\n",
            "Epoch 343/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0193 - val_loss: 0.0185\n",
            "Epoch 344/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0183 - val_loss: 0.0189\n",
            "Epoch 345/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0192 - val_loss: 0.0191\n",
            "Epoch 346/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0195 - val_loss: 0.0298\n",
            "Epoch 347/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0207 - val_loss: 0.0166\n",
            "Epoch 348/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0188 - val_loss: 0.0163\n",
            "Epoch 349/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0185 - val_loss: 0.0170\n",
            "Epoch 350/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0188 - val_loss: 0.0168\n",
            "Epoch 351/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0186 - val_loss: 0.0200\n",
            "Epoch 352/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0179 - val_loss: 0.0168\n",
            "Epoch 353/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0182 - val_loss: 0.0177\n",
            "Epoch 354/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0191 - val_loss: 0.0176\n",
            "Epoch 355/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0188 - val_loss: 0.0160\n",
            "Epoch 356/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0186 - val_loss: 0.0165\n",
            "Epoch 357/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0181 - val_loss: 0.0192\n",
            "Epoch 358/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0183 - val_loss: 0.0169\n",
            "Epoch 359/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0173 - val_loss: 0.0189\n",
            "Epoch 360/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0190 - val_loss: 0.0195\n",
            "Epoch 361/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0195 - val_loss: 0.0180\n",
            "Epoch 362/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0181 - val_loss: 0.0228\n",
            "Epoch 363/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0187 - val_loss: 0.0176\n",
            "Epoch 364/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0185 - val_loss: 0.0184\n",
            "Epoch 365/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0205 - val_loss: 0.0156\n",
            "Epoch 366/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0163 - val_loss: 0.0175\n",
            "Epoch 367/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0198 - val_loss: 0.0212\n",
            "Epoch 368/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0186 - val_loss: 0.0163\n",
            "Epoch 369/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0176 - val_loss: 0.0226\n",
            "Epoch 370/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0198 - val_loss: 0.0204\n",
            "Epoch 371/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0197 - val_loss: 0.0172\n",
            "Epoch 372/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0185 - val_loss: 0.0221\n",
            "Epoch 373/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0183 - val_loss: 0.0261\n",
            "Epoch 374/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0206 - val_loss: 0.0184\n",
            "Epoch 375/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0204 - val_loss: 0.0203\n",
            "Epoch 376/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0185 - val_loss: 0.0168\n",
            "Epoch 377/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0192 - val_loss: 0.0160\n",
            "Epoch 378/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0169 - val_loss: 0.0153\n",
            "Epoch 379/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0180 - val_loss: 0.0190\n",
            "Epoch 380/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0193 - val_loss: 0.0192\n",
            "Epoch 381/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0182 - val_loss: 0.0182\n",
            "Epoch 382/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0209 - val_loss: 0.0219\n",
            "Epoch 383/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0173 - val_loss: 0.0167\n",
            "Epoch 384/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0193 - val_loss: 0.0155\n",
            "Epoch 385/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0180 - val_loss: 0.0175\n",
            "Epoch 386/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0179 - val_loss: 0.0211\n",
            "Epoch 387/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0185 - val_loss: 0.0206\n",
            "Epoch 388/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0185 - val_loss: 0.0240\n",
            "Epoch 389/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0187 - val_loss: 0.0203\n",
            "Epoch 390/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0192 - val_loss: 0.0183\n",
            "Epoch 391/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0180 - val_loss: 0.0184\n",
            "Epoch 392/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0194 - val_loss: 0.0167\n",
            "Epoch 393/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0185 - val_loss: 0.0168\n",
            "Epoch 394/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0198 - val_loss: 0.0217\n",
            "Epoch 395/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0201 - val_loss: 0.0194\n",
            "Epoch 396/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0184 - val_loss: 0.0161\n",
            "Epoch 397/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0177 - val_loss: 0.0178\n",
            "Epoch 398/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0205 - val_loss: 0.0174\n",
            "Epoch 399/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0177 - val_loss: 0.0187\n",
            "Epoch 400/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0192 - val_loss: 0.0155\n",
            "StackedAutoencoder: autoencoder 0 - model history\n",
            "{'loss': [1.1035516262054443, 0.47192177176475525, 0.3119637668132782, 0.23622751235961914, 0.1917114555835724, 0.16114911437034607, 0.1391921192407608, 0.12224755436182022, 0.10819417983293533, 0.09793248772621155, 0.08850177377462387, 0.08199182897806168, 0.07608694583177567, 0.06961613148450851, 0.06491804867982864, 0.06144417077302933, 0.057560503482818604, 0.0547330379486084, 0.05191752687096596, 0.04994980990886688, 0.04755377024412155, 0.045794300734996796, 0.04420003294944763, 0.04161643609404564, 0.04125934839248657, 0.039110925048589706, 0.03853825479745865, 0.03706479072570801, 0.03681306913495064, 0.035085733979940414, 0.03534499555826187, 0.033770594745874405, 0.03481728956103325, 0.031440213322639465, 0.032494038343429565, 0.03141355141997337, 0.03150391951203346, 0.02954334020614624, 0.031206056475639343, 0.029844289645552635, 0.028366070240736008, 0.029671723023056984, 0.028528714552521706, 0.02774898149073124, 0.02780933491885662, 0.02858550474047661, 0.028272056952118874, 0.027632012963294983, 0.026768632233142853, 0.028068047016859055, 0.026348214596509933, 0.027080398052930832, 0.026133200153708458, 0.026283850893378258, 0.02659907005727291, 0.025206509977579117, 0.02573843114078045, 0.025527017191052437, 0.025294888764619827, 0.026393594220280647, 0.024649685248732567, 0.024619299918413162, 0.02536958083510399, 0.024031970649957657, 0.025222068652510643, 0.024338513612747192, 0.023760158568620682, 0.025959668681025505, 0.02319658361375332, 0.024168668314814568, 0.02416393905878067, 0.02367989718914032, 0.02499360963702202, 0.022126080468297005, 0.02319497987627983, 0.0236746147274971, 0.023539112880825996, 0.023806193843483925, 0.023906247690320015, 0.022560609504580498, 0.023600682616233826, 0.02318575792014599, 0.023306021466851234, 0.022852139547467232, 0.023495325818657875, 0.021490618586540222, 0.024438994005322456, 0.022282717749476433, 0.023080987855792046, 0.021640092134475708, 0.023334667086601257, 0.022525852546095848, 0.024117304012179375, 0.022070353850722313, 0.02137349359691143, 0.022946346551179886, 0.021157272160053253, 0.022450681775808334, 0.02281108684837818, 0.021258119493722916, 0.022167984396219254, 0.023508692160248756, 0.02126234956085682, 0.022619538009166718, 0.02103300765156746, 0.022059137001633644, 0.021044664084911346, 0.022932086139917374, 0.02265229821205139, 0.020587000995874405, 0.02157195843756199, 0.022146645933389664, 0.021264322102069855, 0.02359042875468731, 0.02112716995179653, 0.022461652755737305, 0.020349182188510895, 0.02191188745200634, 0.02183287963271141, 0.02103658951818943, 0.021247498691082, 0.02101258374750614, 0.02261453866958618, 0.021670551970601082, 0.021285517141222954, 0.02092192880809307, 0.021486172452569008, 0.021003739908337593, 0.021528102457523346, 0.021584579721093178, 0.020783796906471252, 0.020716499537229538, 0.022068681195378304, 0.020373547449707985, 0.021641690284013748, 0.020791491493582726, 0.02129761129617691, 0.021310102194547653, 0.020747143775224686, 0.02051592245697975, 0.02058013714849949, 0.02103845402598381, 0.02142728492617607, 0.0207821112126112, 0.020475337281823158, 0.020788710564374924, 0.020683644339442253, 0.021184071898460388, 0.020274516195058823, 0.021491559222340584, 0.019814016297459602, 0.021103017032146454, 0.02087077870965004, 0.021225731819868088, 0.02107178047299385, 0.020399227738380432, 0.021365344524383545, 0.019441578537225723, 0.02079065330326557, 0.020713578909635544, 0.020153874531388283, 0.021792270243167877, 0.020097753033041954, 0.020173262804746628, 0.020380426198244095, 0.0206705704331398, 0.02025531232357025, 0.020032158121466637, 0.019731003791093826, 0.021226124837994576, 0.020963482558727264, 0.01956797018647194, 0.020459981635212898, 0.019976181909441948, 0.020845981314778328, 0.019896233454346657, 0.020360486581921577, 0.0192550215870142, 0.020681271329522133, 0.02007065713405609, 0.02058587595820427, 0.019793661311268806, 0.02086310088634491, 0.019445668905973434, 0.02162524126470089, 0.02068173885345459, 0.019296186044812202, 0.02005983702838421, 0.02041645348072052, 0.01970251277089119, 0.021489186212420464, 0.019819296896457672, 0.020096521824598312, 0.020557940006256104, 0.01951873116195202, 0.020411992445588112, 0.02089814841747284, 0.020206790417432785, 0.019719356670975685, 0.01976684294641018, 0.0198585856705904, 0.021092530339956284, 0.019487107172608376, 0.019619347527623177, 0.020381996408104897, 0.01959878019988537, 0.019971488043665886, 0.019562480971217155, 0.019694624468684196, 0.01963411271572113, 0.020786039531230927, 0.018975986167788506, 0.019853221252560616, 0.019469784572720528, 0.019584063440561295, 0.020353982225060463, 0.020433498546481133, 0.019287491217255592, 0.020112523809075356, 0.019335664808750153, 0.020641803741455078, 0.01929653063416481, 0.019312463700771332, 0.01969783566892147, 0.019820861518383026, 0.019991319626569748, 0.0197449978441, 0.019789867103099823, 0.020746828988194466, 0.0196395143866539, 0.018390271812677383, 0.01953463815152645, 0.019302215427160263, 0.01982436142861843, 0.020203467458486557, 0.019750481471419334, 0.019085027277469635, 0.019100304692983627, 0.019524862989783287, 0.01942511647939682, 0.01936815120279789, 0.01996784098446369, 0.019413795322179794, 0.019749455153942108, 0.01950559951364994, 0.02001572772860527, 0.01849868893623352, 0.019113099202513695, 0.01993446797132492, 0.019246965646743774, 0.019737951457500458, 0.020682234317064285, 0.01898382045328617, 0.018244311213493347, 0.020690640434622765, 0.019263455644249916, 0.019098613411188126, 0.019075442105531693, 0.01955943927168846, 0.020541295409202576, 0.018585288897156715, 0.0202761422842741, 0.01957443170249462, 0.020165983587503433, 0.018505854532122612, 0.020091485232114792, 0.01906026341021061, 0.019598308950662613, 0.019851239398121834, 0.01910209283232689, 0.01913057640194893, 0.019648263230919838, 0.019452109932899475, 0.020472561940550804, 0.01872059889137745, 0.018862683326005936, 0.02023349702358246, 0.018757808953523636, 0.01917329616844654, 0.01902160234749317, 0.02035350911319256, 0.019224749878048897, 0.019155427813529968, 0.018403202295303345, 0.020434513688087463, 0.018754882737994194, 0.018854228779673576, 0.02019127458333969, 0.018840590491890907, 0.0194034893065691, 0.018450817093253136, 0.020176991820335388, 0.018655233085155487, 0.019591089338064194, 0.019694959744811058, 0.01864032633602619, 0.019184982404112816, 0.01978163607418537, 0.018624745309352875, 0.01906665414571762, 0.019559668377041817, 0.018875710666179657, 0.020379364490509033, 0.018145425245165825, 0.01989995874464512, 0.01910438947379589, 0.017977381125092506, 0.02074863202869892, 0.019367041066288948, 0.01911247707903385, 0.019269665703177452, 0.019050879403948784, 0.019005443900823593, 0.019170986488461494, 0.019073640927672386, 0.019265834242105484, 0.020321965217590332, 0.018422698602080345, 0.019276704639196396, 0.019093075767159462, 0.0187788438051939, 0.019551541656255722, 0.018289510160684586, 0.019629666581749916, 0.01820470578968525, 0.01987069845199585, 0.018755923956632614, 0.019915055483579636, 0.019624117761850357, 0.01918713003396988, 0.01866769790649414, 0.019119827076792717, 0.019350707530975342, 0.018502892926335335, 0.019185591489076614, 0.01853247918188572, 0.01944669894874096, 0.018735099583864212, 0.019762184470891953, 0.019468802958726883, 0.01900508441030979, 0.018415922299027443, 0.02024771086871624, 0.018116554245352745, 0.019276997074484825, 0.018787115812301636, 0.019507626071572304, 0.018652349710464478, 0.018883051350712776, 0.019161127507686615, 0.019565967842936516, 0.01887759193778038, 0.018994107842445374, 0.018664825707674026, 0.019534921273589134, 0.018552357330918312, 0.018957223743200302, 0.018784180283546448, 0.017815161496400833, 0.019234735518693924, 0.01919688656926155, 0.01861591823399067, 0.01877184771001339, 0.019021756947040558, 0.02089712582528591, 0.017490722239017487, 0.019654186442494392, 0.019076773896813393, 0.017754489555954933, 0.01969258487224579, 0.018368607386946678, 0.01926265098154545, 0.01848289556801319, 0.019205253571271896, 0.01946568675339222, 0.01781248301267624, 0.02077314257621765, 0.01744789630174637, 0.018319187685847282, 0.019446007907390594, 0.01881641149520874, 0.021640349179506302, 0.017730340361595154, 0.01860286109149456, 0.019159452989697456, 0.018175724893808365, 0.019202426075935364, 0.018016589805483818, 0.018217554315924644, 0.019532067701220512, 0.01916821300983429, 0.018144408240914345, 0.018834592774510384, 0.01945497654378414, 0.019899314269423485, 0.018751418218016624, 0.018313564360141754, 0.019392898306250572, 0.018420686945319176, 0.019273661077022552], 'val_loss': [0.6174629926681519, 0.3625529408454895, 0.26034632325172424, 0.2081880122423172, 0.16976836323738098, 0.145424485206604, 0.12931860983371735, 0.11418124288320541, 0.0983075350522995, 0.0915306806564331, 0.08267927169799805, 0.0762781873345375, 0.07014190405607224, 0.0628608763217926, 0.06342484056949615, 0.05784612521529198, 0.05232354253530502, 0.05133137106895447, 0.05036244913935661, 0.04836876690387726, 0.04595030099153519, 0.04618553817272186, 0.040821172297000885, 0.04133855551481247, 0.03987319767475128, 0.03620826080441475, 0.04030904173851013, 0.0329926498234272, 0.0339292548596859, 0.031942687928676605, 0.03851453214883804, 0.034012846648693085, 0.030097341164946556, 0.03014492802321911, 0.030424533411860466, 0.028210405260324478, 0.030465424060821533, 0.031164728105068207, 0.03016088344156742, 0.02729662135243416, 0.0260673388838768, 0.028780993074178696, 0.02595183253288269, 0.028189798817038536, 0.026553280651569366, 0.03329470753669739, 0.025004813447594643, 0.022907858714461327, 0.025587568059563637, 0.028026701882481575, 0.02629457786679268, 0.02300695888698101, 0.024471299722790718, 0.024021601304411888, 0.025776037946343422, 0.025173258036375046, 0.026227589696645737, 0.024281691759824753, 0.026485972106456757, 0.023368792608380318, 0.021948926150798798, 0.027795571833848953, 0.021586965769529343, 0.022939542308449745, 0.027060389518737793, 0.02156206965446472, 0.02412717230618, 0.02075986936688423, 0.02358093671500683, 0.020689943805336952, 0.02175106853246689, 0.020696960389614105, 0.023388389497995377, 0.021794937551021576, 0.022647274658083916, 0.022438637912273407, 0.020788725465536118, 0.021851565688848495, 0.019935395568609238, 0.021435236558318138, 0.01957368478178978, 0.0207147728651762, 0.021364225074648857, 0.021104024723172188, 0.020279457792639732, 0.02164955995976925, 0.019917385652661324, 0.022098442539572716, 0.020859427750110626, 0.020591869950294495, 0.024893274530768394, 0.02257922664284706, 0.023107076063752174, 0.022866234183311462, 0.023715484887361526, 0.023884722962975502, 0.020896872505545616, 0.020644130185246468, 0.020756756886839867, 0.028035292401909828, 0.020592881366610527, 0.019545461982488632, 0.023318767547607422, 0.01944315806031227, 0.020951610058546066, 0.02322150394320488, 0.0251464881002903, 0.024204324930906296, 0.018167126923799515, 0.021014895290136337, 0.02237953059375286, 0.017918461933732033, 0.021084140986204147, 0.019400382414460182, 0.019398363307118416, 0.01895732991397381, 0.02373036928474903, 0.0286149550229311, 0.020247362554073334, 0.024054350331425667, 0.021087730303406715, 0.01983281783759594, 0.01756245456635952, 0.019470203667879105, 0.020352641120553017, 0.019909970462322235, 0.02174178883433342, 0.017727894708514214, 0.02008500136435032, 0.024585844948887825, 0.020127911120653152, 0.025402208790183067, 0.021534843370318413, 0.017702287063002586, 0.022466570138931274, 0.018469426780939102, 0.018090875819325447, 0.01886889524757862, 0.017460141330957413, 0.02399766445159912, 0.025914838537573814, 0.021732185035943985, 0.019071580842137337, 0.017105981707572937, 0.01730087399482727, 0.027945172041654587, 0.018284449353814125, 0.019056472927331924, 0.01966623030602932, 0.023818323388695717, 0.020173711702227592, 0.023091591894626617, 0.021679973229765892, 0.01757850870490074, 0.03505878150463104, 0.027570677921175957, 0.01917465403676033, 0.019853174686431885, 0.020365005359053612, 0.019886888563632965, 0.028521418571472168, 0.01568465866148472, 0.017315762117505074, 0.015876691788434982, 0.01978970505297184, 0.017847105860710144, 0.02078317664563656, 0.0179289560765028, 0.02013176679611206, 0.019689321517944336, 0.0197139959782362, 0.019305843859910965, 0.01791548542678356, 0.020250270143151283, 0.023738021031022072, 0.016870642080903053, 0.019445564597845078, 0.01779424585402012, 0.022396951913833618, 0.0236886665225029, 0.021376661956310272, 0.01677115075290203, 0.018792355433106422, 0.018317345529794693, 0.016058925539255142, 0.01920367404818535, 0.019315944984555244, 0.01635923981666565, 0.02130962908267975, 0.01839333027601242, 0.021622449159622192, 0.018827024847269058, 0.021828841418027878, 0.0221156757324934, 0.017498165369033813, 0.01851871982216835, 0.016384253278374672, 0.019330637529492378, 0.019506417214870453, 0.02555319294333458, 0.017336277291178703, 0.01827911287546158, 0.02102859504520893, 0.018769022077322006, 0.03173229098320007, 0.027660751715302467, 0.019803479313850403, 0.022409873083233833, 0.017152339220046997, 0.01918279565870762, 0.017578324303030968, 0.01804411970078945, 0.017832474783062935, 0.01569466106593609, 0.019774245098233223, 0.01997074857354164, 0.02126852609217167, 0.01866026595234871, 0.031130541115999222, 0.016421694308519363, 0.014764674007892609, 0.017931995913386345, 0.019985828548669815, 0.020378654822707176, 0.02811233513057232, 0.018622683361172676, 0.023825043812394142, 0.019340671598911285, 0.027922775596380234, 0.019282260909676552, 0.017512554302811623, 0.017838092520833015, 0.022286901250481606, 0.02038140594959259, 0.016252007335424423, 0.01969558373093605, 0.019673993811011314, 0.02144971489906311, 0.018526382744312286, 0.03285149112343788, 0.0230525191873312, 0.020491600036621094, 0.023306887596845627, 0.01932583376765251, 0.020397743210196495, 0.02035779133439064, 0.018514325842261314, 0.019580809399485588, 0.017300548031926155, 0.01808195374906063, 0.020136989653110504, 0.033496398478746414, 0.018316954374313354, 0.01881675235927105, 0.016433749347925186, 0.016459526494145393, 0.01908876933157444, 0.016948269680142403, 0.015646222978830338, 0.020612139254808426, 0.019798651337623596, 0.02085171826183796, 0.017731675878167152, 0.01737181469798088, 0.020458998158574104, 0.022396685555577278, 0.02023329958319664, 0.02290363796055317, 0.019030461087822914, 0.015930984169244766, 0.020410679280757904, 0.01887909695506096, 0.01793690025806427, 0.016110077500343323, 0.02643900364637375, 0.018762586638331413, 0.019796282052993774, 0.017751608043909073, 0.0192907452583313, 0.019703412428498268, 0.019325178116559982, 0.018050383776426315, 0.02055443450808525, 0.01728823222219944, 0.016034109517931938, 0.018350740894675255, 0.019959816709160805, 0.019545087590813637, 0.018680747598409653, 0.02416183426976204, 0.01592191867530346, 0.020471669733524323, 0.015410845167934895, 0.02152782864868641, 0.017681652680039406, 0.01591252163052559, 0.01676676981151104, 0.015666920691728592, 0.016003016382455826, 0.019823256880044937, 0.01818501017987728, 0.02023284323513508, 0.019568422809243202, 0.019136982038617134, 0.020390963181853294, 0.01644746959209442, 0.01755661889910698, 0.01833777129650116, 0.0190624687820673, 0.0174795500934124, 0.021964056417346, 0.02055545151233673, 0.02003556862473488, 0.01631757989525795, 0.02212248370051384, 0.019504982978105545, 0.016808094456791878, 0.022240376099944115, 0.015488389879465103, 0.018186045810580254, 0.017622755840420723, 0.015116819180548191, 0.02256707288324833, 0.021295232698321342, 0.017485348507761955, 0.017519714310765266, 0.01762988604605198, 0.016147056594491005, 0.023108405992388725, 0.01743936538696289, 0.01692892424762249, 0.018001658841967583, 0.0178176648914814, 0.021281648427248, 0.017598740756511688, 0.017086103558540344, 0.020241308957338333, 0.01807600073516369, 0.018868321552872658, 0.016964133828878403, 0.018111946061253548, 0.01590089127421379, 0.018480973318219185, 0.018914418295025826, 0.01905340701341629, 0.029807399958372116, 0.01657470315694809, 0.016326086595654488, 0.01698010042309761, 0.016772419214248657, 0.01998383179306984, 0.01684568077325821, 0.017661523073911667, 0.017567463219165802, 0.01597725972533226, 0.01648849807679653, 0.019181177020072937, 0.0169301088899374, 0.018865296617150307, 0.019481085240840912, 0.01798751950263977, 0.022787019610404968, 0.01756853051483631, 0.018389055505394936, 0.015565652400255203, 0.017514187842607498, 0.02122471109032631, 0.016252426430583, 0.022595230489969254, 0.020409056916832924, 0.01716693677008152, 0.02208731323480606, 0.0260885339230299, 0.018434375524520874, 0.020304206758737564, 0.01683845929801464, 0.015950601547956467, 0.015256566926836967, 0.018969198688864708, 0.01916925050318241, 0.018239891156554222, 0.02185439132153988, 0.016712380573153496, 0.015493078157305717, 0.01745162159204483, 0.02108410745859146, 0.020615672692656517, 0.024001728743314743, 0.020289456471800804, 0.01827242784202099, 0.018371734768152237, 0.016722068190574646, 0.016773998737335205, 0.021711787208914757, 0.01942378841340542, 0.016069795936346054, 0.01783016510307789, 0.017361897975206375, 0.018711958080530167, 0.015528084710240364]}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.callbacks.History at 0x7efb7e924a90>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ImSaS-mfzDHk",
        "outputId": "fb93e7aa-e0a2-4afb-ea72-d8840bf15515"
      },
      "source": [
        "# full reconstruction long\n",
        "\n",
        "sae = StackedAutoencoder([768, 768], activation='LeakyReLU')\n",
        "sae.fit(bert_embeddings[:, :768], bert_embeddings[:, :768], verbose=True, epochs=400)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "StackAutoencoder: fitting 1 autoencoders with dimensions [768, 768]\n",
            "\tfitting autoencoder number 0 with dimensions (768, 768)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/400\n",
            "477/477 [==============================] - 53s 20ms/step - loss: 1.3385 - val_loss: 0.2750\n",
            "Epoch 2/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.2324 - val_loss: 0.1470\n",
            "Epoch 3/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.1340 - val_loss: 0.0964\n",
            "Epoch 4/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0922 - val_loss: 0.0753\n",
            "Epoch 5/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0712 - val_loss: 0.0568\n",
            "Epoch 6/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0546 - val_loss: 0.0451\n",
            "Epoch 7/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0451 - val_loss: 0.0386\n",
            "Epoch 8/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0363 - val_loss: 0.0302\n",
            "Epoch 9/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0309 - val_loss: 0.0306\n",
            "Epoch 10/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0281 - val_loss: 0.0215\n",
            "Epoch 11/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0234 - val_loss: 0.0188\n",
            "Epoch 12/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0220 - val_loss: 0.0162\n",
            "Epoch 13/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0178 - val_loss: 0.0160\n",
            "Epoch 14/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0164 - val_loss: 0.0151\n",
            "Epoch 15/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0156 - val_loss: 0.0178\n",
            "Epoch 16/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0153 - val_loss: 0.0146\n",
            "Epoch 17/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0146 - val_loss: 0.0108\n",
            "Epoch 18/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0127 - val_loss: 0.0108\n",
            "Epoch 19/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0111 - val_loss: 0.0118\n",
            "Epoch 20/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0116 - val_loss: 0.0145\n",
            "Epoch 21/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0118 - val_loss: 0.0071\n",
            "Epoch 22/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0106 - val_loss: 0.0086\n",
            "Epoch 23/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0096 - val_loss: 0.0097\n",
            "Epoch 24/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0102 - val_loss: 0.0090\n",
            "Epoch 25/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0101 - val_loss: 0.0100\n",
            "Epoch 26/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0086 - val_loss: 0.0082\n",
            "Epoch 27/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0092 - val_loss: 0.0114\n",
            "Epoch 28/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0086 - val_loss: 0.0111\n",
            "Epoch 29/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0093 - val_loss: 0.0080\n",
            "Epoch 30/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0083 - val_loss: 0.0108\n",
            "Epoch 31/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0094 - val_loss: 0.0077\n",
            "Epoch 32/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0091 - val_loss: 0.0065\n",
            "Epoch 33/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0089 - val_loss: 0.0087\n",
            "Epoch 34/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0088 - val_loss: 0.0079\n",
            "Epoch 35/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0091 - val_loss: 0.0066\n",
            "Epoch 36/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0082 - val_loss: 0.0097\n",
            "Epoch 37/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0080 - val_loss: 0.0059\n",
            "Epoch 38/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0079 - val_loss: 0.0154\n",
            "Epoch 39/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0086 - val_loss: 0.0063\n",
            "Epoch 40/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0076 - val_loss: 0.0051\n",
            "Epoch 41/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0099 - val_loss: 0.0077\n",
            "Epoch 42/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0081 - val_loss: 0.0117\n",
            "Epoch 43/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0083 - val_loss: 0.0088\n",
            "Epoch 44/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0089 - val_loss: 0.0079\n",
            "Epoch 45/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0074 - val_loss: 0.0075\n",
            "Epoch 46/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0072 - val_loss: 0.0069\n",
            "Epoch 47/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0083 - val_loss: 0.0072\n",
            "Epoch 48/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0074 - val_loss: 0.0068\n",
            "Epoch 49/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0075 - val_loss: 0.0077\n",
            "Epoch 50/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0069 - val_loss: 0.0064\n",
            "Epoch 51/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0075 - val_loss: 0.0081\n",
            "Epoch 52/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0076 - val_loss: 0.0059\n",
            "Epoch 53/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0082 - val_loss: 0.0066\n",
            "Epoch 54/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0077 - val_loss: 0.0079\n",
            "Epoch 55/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0070 - val_loss: 0.0071\n",
            "Epoch 56/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0078 - val_loss: 0.0067\n",
            "Epoch 57/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0078 - val_loss: 0.0048\n",
            "Epoch 58/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0068 - val_loss: 0.0063\n",
            "Epoch 59/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0079 - val_loss: 0.0073\n",
            "Epoch 60/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0072 - val_loss: 0.0053\n",
            "Epoch 61/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0069 - val_loss: 0.0070\n",
            "Epoch 62/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0072 - val_loss: 0.0058\n",
            "Epoch 63/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0065 - val_loss: 0.0084\n",
            "Epoch 64/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0079 - val_loss: 0.0093\n",
            "Epoch 65/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0086 - val_loss: 0.0059\n",
            "Epoch 66/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0067 - val_loss: 0.0064\n",
            "Epoch 67/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0066 - val_loss: 0.0079\n",
            "Epoch 68/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0074 - val_loss: 0.0047\n",
            "Epoch 69/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0068 - val_loss: 0.0049\n",
            "Epoch 70/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0069 - val_loss: 0.0060\n",
            "Epoch 71/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0072 - val_loss: 0.0050\n",
            "Epoch 72/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0068 - val_loss: 0.0059\n",
            "Epoch 73/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0071 - val_loss: 0.0065\n",
            "Epoch 74/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0073 - val_loss: 0.0055\n",
            "Epoch 75/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0070 - val_loss: 0.0075\n",
            "Epoch 76/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0072 - val_loss: 0.0055\n",
            "Epoch 77/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0073 - val_loss: 0.0059\n",
            "Epoch 78/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0061 - val_loss: 0.0127\n",
            "Epoch 79/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0074 - val_loss: 0.0050\n",
            "Epoch 80/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0074 - val_loss: 0.0057\n",
            "Epoch 81/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0074 - val_loss: 0.0175\n",
            "Epoch 82/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0079 - val_loss: 0.0054\n",
            "Epoch 83/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0063 - val_loss: 0.0074\n",
            "Epoch 84/400\n",
            "477/477 [==============================] - 10s 20ms/step - loss: 0.0076 - val_loss: 0.0048\n",
            "Epoch 85/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0065 - val_loss: 0.0052\n",
            "Epoch 86/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0068 - val_loss: 0.0119\n",
            "Epoch 87/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0066 - val_loss: 0.0065\n",
            "Epoch 88/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0067 - val_loss: 0.0057\n",
            "Epoch 89/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0062 - val_loss: 0.0067\n",
            "Epoch 90/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0069 - val_loss: 0.0099\n",
            "Epoch 91/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0074 - val_loss: 0.0066\n",
            "Epoch 92/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0062 - val_loss: 0.0054\n",
            "Epoch 93/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0065 - val_loss: 0.0050\n",
            "Epoch 94/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0065 - val_loss: 0.0065\n",
            "Epoch 95/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0069 - val_loss: 0.0062\n",
            "Epoch 96/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0061 - val_loss: 0.0075\n",
            "Epoch 97/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0060 - val_loss: 0.0043\n",
            "Epoch 98/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0067 - val_loss: 0.0057\n",
            "Epoch 99/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0067 - val_loss: 0.0035\n",
            "Epoch 100/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0064 - val_loss: 0.0057\n",
            "Epoch 101/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0067 - val_loss: 0.0075\n",
            "Epoch 102/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0069 - val_loss: 0.0064\n",
            "Epoch 103/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0064 - val_loss: 0.0063\n",
            "Epoch 104/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0065 - val_loss: 0.0051\n",
            "Epoch 105/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0067 - val_loss: 0.0058\n",
            "Epoch 106/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0066 - val_loss: 0.0054\n",
            "Epoch 107/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0061 - val_loss: 0.0079\n",
            "Epoch 108/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0072 - val_loss: 0.0060\n",
            "Epoch 109/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0062 - val_loss: 0.0066\n",
            "Epoch 110/400\n",
            "477/477 [==============================] - 10s 21ms/step - loss: 0.0068 - val_loss: 0.0086\n",
            "Epoch 111/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0075 - val_loss: 0.0127\n",
            "Epoch 112/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0066 - val_loss: 0.0057\n",
            "Epoch 113/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0063 - val_loss: 0.0078\n",
            "Epoch 114/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0068 - val_loss: 0.0104\n",
            "Epoch 115/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0069 - val_loss: 0.0062\n",
            "Epoch 116/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0064 - val_loss: 0.0071\n",
            "Epoch 117/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0073 - val_loss: 0.0069\n",
            "Epoch 118/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0068 - val_loss: 0.0061\n",
            "Epoch 119/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0062 - val_loss: 0.0090\n",
            "Epoch 120/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0068 - val_loss: 0.0050\n",
            "Epoch 121/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0061 - val_loss: 0.0062\n",
            "Epoch 122/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0065 - val_loss: 0.0046\n",
            "Epoch 123/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0059 - val_loss: 0.0094\n",
            "Epoch 124/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0066 - val_loss: 0.0067\n",
            "Epoch 125/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0064 - val_loss: 0.0056\n",
            "Epoch 126/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0065 - val_loss: 0.0052\n",
            "Epoch 127/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0063 - val_loss: 0.0051\n",
            "Epoch 128/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0065 - val_loss: 0.0071\n",
            "Epoch 129/400\n",
            "477/477 [==============================] - 9s 19ms/step - loss: 0.0067 - val_loss: 0.0059\n",
            "Epoch 130/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0066 - val_loss: 0.0048\n",
            "Epoch 131/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0062 - val_loss: 0.0062\n",
            "Epoch 132/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0058 - val_loss: 0.0048\n",
            "Epoch 133/400\n",
            "477/477 [==============================] - 9s 20ms/step - loss: 0.0058 - val_loss: 0.0061\n",
            "Epoch 134/400\n",
            "476/477 [============================>.] - ETA: 0s - loss: 0.0061"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-54dcb51b1a82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStackedAutoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LeakyReLU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-88-224d6c8d7d30>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, val_data, verbose, epochs)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data_reduced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_reduced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                 \u001b[0;31m# callbacks=[tensorboard]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1198\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m               \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m               _use_cached_eval_dataset=True)\n\u001b[0m\u001b[1;32m   1201\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1464\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1465\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlE62yAwRH8D",
        "outputId": "ae822524-f04b-47df-90fa-b7652048a217"
      },
      "source": [
        "autoencoder.fit(bert_embeddings, bert_embeddings, epochs=6, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "477/477 [==============================] - 4s 7ms/step - loss: 0.5589\n",
            "Epoch 2/6\n",
            "477/477 [==============================] - 3s 7ms/step - loss: 0.5312\n",
            "Epoch 3/6\n",
            "477/477 [==============================] - 3s 7ms/step - loss: 0.5092\n",
            "Epoch 4/6\n",
            "477/477 [==============================] - 3s 6ms/step - loss: 0.4897\n",
            "Epoch 5/6\n",
            "477/477 [==============================] - 3s 6ms/step - loss: 0.4770\n",
            "Epoch 6/6\n",
            "477/477 [==============================] - 4s 8ms/step - loss: 0.4650\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efbd8bd0b10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tjFXz2vuYtZ"
      },
      "source": [
        "#### MNIST Autoencoder quality test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pclXWXVluZkw",
        "outputId": "93a23721-d13a-4d3a-ba34-18711ade6099"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(X_train, _), (X_test, _) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
        "X_train = X_train.astype(\"float32\")/255.\n",
        "X_test = X_test.astype(\"float32\")/255.\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "X_train = X_train.reshape((len(X_train), np.prod(X_train.shape[1:])))\n",
        "X_test = X_test.reshape((len(X_test), np.prod(X_test.shape[1:])))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "X_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXQQsD5suiA3",
        "outputId": "f49dea50-cc24-47d5-a40c-fb0203ad7987"
      },
      "source": [
        "sae_mnist = StackedAutoencoder([784, 64])\n",
        "sae_mnist.fit(X_train, X_test, verbose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "StackAutoencoder: fitting 1 autoencoders with dimensions [784, 64]\n",
            "\tfitting autoencoder number 0 with dimensions (784, 64)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "938/938 [==============================] - 61s 65ms/step - loss: 0.0356 - val_loss: 0.0110\n",
            "Epoch 2/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0108 - val_loss: 0.0099\n",
            "Epoch 3/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0100 - val_loss: 0.0096\n",
            "Epoch 4/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0098 - val_loss: 0.0095\n",
            "Epoch 5/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0097 - val_loss: 0.0094\n",
            "Epoch 6/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0096 - val_loss: 0.0093\n",
            "Epoch 7/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0096 - val_loss: 0.0093\n",
            "Epoch 8/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0095 - val_loss: 0.0093\n",
            "Epoch 9/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0096 - val_loss: 0.0093\n",
            "Epoch 10/10\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0095 - val_loss: 0.0093\n",
            "StackedAutoencoder: autoencoder 0 - model history\n",
            "{'loss': [0.02103051356971264, 0.010550692677497864, 0.00998113676905632, 0.009789278730750084, 0.009679277427494526, 0.009621988981962204, 0.00958932563662529, 0.009563825093209743, 0.009550974704325199, 0.009523743763566017], 'val_loss': [0.011023144237697124, 0.009858118370175362, 0.009594621136784554, 0.009482171386480331, 0.009380010887980461, 0.009341046214103699, 0.009307993575930595, 0.0092721963301301, 0.00927059631794691, 0.009290860034525394]}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.callbacks.History at 0x7efb79f05ad0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIDGeJKlW0Aw"
      },
      "source": [
        "#### Перебор для перехода 768 -> 600"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QFBmGspKu46x",
        "outputId": "4fcee620-7c9f-425e-d85d-61257cc34859"
      },
      "source": [
        "sae = StackedAutoencoder([768, 600, 400], activation='LeakyReLU')\n",
        "sae.fit(bert_embeddings, bert_embeddings, verbose=True, epochs=40)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "StackAutoencoder: fitting 2 autoencoders with dimensions [768, 600, 400]\n",
            "\tfitting autoencoder number 0 with dimensions (768, 600)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "477/477 [==============================] - 9s 17ms/step - loss: 1.3889 - val_loss: 0.3309\n",
            "Epoch 2/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.2833 - val_loss: 0.1847\n",
            "Epoch 3/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.1697 - val_loss: 0.1299\n",
            "Epoch 4/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.1256 - val_loss: 0.1011\n",
            "Epoch 5/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0992 - val_loss: 0.0871\n",
            "Epoch 6/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0825 - val_loss: 0.0748\n",
            "Epoch 7/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0716 - val_loss: 0.0635\n",
            "Epoch 8/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0641 - val_loss: 0.0599\n",
            "Epoch 9/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0577 - val_loss: 0.0522\n",
            "Epoch 10/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0549 - val_loss: 0.0531\n",
            "Epoch 11/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0514 - val_loss: 0.0507\n",
            "Epoch 12/40\n",
            "477/477 [==============================] - 7s 16ms/step - loss: 0.0488 - val_loss: 0.0500\n",
            "Epoch 13/40\n",
            "477/477 [==============================] - 7s 16ms/step - loss: 0.0474 - val_loss: 0.0514\n",
            "Epoch 14/40\n",
            "477/477 [==============================] - 7s 16ms/step - loss: 0.0476 - val_loss: 0.0494\n",
            "Epoch 15/40\n",
            "477/477 [==============================] - 7s 16ms/step - loss: 0.0464 - val_loss: 0.0509\n",
            "Epoch 16/40\n",
            "477/477 [==============================] - 7s 15ms/step - loss: 0.0458 - val_loss: 0.0452\n",
            "Epoch 17/40\n",
            "477/477 [==============================] - 7s 16ms/step - loss: 0.0455 - val_loss: 0.0441\n",
            "Epoch 18/40\n",
            "477/477 [==============================] - 7s 15ms/step - loss: 0.0450 - val_loss: 0.0422\n",
            "Epoch 19/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0437 - val_loss: 0.0443\n",
            "Epoch 20/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0446 - val_loss: 0.0408\n",
            "Epoch 21/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0428 - val_loss: 0.0427\n",
            "Epoch 22/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0425 - val_loss: 0.0397\n",
            "Epoch 23/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0420 - val_loss: 0.0453\n",
            "Epoch 24/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0425 - val_loss: 0.0437\n",
            "Epoch 25/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0425 - val_loss: 0.0412\n",
            "Epoch 26/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0422 - val_loss: 0.0457\n",
            "Epoch 27/40\n",
            "477/477 [==============================] - 8s 17ms/step - loss: 0.0413 - val_loss: 0.0442\n",
            "Epoch 28/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0426 - val_loss: 0.0431\n",
            "Epoch 29/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0442 - val_loss: 0.0392\n",
            "Epoch 30/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0411 - val_loss: 0.0404\n",
            "Epoch 31/40\n",
            "477/477 [==============================] - 8s 17ms/step - loss: 0.0411 - val_loss: 0.0388\n",
            "Epoch 32/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0420 - val_loss: 0.0382\n",
            "Epoch 33/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0400 - val_loss: 0.0430\n",
            "Epoch 34/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0411 - val_loss: 0.0384\n",
            "Epoch 35/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0403 - val_loss: 0.0400\n",
            "Epoch 36/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0403 - val_loss: 0.0400\n",
            "Epoch 37/40\n",
            "477/477 [==============================] - 8s 17ms/step - loss: 0.0415 - val_loss: 0.0394\n",
            "Epoch 38/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0404 - val_loss: 0.0389\n",
            "Epoch 39/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0405 - val_loss: 0.0390\n",
            "Epoch 40/40\n",
            "477/477 [==============================] - 8s 16ms/step - loss: 0.0398 - val_loss: 0.0401\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\tfitting autoencoder number 1 with dimensions (600, 400)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "477/477 [==============================] - 6s 11ms/step - loss: 0.8433 - val_loss: 0.3174\n",
            "Epoch 2/40\n",
            "477/477 [==============================] - 5s 10ms/step - loss: 0.2709 - val_loss: 0.1759\n",
            "Epoch 3/40\n",
            "477/477 [==============================] - 5s 10ms/step - loss: 0.1610 - val_loss: 0.1225\n",
            "Epoch 4/40\n",
            "477/477 [==============================] - 6s 13ms/step - loss: 0.1145 - val_loss: 0.0944\n",
            "Epoch 5/40\n",
            "477/477 [==============================] - 5s 10ms/step - loss: 0.0916 - val_loss: 0.0783\n",
            "Epoch 6/40\n",
            "477/477 [==============================] - 5s 10ms/step - loss: 0.0772 - val_loss: 0.0701\n",
            "Epoch 7/40\n",
            "477/477 [==============================] - 5s 10ms/step - loss: 0.0683 - val_loss: 0.0633\n",
            "Epoch 8/40\n",
            "477/477 [==============================] - 5s 10ms/step - loss: 0.0631 - val_loss: 0.0620\n",
            "Epoch 9/40\n",
            "477/477 [==============================] - 5s 10ms/step - loss: 0.0591 - val_loss: 0.0565\n",
            "Epoch 10/40\n",
            "477/477 [==============================] - 5s 10ms/step - loss: 0.0567 - val_loss: 0.0556\n",
            "Epoch 11/40\n",
            "477/477 [==============================] - 5s 10ms/step - loss: 0.0549 - val_loss: 0.0533\n",
            "Epoch 12/40\n",
            "477/477 [==============================] - 5s 10ms/step - loss: 0.0539 - val_loss: 0.0529\n",
            "Epoch 13/40\n",
            "477/477 [==============================] - 5s 11ms/step - loss: 0.0527 - val_loss: 0.0523\n",
            "Epoch 14/40\n",
            "477/477 [==============================] - 5s 10ms/step - loss: 0.0524 - val_loss: 0.0514\n",
            "Epoch 15/40\n",
            "477/477 [==============================] - 5s 10ms/step - loss: 0.0519 - val_loss: 0.0510\n",
            "Epoch 16/40\n",
            "477/477 [==============================] - 5s 10ms/step - loss: 0.0508 - val_loss: 0.0511\n",
            "Epoch 17/40\n",
            "477/477 [==============================] - 5s 10ms/step - loss: 0.0509 - val_loss: 0.0497\n",
            "Epoch 18/40\n",
            "477/477 [==============================] - 5s 10ms/step - loss: 0.0507 - val_loss: 0.0491\n",
            "Epoch 19/40\n",
            "477/477 [==============================] - 5s 10ms/step - loss: 0.0503 - val_loss: 0.0505\n",
            "Epoch 20/40\n",
            "477/477 [==============================] - 5s 10ms/step - loss: 0.0503 - val_loss: 0.0494\n",
            "Epoch 21/40\n",
            "477/477 [==============================] - 5s 10ms/step - loss: 0.0502 - val_loss: 0.0495\n",
            "Epoch 22/40\n",
            "477/477 [==============================] - 5s 10ms/step - loss: 0.0501 - val_loss: 0.0509\n",
            "Epoch 23/40\n",
            " 22/477 [>.............................] - ETA: 3s - loss: 0.0505"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-b78b919b0968>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStackedAutoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LeakyReLU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-88-224d6c8d7d30>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_data, val_data, verbose, epochs)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data_reduced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_reduced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                 \u001b[0;31m# callbacks=[tensorboard]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 _r=1):\n\u001b[1;32m   1157\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zEfoOfKW7EX",
        "outputId": "f87e3551-2f6c-4986-f989-70ad8ebd035f"
      },
      "source": [
        "mse(bert_embeddings, sae.decode(sae.encode(bert_embeddings)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.15034533"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKiU_XR4a0cL",
        "outputId": "8f7e3f8c-d236-4223-934c-1f31848f9f38"
      },
      "source": [
        "mse(train_data, sae.decode(sae.encode(train_data)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.2651944"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YL-cVKEnIxa"
      },
      "source": [
        "Добавил файнтюнинг по всей цепи..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdHUihhenb8C",
        "outputId": "399387ca-aaa9-42c3-f55a-f18ba569a2bc"
      },
      "source": [
        "# до файнтьюнинга\n",
        "sae = StackedAutoencoder([768, 600, 400], activation='LeakyReLU')\n",
        "sae.fit(bert_embeddings, bert_embeddings, verbose=True, epochs=4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "StackAutoencoder: fitting 2 autoencoders with dimensions [768, 600, 400]\n",
            "Epoch 1/4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\tfitting autoencoder number 0 with dimensions (768, 600)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "477/477 [==============================] - 9s 18ms/step - loss: 1.4110 - val_loss: 0.3332\n",
            "Epoch 2/4\n",
            "477/477 [==============================] - 8s 17ms/step - loss: 0.2882 - val_loss: 0.1874\n",
            "Epoch 3/4\n",
            "477/477 [==============================] - 8s 17ms/step - loss: 0.1735 - val_loss: 0.1327\n",
            "Epoch 4/4\n",
            "477/477 [==============================] - 8s 17ms/step - loss: 0.1262 - val_loss: 0.1022\n",
            "StackedAutoencoder: autoencoder 0 - model history\n",
            "<keras.callbacks.History object at 0x7efb7de9abd0>\n",
            "Epoch 1/4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\tfitting autoencoder number 1 with dimensions (600, 400)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "477/477 [==============================] - 7s 12ms/step - loss: 1.2081 - val_loss: 0.3549\n",
            "Epoch 2/4\n",
            "477/477 [==============================] - 6s 12ms/step - loss: 0.2873 - val_loss: 0.1636\n",
            "Epoch 3/4\n",
            "477/477 [==============================] - 6s 12ms/step - loss: 0.1464 - val_loss: 0.1048\n",
            "Epoch 4/4\n",
            "477/477 [==============================] - 6s 12ms/step - loss: 0.0968 - val_loss: 0.0796\n",
            "StackedAutoencoder: autoencoder 1 - model history\n",
            "<keras.callbacks.History object at 0x7efb7e92cb50>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.callbacks.History at 0x7efb7de9abd0>,\n",
              " <keras.callbacks.History at 0x7efb7e92cb50>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ozRrXvhnd_U",
        "outputId": "b2a2eecc-acd0-4e33-87de-8c66065fede8"
      },
      "source": [
        "mse(bert_embeddings, sae.decode(sae.encode(bert_embeddings)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.23899229"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVN6pCXzbK8N",
        "outputId": "a016ad85-a551-439c-8174-ed3fbbc02039"
      },
      "source": [
        "# с файнтьюнингом\n",
        "sae = StackedAutoencoder([768, 600, 400], activation='LeakyReLU')\n",
        "sae.fit(bert_embeddings, bert_embeddings, verbose=True, epochs=4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "StackAutoencoder: fitting 2 autoencoders with dimensions [768, 600, 400]\n",
            "Epoch 1/4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\tfitting autoencoder number 0 with dimensions (768, 600)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "477/477 [==============================] - 9s 18ms/step - loss: 1.3799 - val_loss: 0.3303\n",
            "Epoch 2/4\n",
            "477/477 [==============================] - 8s 17ms/step - loss: 0.2809 - val_loss: 0.1856\n",
            "Epoch 3/4\n",
            "477/477 [==============================] - 8s 17ms/step - loss: 0.1716 - val_loss: 0.1318\n",
            "Epoch 4/4\n",
            "477/477 [==============================] - 8s 17ms/step - loss: 0.1260 - val_loss: 0.1049\n",
            "StackedAutoencoder: autoencoder 0 - model history\n",
            "<keras.callbacks.History object at 0x7efb7f775790>\n",
            "Epoch 1/4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\tfitting autoencoder number 1 with dimensions (600, 400)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "477/477 [==============================] - 7s 14ms/step - loss: 1.2215 - val_loss: 0.3655\n",
            "Epoch 2/4\n",
            "477/477 [==============================] - 6s 13ms/step - loss: 0.2954 - val_loss: 0.1672\n",
            "Epoch 3/4\n",
            "477/477 [==============================] - 5s 11ms/step - loss: 0.1504 - val_loss: 0.1081\n",
            "Epoch 4/4\n",
            "477/477 [==============================] - 5s 11ms/step - loss: 0.1011 - val_loss: 0.0868\n",
            "StackedAutoencoder: autoencoder 1 - model history\n",
            "<keras.callbacks.History object at 0x7efb7df73cd0>\n",
            "StackedAutoencoder: fine-tuning...\n",
            "Epoch 1/4\n",
            "477/477 [==============================] - 40s 27ms/step - loss: 1.0213 - val_loss: 0.2452\n",
            "Epoch 2/4\n",
            "477/477 [==============================] - 12s 26ms/step - loss: 0.2297 - val_loss: 0.1989\n",
            "Epoch 3/4\n",
            "477/477 [==============================] - 13s 26ms/step - loss: 0.1941 - val_loss: 0.1788\n",
            "Epoch 4/4\n",
            "477/477 [==============================] - 13s 26ms/step - loss: 0.1779 - val_loss: 0.1705\n",
            "StackedAutoencoder: fine-tuning history model history\n",
            "<keras.callbacks.History object at 0x7efb7df19990>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.callbacks.History at 0x7efb7f775790>,\n",
              " <keras.callbacks.History at 0x7efb7df73cd0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "003qBqSTnLeq",
        "outputId": "cee67b64-af64-497f-dfee-a808b88bf446"
      },
      "source": [
        "mse(bert_embeddings, sae.decode(sae.encode(bert_embeddings)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.17047693"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRzwC9C9uO62"
      },
      "source": [
        "Побольше шагов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1LNxNR3t-ua",
        "outputId": "2db84012-9a9d-45fc-bcd6-a72089a4cbaa"
      },
      "source": [
        "# с файнтьюнингом\n",
        "sae = StackedAutoencoder([768, 600, 400, 200], activation='LeakyReLU')\n",
        "sae.fit(bert_embeddings, bert_embeddings, verbose=True, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "StackAutoencoder: fitting 3 autoencoders with dimensions [768, 600, 400, 200]\n",
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\tfitting autoencoder number 0 with dimensions (768, 600)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "477/477 [==============================] - 10s 20ms/step - loss: 1.4226 - val_loss: 0.3357\n",
            "Epoch 2/10\n",
            "477/477 [==============================] - 8s 17ms/step - loss: 0.2904 - val_loss: 0.1888\n",
            "Epoch 3/10\n",
            "477/477 [==============================] - 8s 17ms/step - loss: 0.1729 - val_loss: 0.1373\n",
            "Epoch 4/10\n",
            "477/477 [==============================] - 8s 17ms/step - loss: 0.1273 - val_loss: 0.1057\n",
            "Epoch 5/10\n",
            "477/477 [==============================] - 8s 17ms/step - loss: 0.1011 - val_loss: 0.0857\n",
            "Epoch 6/10\n",
            "477/477 [==============================] - 11s 23ms/step - loss: 0.0847 - val_loss: 0.0738\n",
            "Epoch 7/10\n",
            "477/477 [==============================] - 9s 18ms/step - loss: 0.0728 - val_loss: 0.0656\n",
            "Epoch 8/10\n",
            "477/477 [==============================] - 9s 18ms/step - loss: 0.0640 - val_loss: 0.0604\n",
            "Epoch 9/10\n",
            "477/477 [==============================] - 9s 18ms/step - loss: 0.0592 - val_loss: 0.0537\n",
            "Epoch 10/10\n",
            "477/477 [==============================] - 9s 18ms/step - loss: 0.0548 - val_loss: 0.0515\n",
            "StackedAutoencoder: autoencoder 0 - model history\n",
            "<keras.callbacks.History object at 0x7efb7dd9c190>\n",
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\tfitting autoencoder number 1 with dimensions (600, 400)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "477/477 [==============================] - 7s 14ms/step - loss: 1.1739 - val_loss: 0.4186\n",
            "Epoch 2/10\n",
            "477/477 [==============================] - 5s 11ms/step - loss: 0.3509 - val_loss: 0.2170\n",
            "Epoch 3/10\n",
            "477/477 [==============================] - 5s 11ms/step - loss: 0.1961 - val_loss: 0.1467\n",
            "Epoch 4/10\n",
            "477/477 [==============================] - 6s 13ms/step - loss: 0.1386 - val_loss: 0.1115\n",
            "Epoch 5/10\n",
            "477/477 [==============================] - 6s 13ms/step - loss: 0.1084 - val_loss: 0.0931\n",
            "Epoch 6/10\n",
            "477/477 [==============================] - 5s 11ms/step - loss: 0.0892 - val_loss: 0.0801\n",
            "Epoch 7/10\n",
            "477/477 [==============================] - 5s 11ms/step - loss: 0.0776 - val_loss: 0.0703\n",
            "Epoch 8/10\n",
            "477/477 [==============================] - 5s 11ms/step - loss: 0.0698 - val_loss: 0.0652\n",
            "Epoch 9/10\n",
            "477/477 [==============================] - 5s 11ms/step - loss: 0.0639 - val_loss: 0.0592\n",
            "Epoch 10/10\n",
            "477/477 [==============================] - 5s 11ms/step - loss: 0.0593 - val_loss: 0.0550\n",
            "StackedAutoencoder: autoencoder 1 - model history\n",
            "<keras.callbacks.History object at 0x7efb7e92a450>\n",
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\tfitting autoencoder number 2 with dimensions (400, 200)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "477/477 [==============================] - 4s 6ms/step - loss: 1.1373 - val_loss: 0.5036\n",
            "Epoch 2/10\n",
            "477/477 [==============================] - 3s 6ms/step - loss: 0.4360 - val_loss: 0.2886\n",
            "Epoch 3/10\n",
            "477/477 [==============================] - 3s 6ms/step - loss: 0.2619 - val_loss: 0.1990\n",
            "Epoch 4/10\n",
            "477/477 [==============================] - 3s 6ms/step - loss: 0.1876 - val_loss: 0.1589\n",
            "Epoch 5/10\n",
            "477/477 [==============================] - 3s 6ms/step - loss: 0.1519 - val_loss: 0.1360\n",
            "Epoch 6/10\n",
            "477/477 [==============================] - 3s 7ms/step - loss: 0.1341 - val_loss: 0.1248\n",
            "Epoch 7/10\n",
            "477/477 [==============================] - 3s 7ms/step - loss: 0.1240 - val_loss: 0.1196\n",
            "Epoch 8/10\n",
            "477/477 [==============================] - 3s 6ms/step - loss: 0.1194 - val_loss: 0.1158\n",
            "Epoch 9/10\n",
            "477/477 [==============================] - 3s 6ms/step - loss: 0.1160 - val_loss: 0.1140\n",
            "Epoch 10/10\n",
            "477/477 [==============================] - 3s 7ms/step - loss: 0.1144 - val_loss: 0.1144\n",
            "StackedAutoencoder: autoencoder 2 - model history\n",
            "<keras.callbacks.History object at 0x7efb7ed4cc10>\n",
            "StackedAutoencoder: fine-tuning...\n",
            "Epoch 1/10\n",
            "477/477 [==============================] - 16s 30ms/step - loss: 1.4470 - val_loss: 0.4236\n",
            "Epoch 2/10\n",
            "477/477 [==============================] - 14s 29ms/step - loss: 0.4046 - val_loss: 0.3537\n",
            "Epoch 3/10\n",
            "477/477 [==============================] - 14s 29ms/step - loss: 0.3536 - val_loss: 0.3358\n",
            "Epoch 4/10\n",
            "477/477 [==============================] - 14s 29ms/step - loss: 0.3421 - val_loss: 0.3280\n",
            "Epoch 5/10\n",
            "477/477 [==============================] - 14s 29ms/step - loss: 0.3365 - val_loss: 0.3253\n",
            "Epoch 6/10\n",
            "477/477 [==============================] - 14s 29ms/step - loss: 0.3355 - val_loss: 0.3225\n",
            "Epoch 7/10\n",
            "477/477 [==============================] - 14s 29ms/step - loss: 0.3463 - val_loss: 0.3376\n",
            "Epoch 8/10\n",
            "477/477 [==============================] - 14s 29ms/step - loss: 0.3320 - val_loss: 0.3350\n",
            "Epoch 9/10\n",
            "477/477 [==============================] - 14s 29ms/step - loss: 0.3291 - val_loss: 0.3267\n",
            "Epoch 10/10\n",
            "477/477 [==============================] - 14s 29ms/step - loss: 0.3355 - val_loss: 0.3346\n",
            "StackedAutoencoder: fine-tuning history model history\n",
            "<keras.callbacks.History object at 0x7efb7de9df90>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.callbacks.History at 0x7efb7dd9c190>,\n",
              " <keras.callbacks.History at 0x7efb7e92a450>,\n",
              " <keras.callbacks.History at 0x7efb7ed4cc10>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xp06du_vwHa",
        "outputId": "8f2a62cb-8798-4117-9aea-a1195c02b3e8"
      },
      "source": [
        "mse(bert_embeddings, sae.decode(sae.encode(bert_embeddings)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.33462942"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBumOwuGv37j",
        "outputId": "19594f57-9e64-4bf6-ff1f-f49eda5d8c22"
      },
      "source": [
        "sae = StackedAutoencoder([768, 600, 400, 200], activation='sigmoid')\n",
        "histories = sae.fit(bert_embeddings, bert_embeddings, verbose=False, epochs=10)\n",
        "for hist in histories:\n",
        "    print(hist.history)\n",
        "mse(bert_embeddings, sae.decode(sae.encode(bert_embeddings)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'loss': [1.186599612236023, 0.5467504262924194, 0.3734595477581024, 0.2905373275279999, 0.24023789167404175, 0.2056451439857483, 0.1804606020450592, 0.1617894321680069, 0.14588679373264313, 0.1341707706451416], 'val_loss': [0.7022616267204285, 0.42903879284858704, 0.3183573782444, 0.2571840286254883, 0.21645106375217438, 0.18688027560710907, 0.16714829206466675, 0.15346002578735352, 0.13753551244735718, 0.12441031634807587]}\n",
            "{'loss': [0.06439521163702011, 0.0364956371486187, 0.02209322713315487, 0.014913272112607956, 0.011058446019887924, 0.008905620314180851, 0.007704921532422304, 0.007055368274450302, 0.006699654273688793, 0.006485307589173317], 'val_loss': [0.04739822819828987, 0.027456462383270264, 0.01747775264084339, 0.012513060122728348, 0.009655347093939781, 0.008102896623313427, 0.007220454514026642, 0.006860638968646526, 0.0065798633731901646, 0.006393322721123695]}\n",
            "{'loss': [0.029452137649059296, 0.022242987528443336, 0.01829284057021141, 0.015084461309015751, 0.012841317802667618, 0.011375578120350838, 0.010507525876164436, 0.010045034810900688, 0.009780661202967167, 0.009611211717128754], 'val_loss': [0.024200843647122383, 0.020215759053826332, 0.016563190147280693, 0.01390229444950819, 0.01204721536487341, 0.010820545256137848, 0.010165439918637276, 0.009863132610917091, 0.009665239602327347, 0.009536294266581535]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.34195757"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4rHC7KewEmg",
        "outputId": "e37b4a68-cd94-4cec-d756-79bfac5f7c51"
      },
      "source": [
        "sae = StackedAutoencoder([768, 600, 400, 200], activation='relu')\n",
        "histories = sae.fit(bert_embeddings, bert_embeddings, verbose=False, epochs=10)\n",
        "for hist in histories:\n",
        "    print(hist.history)\n",
        "mse(bert_embeddings, sae.decode(sae.encode(bert_embeddings)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'loss': [0.9649438261985779, 0.4130903482437134, 0.321049302816391, 0.28241997957229614, 0.2593977153301239, 0.23874534666538239, 0.22367535531520844, 0.21542538702487946, 0.20764648914337158, 0.20317386090755463], 'val_loss': [0.5016083717346191, 0.347822368144989, 0.2940463721752167, 0.2606540322303772, 0.24147269129753113, 0.22876673936843872, 0.2144477367401123, 0.2104853391647339, 0.19965413212776184, 0.1996568888425827]}\n",
            "{'loss': [0.8566482663154602, 0.43232160806655884, 0.31111717224121094, 0.2680712640285492, 0.24626187980175018, 0.23718233406543732, 0.22985625267028809, 0.21556824445724487, 0.20688533782958984, 0.19742079079151154], 'val_loss': [0.5483927130699158, 0.34511980414390564, 0.2817358076572418, 0.25325655937194824, 0.23808099329471588, 0.2302284985780716, 0.22071976959705353, 0.21023723483085632, 0.2023032307624817, 0.19385464489459991]}\n",
            "{'loss': [0.6761146783828735, 0.36132630705833435, 0.28132158517837524, 0.2459239363670349, 0.22197851538658142, 0.2074882835149765, 0.19527088105678558, 0.18293504416942596, 0.17093630135059357, 0.16606640815734863], 'val_loss': [0.43543508648872375, 0.30746227502822876, 0.258722722530365, 0.2315942347049713, 0.21206669509410858, 0.20092962682247162, 0.18930406868457794, 0.17649589478969574, 0.16618391871452332, 0.16325773298740387]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5794408"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiBx65_KH9Bd",
        "outputId": "b2093bd9-b4c7-4127-ab3d-cc489a73d94d"
      },
      "source": [
        "1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh3CFT7OaOBr"
      },
      "source": [
        "## Tiny BERT\n",
        "\n",
        "Разные параметры SAE для предобученной модели на 128 `hidden_size`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "f7d588adaab34d8b8a77c29685d3358e",
            "debaab3fe05c42ed8ad7a62b8deddfd3",
            "bc18abca13fa4e2ab150509ab7ce4025",
            "e7898a2cc9c742bd89973f27f1c27adf",
            "db4b4418f1a64684a19e1a2d9e11ca4c",
            "c8ef3771fa1d439a9ffe316ea8aa7484",
            "889f66a524644ed381b07ed6449da1fb",
            "f771589126a74cb99215bf730cc1a3f7",
            "f452547d7ac348ccb37360b2b8b4e032",
            "6ca0ac02bb9f4d4aa3d55eedf9e350ef",
            "b772469f80ea4e5cbab2c9b682be24f4",
            "2515f2a05922438d86113dc9a07c2b53",
            "46e7e7935afe46c980713ab216f79dc0",
            "18a310b239094341ac59180a9b0ff1f3",
            "b52968ca34724a56b6f6e85112740f3e",
            "195df3ba5df3451da3f607cb12a9a485",
            "b56d3b3c0bc34841b9bb0cd93afd0d9d",
            "ef8dfcb8adcb44f5ad4a6329e386dc76",
            "88f5849a66874f06ae4f148636b2f3c0",
            "cf16a1a6ec294d8da9e6ee807165c2cc",
            "49b050e3caa346499be7adf7424227be",
            "09e9668247fd4cd3a47bea79738c3b1f",
            "4c0c1c7093244eeea46b960756411a48",
            "6a7f0432d63349df90b381c0f47aa184"
          ]
        },
        "id": "Z-vvznxQU136",
        "outputId": "089e0b7b-77d5-4fd9-bb95-3ca6d83b1863"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('google/bert_uncased_L-12_H-128_A-2')\n",
        "config = BertConfig.from_pretrained('google/bert_uncased_L-12_H-128_A-2', output_hidden_states=True)\n",
        "model = BertModel.from_pretrained('google/bert_uncased_L-12_H-128_A-2', config=config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7d588adaab34d8b8a77c29685d3358e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f452547d7ac348ccb37360b2b8b4e032",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=383.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b56d3b3c0bc34841b9bb0cd93afd0d9d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=25710911.0, style=ProgressStyle(descrip…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at google/bert_uncased_L-12_H-128_A-2 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIJydGu6aSpO"
      },
      "source": [
        "bert_embeddings128 = np.load('drive/MyDrive/Coursework2021/bert_uncased_L-12_H-128_A-2_vocab_embeddings.npy')\n",
        "with open('drive/MyDrive/Coursework2021/bert_uncased_L-12_H-128_A-2_vocab.txt', 'r') as f:\n",
        "    bert_tokens128 = f.read().split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GpP7V4pnjPg",
        "outputId": "dbf66358-bace-4277-80f1-d900767626f1"
      },
      "source": [
        "sae = StackedAutoencoder([128, 100])\n",
        "sae.fit(bert_embeddings128, bert_embeddings128, False, 20)\n",
        "mse(bert_embeddings128, sae.decode(sae.encode(bert_embeddings128)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.15432885"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-a6mZ-7-n2Fs",
        "outputId": "a4b9ed8f-c509-4f03-ff6e-1e15705169b0"
      },
      "source": [
        "sae = StackedAutoencoder([128, 100], activation='LeakyReLU')\n",
        "sae.fit(bert_embeddings128, bert_embeddings128, False, 20)\n",
        "mse(bert_embeddings128, sae.decode(sae.encode(bert_embeddings128)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.018996993"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUwKVwCqoUa3",
        "outputId": "827100fd-d8ed-4125-9c9b-71f65be1c800"
      },
      "source": [
        "sae = StackedAutoencoder([128, 128], activation='LeakyReLU')\n",
        "sae.fit(bert_embeddings128, bert_embeddings128, False, 10)\n",
        "mse(bert_embeddings128, sae.decode(sae.encode(bert_embeddings128)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.006187888"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xulTSkoAommF",
        "outputId": "2c5ed40d-7cb2-4503-af48-26bd4b3bac69"
      },
      "source": [
        "sae = StackedAutoencoder([128, 100, 80, 60], activation='LeakyReLU')\n",
        "sae.fit(bert_embeddings128, bert_embeddings128, False, 30)\n",
        "print(mse(bert_embeddings128, sae.decode(sae.encode(bert_embeddings128))))\n",
        "for hist in sae.model_histories:\n",
        "    print(hist.history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.16740897\n",
            "{'loss': [2.3491480350494385, 0.4364486038684845, 0.21400223672389984, 0.1409820020198822, 0.10502278059720993, 0.08285422623157501, 0.06788118928670883, 0.05723185837268829, 0.049222782254219055, 0.04312426969408989, 0.038309045135974884, 0.03439674153923988, 0.03151087835431099, 0.0290579441934824, 0.02716333046555519, 0.025819942355155945, 0.02462036907672882, 0.023720787838101387, 0.023014595732092857, 0.022535018622875214, 0.02192884311079979, 0.021698379889130592, 0.02136475406587124, 0.021114520728588104, 0.020880380645394325, 0.02080659195780754, 0.020622065290808678, 0.02030559629201889, 0.0202778410166502, 0.019925082102417946], 'val_loss': [0.6998955011367798, 0.2761889100074768, 0.16773951053619385, 0.11985578387975693, 0.09206017106771469, 0.07397755235433578, 0.06212369725108147, 0.052446912974119186, 0.04515308514237404, 0.04087630286812782, 0.035686369985342026, 0.032915011048316956, 0.03012455254793167, 0.02897857502102852, 0.02647588774561882, 0.0248501468449831, 0.02394910901784897, 0.02312076836824417, 0.022266339510679245, 0.02297278679907322, 0.02166992425918579, 0.02177850902080536, 0.02091306820511818, 0.020911091938614845, 0.02153669483959675, 0.020984677597880363, 0.02113458327949047, 0.01972511038184166, 0.02051556669175625, 0.019056154415011406]}\n",
            "{'loss': [2.7574305534362793, 0.8214441537857056, 0.41584575176239014, 0.25037211179733276, 0.17051845788955688, 0.1258118599653244, 0.09850311279296875, 0.0801885575056076, 0.06739780306816101, 0.0576009564101696, 0.04992147907614708, 0.04370347782969475, 0.039156023412942886, 0.035875510424375534, 0.03373272344470024, 0.032267436385154724, 0.030925186350941658, 0.030069421976804733, 0.02941839210689068, 0.028887465596199036, 0.02855043299496174, 0.02817070484161377, 0.027880650013685226, 0.027633316814899445, 0.027264131233096123, 0.027252161875367165, 0.027109142392873764, 0.026991121470928192, 0.026862816885113716, 0.026770150288939476], 'val_loss': [1.2315937280654907, 0.547482967376709, 0.31173065304756165, 0.2020191103219986, 0.14370329678058624, 0.10905671864748001, 0.0875215232372284, 0.07361701130867004, 0.06159072369337082, 0.052842967212200165, 0.04637162759900093, 0.04061970114707947, 0.03760548681020737, 0.034840308129787445, 0.032188478857278824, 0.03180735185742378, 0.031835079193115234, 0.029765445739030838, 0.02886373922228813, 0.028024820610880852, 0.02863011136651039, 0.028619665652513504, 0.027625437825918198, 0.02726990357041359, 0.027422664687037468, 0.02676408737897873, 0.026431990787386894, 0.026817800477147102, 0.02665192447602749, 0.026969147846102715]}\n",
            "{'loss': [2.7125799655914307, 0.879024863243103, 0.49234911799430847, 0.32255277037620544, 0.23285268247127533, 0.17842620611190796, 0.1429375857114792, 0.11970997601747513, 0.10404520481824875, 0.09206780046224594, 0.08212824165821075, 0.074037566781044, 0.06730031967163086, 0.06195371225476265, 0.05820541828870773, 0.055526066571474075, 0.0537077896296978, 0.05231036990880966, 0.051800236105918884, 0.0512237548828125, 0.05057139694690704, 0.050203438848257065, 0.04990212991833687, 0.049747616052627563, 0.04948334023356438, 0.04939083009958267, 0.04914373531937599, 0.049093082547187805, 0.04894183576107025, 0.04882056638598442], 'val_loss': [1.2285927534103394, 0.6298614144325256, 0.38611331582069397, 0.26937708258628845, 0.20025965571403503, 0.15762600302696228, 0.1290615200996399, 0.11013507097959518, 0.09698788821697235, 0.08639821410179138, 0.07750357687473297, 0.07069247961044312, 0.06415338814258575, 0.05916856974363327, 0.05735459178686142, 0.05387843772768974, 0.05240408703684807, 0.0531415119767189, 0.05124540999531746, 0.05036292225122452, 0.05073506385087967, 0.04987200349569321, 0.05019654706120491, 0.04903516545891762, 0.04977784305810928, 0.04876897856593132, 0.04986380413174629, 0.0489787682890892, 0.04915948957204819, 0.04973870515823364]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngi2TsIQynUu",
        "outputId": "9cbd8aaa-7f83-4696-ad46-f6aa38572f67"
      },
      "source": [
        "pca = PCA(60)\n",
        "pca_res = pca.fit_transform(bert_embeddings128)\n",
        "mse(pca.inverse_transform(pca_res), bert_embeddings128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1603217"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DW0uXjE6qoSB",
        "outputId": "3a966207-fd5e-44b9-fe60-36f40554d75b"
      },
      "source": [
        "sae = StackedAutoencoder([128, 100, 80, 60, 40], activation='LeakyReLU')\n",
        "sae.fit(bert_embeddings128, bert_embeddings128, False, 30)\n",
        "print(mse(bert_embeddings128, sae.decode(sae.encode(bert_embeddings128))))\n",
        "for hist in sae.model_histories:\n",
        "    print(hist.history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5249959\n",
            "{'loss': [2.1841461658477783, 0.39729902148246765, 0.18730299174785614, 0.12089979648590088, 0.08945918083190918, 0.07134494930505753, 0.05859724059700966, 0.04966127872467041, 0.04333166033029556, 0.038625411689281464, 0.03474928066134453, 0.03159848228096962, 0.029128653928637505, 0.027361439540982246, 0.025948062539100647, 0.024707812815904617, 0.023960188031196594, 0.023215649649500847, 0.022456243634223938, 0.02211257815361023, 0.02156037651002407, 0.02149883285164833, 0.021208638325333595, 0.02089368924498558, 0.020727841183543205, 0.020427200943231583, 0.020471487194299698, 0.0202300064265728, 0.020032722502946854, 0.020247558131814003], 'val_loss': [0.6550233364105225, 0.2452898472547531, 0.1433161199092865, 0.10147836059331894, 0.07905277609825134, 0.06309933215379715, 0.053267352283000946, 0.046874262392520905, 0.041679129004478455, 0.03655751422047615, 0.032040368765592575, 0.030174320563673973, 0.027910873293876648, 0.02622029557824135, 0.025167839601635933, 0.023052016273140907, 0.02329418621957302, 0.022004537284374237, 0.022414555773139, 0.02135077491402626, 0.021691130474209785, 0.02132224291563034, 0.021334825083613396, 0.020312178879976273, 0.021204445511102676, 0.020989619195461273, 0.020149145275354385, 0.020043281838297844, 0.01999516412615776, 0.02029169350862503]}\n",
            "{'loss': [3.281205415725708, 1.0616207122802734, 0.5241125822067261, 0.3260992765426636, 0.22510723769664764, 0.16535119712352753, 0.12838269770145416, 0.10443537682294846, 0.0878259539604187, 0.07512248307466507, 0.06586816906929016, 0.05867762118577957, 0.05321654677391052, 0.04876318201422691, 0.04552813619375229, 0.04252653941512108, 0.04043320193886757, 0.03841910883784294, 0.03696545213460922, 0.03590010479092598, 0.035197384655475616, 0.03452347218990326, 0.03427321836352348, 0.033778116106987, 0.033565934747457504, 0.03335950896143913, 0.033042799681425095, 0.032959602773189545, 0.032681819051504135, 0.03258588910102844], 'val_loss': [1.5633232593536377, 0.7002549171447754, 0.3984736204147339, 0.2641235888004303, 0.19133691489696503, 0.1424078792333603, 0.11634234338998795, 0.09402786195278168, 0.08078161627054214, 0.07020709663629532, 0.06152506545186043, 0.05697258189320564, 0.049624521285295486, 0.047448448836803436, 0.043157510459423065, 0.041151974350214005, 0.03862309455871582, 0.03724496811628342, 0.03660805895924568, 0.034386955201625824, 0.03447635471820831, 0.03390590101480484, 0.03449565917253494, 0.033598802983760834, 0.03322148695588112, 0.032800380140542984, 0.03202665224671364, 0.033356089144945145, 0.03275289759039879, 0.032424405217170715]}\n",
            "{'loss': [2.7715518474578857, 0.9168338775634766, 0.46313580870628357, 0.2776438593864441, 0.1884741187095642, 0.14233307540416718, 0.11656483262777328, 0.09890878945589066, 0.0864093005657196, 0.0773141011595726, 0.07052285224199295, 0.06524976342916489, 0.06156213954091072, 0.059171583503484726, 0.05715826526284218, 0.05581193044781685, 0.0550110898911953, 0.05419206619262695, 0.053795862942934036, 0.05328800156712532, 0.0530102401971817, 0.0527055487036705, 0.05241033807396889, 0.052260737866163254, 0.05198773741722107, 0.05176248028874397, 0.05182168260216713, 0.051665011793375015, 0.05144329369068146, 0.05168170481920242], 'val_loss': [1.3094061613082886, 0.6199740767478943, 0.34683483839035034, 0.22116993367671967, 0.15999752283096313, 0.12687847018241882, 0.10680937767028809, 0.09112227708101273, 0.08027771860361099, 0.07284976541996002, 0.06753184646368027, 0.06199555844068527, 0.05917399749159813, 0.057380951941013336, 0.0559488981962204, 0.05500105768442154, 0.05477810651063919, 0.05258304998278618, 0.05431235581636429, 0.05281221121549606, 0.053219132125377655, 0.05294014513492584, 0.05197707191109657, 0.053025983273983, 0.05155912786722183, 0.05190576612949371, 0.05171960964798927, 0.05083527788519859, 0.051169253885746, 0.05084112286567688]}\n",
            "{'loss': [3.075340747833252, 1.0433520078659058, 0.6401090025901794, 0.4516294300556183, 0.33839696645736694, 0.2648238241672516, 0.22043675184249878, 0.19256837666034698, 0.1744697242975235, 0.1628684103488922, 0.1551630198955536, 0.14993247389793396, 0.14652439951896667, 0.14400506019592285, 0.14188525080680847, 0.1405806988477707, 0.13961517810821533, 0.13857890665531158, 0.13803714513778687, 0.137481689453125, 0.137020543217659, 0.136850044131279, 0.13635917007923126, 0.13619540631771088, 0.13600820302963257, 0.1357547789812088, 0.135655477643013, 0.1355980485677719, 0.13519929349422455, 0.13540317118167877], 'val_loss': [1.3686469793319702, 0.7865970730781555, 0.5276591777801514, 0.386875718832016, 0.2974696159362793, 0.23841756582260132, 0.20702902972698212, 0.1816694140434265, 0.1667070835828781, 0.15907929837703705, 0.15202048420906067, 0.14830806851387024, 0.1433621346950531, 0.14208099246025085, 0.1405334174633026, 0.13952426612377167, 0.13936680555343628, 0.13634803891181946, 0.13632112741470337, 0.13655667006969452, 0.13852311670780182, 0.1361062377691269, 0.1367390900850296, 0.13634327054023743, 0.1349707543849945, 0.1349937468767166, 0.13456977903842926, 0.1351582407951355, 0.13322332501411438, 0.1344069391489029]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mx8_526OvaQ7",
        "outputId": "ef74a56c-dcd4-4ee8-9c8b-19be328d1803"
      },
      "source": [
        "sae = StackedAutoencoder([128, 100, 80, 60, 50, 40], activation='LeakyReLU')\n",
        "sae.fit(bert_embeddings128, bert_embeddings128, False, 30)\n",
        "print(mse(bert_embeddings128, sae.decode(sae.encode(bert_embeddings128))))\n",
        "for hist in sae.model_histories:\n",
        "    print(hist.history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5274845\n",
            "{'loss': [2.3317503929138184, 0.41354456543922424, 0.2007095366716385, 0.13407497107982635, 0.10065203160047531, 0.07989124208688736, 0.06585010141134262, 0.05565850809216499, 0.048114750534296036, 0.042018499225378036, 0.037075627595186234, 0.032988980412483215, 0.029784156009554863, 0.02728697657585144, 0.025463292375206947, 0.023885764181613922, 0.022806255146861076, 0.02235797792673111, 0.021615678444504738, 0.02132830023765564, 0.020953159779310226, 0.020541716367006302, 0.02009483054280281, 0.019861901178956032, 0.01970750465989113, 0.019686441868543625, 0.019538674503564835, 0.01942487806081772, 0.019328149035573006, 0.019147839397192], 'val_loss': [0.6716212630271912, 0.2585986852645874, 0.16017849743366241, 0.11424896866083145, 0.08734660595655441, 0.07173994183540344, 0.060361579060554504, 0.050605226308107376, 0.043660588562488556, 0.038658082485198975, 0.03386984020471573, 0.03066762164235115, 0.028593068942427635, 0.02583126910030842, 0.024107621982693672, 0.023120535537600517, 0.022452525794506073, 0.021505611017346382, 0.021085312590003014, 0.020743779838085175, 0.020679565146565437, 0.020502494648098946, 0.02019280008971691, 0.02001672238111496, 0.020223958417773247, 0.019443761557340622, 0.018725799396634102, 0.020092274993658066, 0.019014183431863785, 0.01847177743911743]}\n",
            "{'loss': [3.196925163269043, 0.9476738572120667, 0.4865255057811737, 0.2947307527065277, 0.20574268698692322, 0.15536312758922577, 0.12300565838813782, 0.10051975399255753, 0.08459973335266113, 0.07164004445075989, 0.062118615955114365, 0.054465848952531815, 0.04862561449408531, 0.044059962034225464, 0.040358640253543854, 0.0372794009745121, 0.034915611147880554, 0.032971370965242386, 0.03181932866573334, 0.030718769878149033, 0.03004574589431286, 0.029415706172585487, 0.02902328222990036, 0.0287559162825346, 0.028460565954446793, 0.028350036591291428, 0.028284920379519463, 0.028040185570716858, 0.02796935848891735, 0.027742845937609673], 'val_loss': [1.381354570388794, 0.6438283920288086, 0.3641796410083771, 0.239156112074852, 0.17586007714271545, 0.13563348352909088, 0.1110813170671463, 0.09199569374322891, 0.0795104056596756, 0.0667015090584755, 0.057951558381319046, 0.0503711961209774, 0.04525899142026901, 0.04182083159685135, 0.03856268152594566, 0.03535373508930206, 0.033264920115470886, 0.032413050532341, 0.031351372599601746, 0.029813580214977264, 0.02918287366628647, 0.028744211420416832, 0.02865580841898918, 0.02786341682076454, 0.027971308678388596, 0.028219074010849, 0.02816942147910595, 0.028816845268011093, 0.02907381020486355, 0.02839820459485054]}\n",
            "{'loss': [3.1811981201171875, 0.9492284059524536, 0.522942841053009, 0.34046700596809387, 0.24835653603076935, 0.1904430091381073, 0.1499275416135788, 0.12108919024467468, 0.10097238421440125, 0.08691716939210892, 0.07597753405570984, 0.06763410568237305, 0.061233039945364, 0.05697978660464287, 0.05366240069270134, 0.051813315600156784, 0.05052083358168602, 0.049569062888622284, 0.04904866963624954, 0.048553407192230225, 0.048277597874403, 0.04776296019554138, 0.047564055770635605, 0.04733220115303993, 0.04723687097430229, 0.04701409861445427, 0.04682682082056999, 0.04689422994852066, 0.046602919697761536, 0.046574871987104416], 'val_loss': [1.3250768184661865, 0.6758214831352234, 0.40848222374916077, 0.28328877687454224, 0.2177666425704956, 0.16882158815860748, 0.13194969296455383, 0.10874568670988083, 0.09279946237802505, 0.08175460249185562, 0.07079382985830307, 0.06314712762832642, 0.05851208046078682, 0.05447183921933174, 0.05184110999107361, 0.05125280097126961, 0.04959363862872124, 0.04819466546177864, 0.04830409958958626, 0.04829614609479904, 0.04716187343001366, 0.0471159964799881, 0.047243889421224594, 0.04745010286569595, 0.04864147678017616, 0.04757796600461006, 0.04625818133354187, 0.04658529907464981, 0.046209171414375305, 0.04629192873835564]}\n",
            "{'loss': [3.0956568717956543, 0.999260663986206, 0.5416147112846375, 0.3328397274017334, 0.22870264947414398, 0.16789187490940094, 0.13094747066497803, 0.1056598350405693, 0.08803316205739975, 0.07469306886196136, 0.0645032525062561, 0.05623825266957283, 0.05004919320344925, 0.04548338055610657, 0.04236658290028572, 0.04027631878852844, 0.03894989937543869, 0.03805629909038544, 0.037331659346818924, 0.03673572465777397, 0.03622568026185036, 0.035788170993328094, 0.03541738912463188, 0.03521685674786568, 0.034867577254772186, 0.03478588908910751, 0.03460298106074333, 0.034410737454891205, 0.03410997614264488, 0.03412822633981705], 'val_loss': [1.3649940490722656, 0.7092393636703491, 0.4091509282588959, 0.2699332535266876, 0.19237983226776123, 0.14905700087547302, 0.1175900250673294, 0.09646125882863998, 0.08000602573156357, 0.07001523673534393, 0.058868519961833954, 0.05151695758104324, 0.047627806663513184, 0.043816328048706055, 0.040938250720500946, 0.04032208397984505, 0.03818057104945183, 0.03711998835206032, 0.03661729395389557, 0.03599042445421219, 0.035743262618780136, 0.03511667996644974, 0.036568526178598404, 0.03438909351825714, 0.034812431782484055, 0.03418402746319771, 0.03454047068953514, 0.034060150384902954, 0.03408019244670868, 0.034358952194452286]}\n",
            "{'loss': [3.5759544372558594, 1.075740098953247, 0.6162616014480591, 0.3982648551464081, 0.2720836400985718, 0.19646044075489044, 0.14875555038452148, 0.1173378974199295, 0.09487561136484146, 0.07921398431062698, 0.06786999106407166, 0.060135237872600555, 0.054158564656972885, 0.049695223569869995, 0.0462271012365818, 0.044070158153772354, 0.04271509125828743, 0.04181604087352753, 0.04119995981454849, 0.04073242470622063, 0.04031702131032944, 0.040022291243076324, 0.03976884111762047, 0.03942305967211723, 0.03928178548812866, 0.03925098478794098, 0.03909968584775925, 0.03907192498445511, 0.0388866551220417, 0.038763631135225296], 'val_loss': [1.4300650358200073, 0.790212869644165, 0.4828066825866699, 0.3255225718021393, 0.2260620892047882, 0.1690753698348999, 0.1310252845287323, 0.10432298481464386, 0.08516751229763031, 0.07184678316116333, 0.06412692368030548, 0.05650128424167633, 0.05145984888076782, 0.04741164296865463, 0.044151194393634796, 0.04272715374827385, 0.042132213711738586, 0.041556112468242645, 0.040477920323610306, 0.040594033896923065, 0.040508389472961426, 0.03891855850815773, 0.03958600386977196, 0.04029179736971855, 0.039604321122169495, 0.03917117789387703, 0.03902584686875343, 0.03930502012372017, 0.03837982937693596, 0.03949678689241409]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2-J51aGqs6w",
        "outputId": "6bae1484-1b0b-4d3f-bb1b-f5903fe55172"
      },
      "source": [
        "sae = StackedAutoencoder([128, 100, 80, 60], activation='softplus')\n",
        "sae.fit(bert_embeddings128, bert_embeddings128, False, 30)\n",
        "print(mse(bert_embeddings128, sae.decode(sae.encode(bert_embeddings128))))\n",
        "for hist in sae.model_histories:\n",
        "    print(hist.history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.20125131\n",
            "{'loss': [2.204474687576294, 0.37573540210723877, 0.18384170532226562, 0.12105678766965866, 0.09010176360607147, 0.07248447090387344, 0.0613802969455719, 0.052670951932668686, 0.046545833349227905, 0.042291246354579926, 0.03863244876265526, 0.03610583767294884, 0.034516625106334686, 0.03310806676745415, 0.031275033950805664, 0.03011811338365078, 0.02866891585290432, 0.027322597801685333, 0.025374624878168106, 0.02404964715242386, 0.023264948278665543, 0.022906387224793434, 0.02261531911790371, 0.0220426544547081, 0.021595733240246773, 0.021104944869875908, 0.02028939127922058, 0.019799351692199707, 0.01974482648074627, 0.01969093270599842], 'val_loss': [0.6011069416999817, 0.2391299456357956, 0.1423710733652115, 0.10170350968837738, 0.07971800863742828, 0.06617391854524612, 0.05776528641581535, 0.04958321154117584, 0.04540518671274185, 0.04027245193719864, 0.037567026913166046, 0.03651277720928192, 0.0336158461868763, 0.030958421528339386, 0.031128382310271263, 0.029653705656528473, 0.027315007522702217, 0.026203544810414314, 0.024664899334311485, 0.023809822276234627, 0.02332528494298458, 0.02221117541193962, 0.021697383373975754, 0.022667165845632553, 0.02157086506485939, 0.02112100087106228, 0.019679704681038857, 0.01911909319460392, 0.019354406744241714, 0.019137725234031677]}\n",
            "{'loss': [4.4320268630981445, 1.6804157495498657, 1.003389596939087, 0.600969672203064, 0.412253737449646, 0.3077770471572876, 0.25237172842025757, 0.21447235345840454, 0.19073860347270966, 0.17540934681892395, 0.16567322611808777, 0.159343421459198, 0.15011760592460632, 0.14214538037776947, 0.13879276812076569, 0.13662871718406677, 0.134619802236557, 0.1298833191394806, 0.12612605094909668, 0.1248050108551979, 0.12355092167854309, 0.1201230138540268, 0.11475526541471481, 0.11091221123933792, 0.10659877955913544, 0.10145004093647003, 0.09881902486085892, 0.097421795129776, 0.0945853516459465, 0.09116098284721375], 'val_loss': [2.163886547088623, 1.279837727546692, 0.764931857585907, 0.48885735869407654, 0.3435395061969757, 0.2764171361923218, 0.23058733344078064, 0.1973426789045334, 0.18223664164543152, 0.16826345026493073, 0.1623375564813614, 0.1545926332473755, 0.14447471499443054, 0.14022985100746155, 0.13570384681224823, 0.13413691520690918, 0.13221995532512665, 0.12808135151863098, 0.1250178962945938, 0.12260584533214569, 0.1218506470322609, 0.11655419319868088, 0.11207355558872223, 0.10782691091299057, 0.10207110643386841, 0.09889920055866241, 0.0970415323972702, 0.09469296038150787, 0.09366277605295181, 0.09037566184997559]}\n",
            "{'loss': [5.239308834075928, 1.891296148300171, 1.2485536336898804, 0.9006127119064331, 0.7146996855735779, 0.5740245580673218, 0.45086535811424255, 0.38908788561820984, 0.34425827860832214, 0.3143100142478943, 0.2905809283256531, 0.27895188331604004, 0.2491004914045334, 0.24036157131195068, 0.23485538363456726, 0.22793413698673248, 0.2089795172214508, 0.1803838312625885, 0.1720513254404068, 0.1696843057870865, 0.1637340933084488, 0.1514633446931839, 0.1506379395723343, 0.1495448797941208, 0.14904233813285828, 0.1486079841852188, 0.14833100140094757, 0.14850907027721405, 0.14788730442523956, 0.14763368666172028], 'val_loss': [2.448472261428833, 1.4640541076660156, 1.0613982677459717, 0.7845034599304199, 0.6448589563369751, 0.4926173985004425, 0.41400930285453796, 0.3631957173347473, 0.3303741216659546, 0.29772359132766724, 0.28618815541267395, 0.2638297975063324, 0.24188363552093506, 0.2407308965921402, 0.23129135370254517, 0.21739403903484344, 0.19339385628700256, 0.17195798456668854, 0.17881150543689728, 0.1681448072195053, 0.15259917080402374, 0.1492455005645752, 0.14979246258735657, 0.1467539668083191, 0.15078143775463104, 0.15252020955085754, 0.1502760797739029, 0.14807644486427307, 0.14703334867954254, 0.14586299657821655]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb6Aqbq40Evw",
        "outputId": "62d1e99b-6cb1-415b-93f6-e4c8c241a876"
      },
      "source": [
        "sae = StackedAutoencoder([128, 100, 80, 60], activation='softplus')\n",
        "sae.fit(bert_embeddings128[400:410], bert_embeddings128[400:410], False, 300)\n",
        "print(mse(bert_embeddings128[400:410], sae.decode(sae.encode(bert_embeddings128[400:410]))))\n",
        "for hist in sae.model_histories:\n",
        "    print(hist.history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0042570736\n",
            "{'loss': [43.38262939453125, 37.663002014160156, 32.77708053588867, 28.592620849609375, 24.990039825439453, 21.866374969482422, 19.13454246520996, 16.720136642456055, 14.568554878234863, 12.643537521362305, 10.922219276428223, 9.390085220336914, 8.036715507507324, 6.8501763343811035, 5.816445827484131, 4.925721168518066, 4.168918609619141, 3.533289670944214, 3.0036959648132324, 2.5631542205810547, 2.1932430267333984, 1.8775043487548828, 1.6025989055633545, 1.3627530336380005, 1.1569595336914062, 0.9868201017379761, 0.8502286076545715, 0.7421313524246216, 0.6568524241447449, 0.5883737802505493, 0.5322855710983276, 0.4857160449028015, 0.4465363621711731, 0.41301822662353516, 0.38385826349258423, 0.35814210772514343, 0.3351303040981293, 0.3140542209148407, 0.2941109836101532, 0.27465686202049255, 0.25542640686035156, 0.23660728335380554, 0.2187132090330124, 0.20230770111083984, 0.18773318827152252, 0.17499692738056183, 0.16383808851242065, 0.15388193726539612, 0.14478299021720886, 0.13631370663642883, 0.1283901184797287, 0.12104020267724991, 0.1143464669585228, 0.10839780420064926, 0.10325568914413452, 0.09892351925373077, 0.09532121568918228, 0.09228986501693726, 0.08962893486022949, 0.08714684098958969, 0.08470311760902405, 0.08223124593496323, 0.07973627746105194, 0.07727047801017761, 0.074904665350914, 0.07270985096693039, 0.07074470072984695, 0.06904079020023346, 0.06759092956781387, 0.06635235249996185, 0.06526472419500351, 0.06427431106567383, 0.06335218995809555, 0.06249813362956047, 0.06172696501016617, 0.06104838848114014, 0.060454923659563065, 0.05992376059293747, 0.05942579358816147, 0.058935653418302536, 0.058438628911972046, 0.057933591306209564, 0.05743128061294556, 0.05694982409477234, 0.056508224457502365, 0.05611925199627876, 0.055784113705158234, 0.0554928295314312, 0.05522841215133667, 0.05497246980667114, 0.05470975115895271, 0.05443093925714493, 0.054134439677000046, 0.05382580682635307, 0.05351550504565239, 0.05321527272462845, 0.05293393135070801, 0.052675265818834305, 0.05243859440088272, 0.052221089601516724, 0.05201960355043411, 0.05183109641075134, 0.05165266990661621, 0.051481056958436966, 0.05131269246339798, 0.05114433169364929, 0.05097388103604317, 0.050800621509552, 0.05062518268823624, 0.050449199974536896, 0.050274573266506195, 0.05010274797677994, 0.049934349954128265, 0.04976949468255043, 0.04960809275507927, 0.04944998398423195, 0.04929511621594429, 0.0491432249546051, 0.048994019627571106, 0.04884720966219902, 0.048702556639909744, 0.04855998605489731, 0.04841921851038933, 0.04828000068664551, 0.0481419675052166, 0.0480048768222332, 0.04786849394440651, 0.04773273691534996, 0.04759768396615982, 0.04746348410844803, 0.04733038321137428, 0.04719867557287216, 0.047068607062101364, 0.04694022610783577, 0.0468134805560112, 0.046688247472047806, 0.04656435176730156, 0.046441700309515, 0.046320028603076935, 0.046199336647987366, 0.046079523861408234, 0.04596061259508133, 0.04584251716732979, 0.045725248754024506, 0.04560887813568115, 0.045493386685848236, 0.04537886008620262, 0.04526524990797043, 0.045152612030506134, 0.04504084587097168, 0.044929973781108856, 0.04481997340917587, 0.04471076652407646, 0.04460243508219719, 0.044494934380054474, 0.04438819736242294, 0.044282298535108566, 0.044177182018756866, 0.04407281428575516, 0.04396919161081314, 0.0438663475215435, 0.04376421123743057, 0.04366287216544151, 0.04356225207448006, 0.04346234351396561, 0.04336313530802727, 0.04326457902789116, 0.04316670820116997, 0.043069493025541306, 0.04297292232513428, 0.04287698492407799, 0.042781706899404526, 0.042687054723501205, 0.04259303957223892, 0.042499616742134094, 0.042406804859638214, 0.04231465235352516, 0.04222302883863449, 0.04213201254606247, 0.04204154759645462, 0.041951678693294525, 0.04186232388019562, 0.041773535311222076, 0.041685305535793304, 0.04159758239984512, 0.04151039943099022, 0.04142371565103531, 0.04133755713701248, 0.04125189781188965, 0.04116673022508621, 0.041082050651311874, 0.04099787399172783, 0.040914136916399, 0.040830887854099274, 0.04074808210134506, 0.04066575691103935, 0.04058385640382767, 0.04050242155790329, 0.04042141139507294, 0.04034079983830452, 0.04026063531637192, 0.04018085449934006, 0.040101516991853714, 0.04002255201339722, 0.03994402289390564, 0.03986585885286331, 0.03978807479143143, 0.03971070423722267, 0.03963366150856018, 0.03955699875950813, 0.03948069363832474, 0.03940478339791298, 0.039329174906015396, 0.03925394266843796, 0.03917901590466499, 0.03910444304347038, 0.03903018683195114, 0.038956254720687866, 0.03888268768787384, 0.03880937770009041, 0.03873642161488533, 0.03866373375058174, 0.038591381162405014, 0.03851928561925888, 0.03844749182462692, 0.03837598115205765, 0.038304757326841354, 0.03823380917310715, 0.03816312551498413, 0.038092710077762604, 0.03802257031202316, 0.037952665239572525, 0.03788304701447487, 0.03781365230679512, 0.03774448484182358, 0.037675581872463226, 0.03760696202516556, 0.03753850236535072, 0.03747030347585678, 0.03740231692790985, 0.03733457252383232, 0.03726700693368912, 0.03719969466328621, 0.03713258355855942, 0.037065666168928146, 0.03699897602200508, 0.036932483315467834, 0.036866165697574615, 0.03680004924535751, 0.03673411160707474, 0.03666837140917778, 0.03660280257463455, 0.03653741627931595, 0.036472201347351074, 0.03640718013048172, 0.03634228929877281, 0.03627760335803032, 0.03621306270360947, 0.03614868223667145, 0.036084458231925964, 0.03602037951350212, 0.03595644608139992, 0.035892654210329056, 0.03582903370261192, 0.03576553985476494, 0.0357021763920784, 0.0356389582157135, 0.03557587414979935, 0.03551289811730385, 0.03545006364583969, 0.035387374460697174, 0.03532475605607033, 0.035262275487184525, 0.03519992530345917, 0.035137660801410675, 0.035075508058071136, 0.035013455897569656, 0.03495151922106743, 0.034889668226242065, 0.034827910363674164, 0.034766264259815216, 0.03470469266176224, 0.03464319556951523, 0.034581780433654785, 0.03452043980360031, 0.03445916622877121, 0.03439795598387718, 0.03433681279420853, 0.03427570313215256, 0.034214675426483154, 0.03415364772081375, 0.03409264236688614, 0.034031666815280914, 0.033970680087804794, 0.033909715712070465, 0.03384865075349808, 0.033787552267313004, 0.033726345747709274, 0.03366496413946152, 0.03360338881611824], 'val_loss': [37.663002014160156, 32.77708435058594, 28.592620849609375, 24.990039825439453, 21.866374969482422, 19.134540557861328, 16.720134735107422, 14.568554878234863, 12.643537521362305, 10.922218322753906, 9.390085220336914, 8.036715507507324, 6.8501763343811035, 5.816445827484131, 4.925721168518066, 4.168918609619141, 3.533289670944214, 3.0036959648132324, 2.5631542205810547, 2.1932430267333984, 1.8775043487548828, 1.6025991439819336, 1.36275315284729, 1.1569595336914062, 0.9868201017379761, 0.8502286076545715, 0.7421313524246216, 0.6568524837493896, 0.5883737802505493, 0.5322855710983276, 0.4857160449028015, 0.4465363621711731, 0.41301822662353516, 0.38385826349258423, 0.35814207792282104, 0.3351303040981293, 0.3140541911125183, 0.2941109538078308, 0.27465689182281494, 0.25542640686035156, 0.23660728335380554, 0.2187132090330124, 0.20230770111083984, 0.18773317337036133, 0.17499692738056183, 0.16383807361125946, 0.15388193726539612, 0.14478299021720886, 0.13631370663642883, 0.1283901184797287, 0.12104020267724991, 0.1143464669585228, 0.10839781910181046, 0.10325567424297333, 0.09892351180315018, 0.09532120078802109, 0.09228987246751785, 0.0896289274096489, 0.08714684098958969, 0.08470311760902405, 0.08223124593496323, 0.07973627746105194, 0.07727047055959702, 0.074904665350914, 0.07270984351634979, 0.07074470072984695, 0.06904079020023346, 0.06759092956781387, 0.06635235249996185, 0.06526472419500351, 0.06427431106567383, 0.06335218250751495, 0.06249813362956047, 0.06172695755958557, 0.06104838848114014, 0.060454923659563065, 0.05992376059293747, 0.05942579358816147, 0.05893564969301224, 0.058438628911972046, 0.057933591306209564, 0.05743127316236496, 0.056949831545352936, 0.056508224457502365, 0.05611925199627876, 0.055784113705158234, 0.055492836982011795, 0.05522840470075607, 0.05497246980667114, 0.05470975115895271, 0.05443093925714493, 0.054134439677000046, 0.05382580682635307, 0.05351550504565239, 0.05321527272462845, 0.05293392390012741, 0.052675265818834305, 0.05243859812617302, 0.05222109705209732, 0.05201960355043411, 0.05183109641075134, 0.05165266990661621, 0.051481056958436966, 0.05131269246339798, 0.05114433169364929, 0.05097388103604317, 0.050800621509552, 0.05062517523765564, 0.0504491925239563, 0.05027458071708679, 0.05010274052619934, 0.049934349954128265, 0.04976949468255043, 0.04960808902978897, 0.04944998770952225, 0.04929511994123459, 0.0491432249546051, 0.04899401590228081, 0.04884720593690872, 0.04870256036520004, 0.04855998605489731, 0.04841921851038933, 0.04828000068664551, 0.0481419675052166, 0.0480048730969429, 0.04786849394440651, 0.04773274064064026, 0.04759768396615982, 0.04746348410844803, 0.04733038321137428, 0.04719867929816246, 0.047068607062101364, 0.04694022610783577, 0.0468134805560112, 0.046688247472047806, 0.04656435549259186, 0.0464417040348053, 0.046320028603076935, 0.046199336647987366, 0.046079520136117935, 0.04596061259508133, 0.04584251344203949, 0.045725248754024506, 0.045608874410390854, 0.045493390411138535, 0.04537886008620262, 0.04526524990797043, 0.045152612030506134, 0.04504084959626198, 0.044929973781108856, 0.04481997340917587, 0.04471077024936676, 0.04460243880748749, 0.044494934380054474, 0.04438819736242294, 0.044282298535108566, 0.044177182018756866, 0.04407281428575516, 0.04396919161081314, 0.0438663475215435, 0.043764207512140274, 0.04366287216544151, 0.04356225207448006, 0.04346233978867531, 0.04336313158273697, 0.04326457530260086, 0.04316670820116997, 0.043069493025541306, 0.04297292232513428, 0.04287698492407799, 0.042781706899404526, 0.04268705099821091, 0.042593035846948624, 0.042499616742134094, 0.042406804859638214, 0.04231465235352516, 0.04222303256392479, 0.04213200882077217, 0.04204154759645462, 0.041951678693294525, 0.04186232015490532, 0.041773539036512375, 0.041685305535793304, 0.04159758239984512, 0.04151039570569992, 0.04142371565103531, 0.04133755713701248, 0.04125189781188965, 0.04116673022508621, 0.041082050651311874, 0.04099787026643753, 0.040914136916399, 0.040830887854099274, 0.04074808210134506, 0.040665753185749054, 0.04058385640382767, 0.04050242155790329, 0.04042141139507294, 0.04034079983830452, 0.04026063531637192, 0.04018085449934006, 0.040101516991853714, 0.040022555738687515, 0.03994402289390564, 0.03986586257815361, 0.03978807479143143, 0.03971070051193237, 0.03963366150856018, 0.03955699875950813, 0.03948069363832474, 0.03940478339791298, 0.039329174906015396, 0.03925394266843796, 0.03917901590466499, 0.03910444304347038, 0.03903019428253174, 0.038956254720687866, 0.03888268768787384, 0.03880937770009041, 0.03873641788959503, 0.03866373375058174, 0.038591381162405014, 0.03851928561925888, 0.03844749182462692, 0.03837597742676735, 0.038304757326841354, 0.03823380917310715, 0.03816312551498413, 0.038092706352472305, 0.03802257031202316, 0.03795266151428223, 0.03788304701447487, 0.03781365230679512, 0.03774448484182358, 0.037675581872463226, 0.03760696202516556, 0.03753850609064102, 0.03747030347585678, 0.03740231320261955, 0.03733457252383232, 0.03726700693368912, 0.03719969466328621, 0.03713258355855942, 0.037065666168928146, 0.03699897974729538, 0.03693248704075813, 0.036866165697574615, 0.03680004924535751, 0.03673411160707474, 0.03666836768388748, 0.03660280629992485, 0.03653741627931595, 0.036472201347351074, 0.03640718385577202, 0.03634229302406311, 0.03627760708332062, 0.03621306270360947, 0.03614868223667145, 0.036084458231925964, 0.03602037951350212, 0.035956449806690216, 0.035892654210329056, 0.03582903370261192, 0.03576553985476494, 0.0357021763920784, 0.0356389619410038, 0.03557587042450905, 0.03551289439201355, 0.03545006737112999, 0.035387374460697174, 0.035324759781360626, 0.035262275487184525, 0.03519992157816887, 0.035137660801410675, 0.03507550433278084, 0.035013455897569656, 0.03495152294635773, 0.03488966450095177, 0.034827910363674164, 0.03476626053452492, 0.03470469266176224, 0.03464319556951523, 0.034581780433654785, 0.03452043980360031, 0.03445916622877121, 0.034397952258586884, 0.03433681279420853, 0.03427570313215256, 0.034214675426483154, 0.03415364772081375, 0.03409264236688614, 0.034031666815280914, 0.033970676362514496, 0.033909715712070465, 0.03384865075349808, 0.033787548542022705, 0.03372635319828987, 0.03366496413946152, 0.03360339254140854, 0.03354150801897049]}\n",
            "{'loss': [24.10162353515625, 21.997318267822266, 20.08681297302246, 18.3447322845459, 16.75246238708496, 15.292352676391602, 13.946383476257324, 12.697199821472168, 11.529396057128906, 10.430329322814941, 9.39109992980957, 8.406949996948242, 7.476683139801025, 6.6020307540893555, 5.787146091461182, 5.037955284118652, 4.36092472076416, 3.760873317718506, 3.239345073699951, 2.7944934368133545, 2.4204609394073486, 2.107308864593506, 1.8421659469604492, 1.6119604110717773, 1.4066627025604248, 1.2202579975128174, 1.049654245376587, 0.893322765827179, 0.7509384751319885, 0.6236717700958252, 0.5133992433547974, 0.42121997475624084, 0.3468850255012512, 0.28922778367996216, 0.24628455936908722, 0.2155269831418991, 0.19429804384708405, 0.18016454577445984, 0.17108412086963654, 0.16540692746639252, 0.16177384555339813, 0.1590169370174408, 0.15614381432533264, 0.15239757299423218, 0.1473315954208374, 0.14084331691265106, 0.13312892615795135, 0.12457404285669327, 0.11562390625476837, 0.10667584091424942, 0.09802378714084625, 0.08986033499240875, 0.08230526000261307, 0.0754321962594986, 0.0692804753780365, 0.06385201215744019, 0.05910254269838333, 0.054947901517152786, 0.05128275603055954, 0.04800105094909668, 0.04501599445939064, 0.042273811995983124, 0.03975773602724075, 0.03748113662004471, 0.03547085449099541, 0.03374519199132919, 0.032297540456056595, 0.031091764569282532, 0.030071279034018517, 0.02917790785431862, 0.028368841856718063, 0.02762489952147007, 0.026948217302560806, 0.026351243257522583, 0.025843363255262375, 0.02542256936430931, 0.02507396973669529, 0.02477402053773403, 0.024498306214809418, 0.024228055030107498, 0.023953834548592567, 0.02367492765188217, 0.023395776748657227, 0.023122284561395645, 0.02285970374941826, 0.022612087428569794, 0.02238301932811737, 0.022175179794430733, 0.021989967674016953, 0.02182687819004059, 0.021683942526578903, 0.02155798114836216, 0.021445458754897118, 0.021342985332012177, 0.021247968077659607, 0.021159090101718903, 0.0210760198533535, 0.02099914476275444, 0.02092866413295269, 0.020864402875304222, 0.020805949345231056, 0.020752832293510437, 0.020704660564661026, 0.020661072805523872, 0.020621325820684433, 0.020584214478731155, 0.020548172295093536, 0.020511722192168236, 0.020474063232541084, 0.02043541893362999, 0.020396774634718895, 0.02035946026444435, 0.020324882119894028, 0.02029377967119217, 0.020266298204660416, 0.020241934806108475, 0.020219918340444565, 0.020199328660964966, 0.020179320126771927, 0.020159300416707993, 0.020138902589678764, 0.020117972046136856, 0.020096521824598312, 0.02007468417286873, 0.020052576437592506, 0.02003042958676815, 0.020008420571684837, 0.019986731931567192, 0.019965432584285736, 0.01994452252984047, 0.019924014806747437, 0.019903874024748802, 0.019884075969457626, 0.019864559173583984, 0.01984531246125698, 0.019826296716928482, 0.01980748400092125, 0.019788792356848717, 0.019770264625549316, 0.019751878455281258, 0.01973363757133484, 0.01971552148461342, 0.01969750225543976, 0.01967950351536274, 0.019661540165543556, 0.019643498584628105, 0.019625410437583923, 0.019607242196798325, 0.019589025527238846, 0.01957077719271183, 0.01955249346792698, 0.019534233957529068, 0.019515980035066605, 0.01949780061841011, 0.01947963610291481, 0.019461553543806076, 0.019443556666374207, 0.019425619393587112, 0.019407767802476883, 0.019389916211366653, 0.019372107461094856, 0.019354302436113358, 0.019336486235260963, 0.01931866630911827, 0.019300850108265877, 0.019283035770058632, 0.019265223294496536, 0.01924743689596653, 0.019229646772146225, 0.019211888313293457, 0.01919412985444069, 0.019176378846168518, 0.019158657640218735, 0.019140945747494698, 0.01912323758006096, 0.01910552568733692, 0.01908787153661251, 0.01907019317150116, 0.019052553921937943, 0.019034896045923233, 0.019017260521650314, 0.01899964176118374, 0.01898203231394291, 0.01896444335579872, 0.01894686371088028, 0.018929293379187584, 0.018911749124526978, 0.018894212320446968, 0.0188766960054636, 0.018859170377254486, 0.018841680139303207, 0.018824202939867973, 0.018806716427206993, 0.01878925785422325, 0.018771793693304062, 0.018754372373223305, 0.0187369491904974, 0.01871955767273903, 0.01870216801762581, 0.01868477836251259, 0.018667399883270264, 0.01865006610751152, 0.01863272115588188, 0.018615413457155228, 0.018598103895783424, 0.01858082413673401, 0.018563535064458847, 0.01854627951979637, 0.018529025837779045, 0.01851179078221321, 0.018494559451937675, 0.018477367237210274, 0.018460175022482872, 0.018443012610077858, 0.018425853922963142, 0.018408678472042084, 0.018391558900475502, 0.018374456092715263, 0.018357355147600174, 0.018340256065130234, 0.018323179334402084, 0.018306143581867218, 0.01828908734023571, 0.01827205717563629, 0.01825506053864956, 0.01823807880282402, 0.018221084028482437, 0.018204107880592346, 0.01818714663386345, 0.018170218914747238, 0.01815330982208252, 0.018136393278837204, 0.01811949908733368, 0.018102630972862244, 0.018085768446326256, 0.018068915233016014, 0.01805206574499607, 0.018035268411040306, 0.018018458038568497, 0.018001675605773926, 0.017984922975301743, 0.017968149855732918, 0.017951423302292824, 0.017934681847691536, 0.017917966470122337, 0.017901279032230377, 0.01788458600640297, 0.017867907881736755, 0.017851268872618675, 0.017834600061178207, 0.017817983403801918, 0.017801370471715927, 0.017784755676984787, 0.017768172547221184, 0.01775158941745758, 0.017735043540596962, 0.017718497663736343, 0.017701951786875725, 0.017685417085886, 0.01766890473663807, 0.017652399837970734, 0.017635945230722427, 0.017619449645280838, 0.017603004351258278, 0.017586547881364822, 0.01757010631263256, 0.017553679645061493, 0.017537280917167664, 0.017520857974886894, 0.017504457384347916, 0.017488079145550728, 0.017471713945269585, 0.01745535060763359, 0.01743900403380394, 0.017422674223780632, 0.017406340688467026, 0.017390016466379166, 0.0173737034201622, 0.017357412725687027, 0.01734110340476036, 0.01732480153441429, 0.017308536916971207, 0.017292264848947525, 0.017275989055633545, 0.017259735614061356, 0.017243491485714912, 0.017227236181497574, 0.01721099391579628, 0.01719476655125618, 0.017178528010845184, 0.01716230809688568, 0.017146090045571327, 0.01712987944483757, 0.017113663256168365, 0.01709745079278946, 0.01708124577999115, 0.017065048217773438, 0.01704883575439453, 0.017032641917467117, 0.017016427591443062], 'val_loss': [21.997316360473633, 20.086814880371094, 18.3447322845459, 16.75246238708496, 15.292352676391602, 13.946383476257324, 12.697199821472168, 11.529396057128906, 10.430330276489258, 9.39109992980957, 8.406949996948242, 7.476683139801025, 6.602030277252197, 5.787146091461182, 5.037955284118652, 4.36092472076416, 3.760873794555664, 3.239345073699951, 2.7944931983947754, 2.4204611778259277, 2.107308864593506, 1.8421659469604492, 1.6119604110717773, 1.4066627025604248, 1.2202579975128174, 1.049654245376587, 0.8933228254318237, 0.7509384155273438, 0.62367182970047, 0.5133992433547974, 0.42121997475624084, 0.3468850255012512, 0.28922778367996216, 0.2462845742702484, 0.2155269831418991, 0.19429802894592285, 0.18016454577445984, 0.17108412086963654, 0.16540692746639252, 0.16177384555339813, 0.1590169370174408, 0.15614381432533264, 0.15239755809307098, 0.1473315954208374, 0.14084331691265106, 0.13312892615795135, 0.12457405030727386, 0.11562390625476837, 0.10667584091424942, 0.09802379459142685, 0.08986033499240875, 0.08230525255203247, 0.0754321962594986, 0.0692804753780365, 0.06385201215744019, 0.05910254269838333, 0.054947901517152786, 0.05128275603055954, 0.04800105094909668, 0.04501599445939064, 0.042273811995983124, 0.03975773602724075, 0.03748113662004471, 0.03547085449099541, 0.03374519199132919, 0.032297540456056595, 0.031091764569282532, 0.030071279034018517, 0.02917790785431862, 0.028368836268782616, 0.02762489952147007, 0.026948219165205956, 0.02635124698281288, 0.025843366980552673, 0.02542256936430931, 0.02507396601140499, 0.02477402240037918, 0.024498306214809418, 0.024228055030107498, 0.023953834548592567, 0.02367492765188217, 0.023395776748657227, 0.023122284561395645, 0.02285970374941826, 0.022612087428569794, 0.02238301746547222, 0.022175179794430733, 0.021989967674016953, 0.021826880052685738, 0.021683942526578903, 0.02155798301100731, 0.021445458754897118, 0.021342983469367027, 0.021247968077659607, 0.021159088239073753, 0.0210760198533535, 0.02099914290010929, 0.02092866599559784, 0.020864402875304222, 0.020805949345231056, 0.020752830430865288, 0.020704660564661026, 0.02066107466816902, 0.020621325820684433, 0.020584214478731155, 0.020548174157738686, 0.020511722192168236, 0.020474061369895935, 0.02043541893362999, 0.020396772772073746, 0.02035946026444435, 0.020324882119894028, 0.02029377780854702, 0.020266298204660416, 0.020241932943463326, 0.020219918340444565, 0.020199328660964966, 0.020179320126771927, 0.020159300416707993, 0.020138902589678764, 0.020117972046136856, 0.020096521824598312, 0.02007468417286873, 0.020052576437592506, 0.02003042958676815, 0.020008418709039688, 0.019986731931567192, 0.019965430721640587, 0.01994452252984047, 0.019924014806747437, 0.0199038777500391, 0.019884074106812477, 0.019864557310938835, 0.01984531432390213, 0.019826296716928482, 0.01980748400092125, 0.019788794219493866, 0.019770264625549316, 0.01975187659263611, 0.01973363757133484, 0.01971552148461342, 0.019697504118084908, 0.01967950165271759, 0.019661542028188705, 0.019643500447273254, 0.019625410437583923, 0.019607242196798325, 0.019589025527238846, 0.01957077719271183, 0.01955249346792698, 0.019534233957529068, 0.019515980035066605, 0.01949780061841011, 0.01947963424026966, 0.019461553543806076, 0.019443556666374207, 0.019425617530941963, 0.019407765939831734, 0.019389916211366653, 0.019372107461094856, 0.01935430057346821, 0.019336486235260963, 0.01931866630911827, 0.019300851970911026, 0.019283033907413483, 0.019265225157141685, 0.01924743875861168, 0.019229646772146225, 0.019211886450648308, 0.01919412799179554, 0.019176378846168518, 0.019158657640218735, 0.019140947610139847, 0.01912323758006096, 0.01910552754998207, 0.01908787153661251, 0.01907019317150116, 0.019052553921937943, 0.019034896045923233, 0.019017260521650314, 0.01899963989853859, 0.01898203045129776, 0.01896444335579872, 0.01894686184823513, 0.018929293379187584, 0.018911749124526978, 0.01889421045780182, 0.0188766960054636, 0.018859168514609337, 0.018841682001948357, 0.018824201077222824, 0.018806714564561844, 0.01878925785422325, 0.018771793693304062, 0.018754370510578156, 0.0187369491904974, 0.018719559535384178, 0.01870216801762581, 0.01868477836251259, 0.018667398020625114, 0.01865006610751152, 0.01863272115588188, 0.018615415319800377, 0.018598102033138275, 0.01858082413673401, 0.018563535064458847, 0.01854627951979637, 0.018529025837779045, 0.01851179078221321, 0.018494559451937675, 0.018477367237210274, 0.01846017688512802, 0.018443012610077858, 0.018425853922963142, 0.018408680334687233, 0.01839156076312065, 0.018374454230070114, 0.018357355147600174, 0.018340257927775383, 0.018323179334402084, 0.01830614171922207, 0.01828908920288086, 0.01827205717563629, 0.01825506053864956, 0.01823807694017887, 0.018221084028482437, 0.018204109743237495, 0.0181871484965086, 0.018170218914747238, 0.01815330795943737, 0.018136393278837204, 0.01811949908733368, 0.018102630972862244, 0.018085766583681107, 0.018068913370370865, 0.01805206760764122, 0.018035268411040306, 0.018018458038568497, 0.018001675605773926, 0.017984922975301743, 0.017968151718378067, 0.017951423302292824, 0.017934683710336685, 0.017917966470122337, 0.017901279032230377, 0.01788458600640297, 0.017867909744381905, 0.017851267009973526, 0.017834600061178207, 0.01781798154115677, 0.017801370471715927, 0.017784755676984787, 0.017768172547221184, 0.01775159128010273, 0.017735043540596962, 0.017718497663736343, 0.017701951786875725, 0.017685417085886, 0.01766890287399292, 0.017652397975325584, 0.017635947093367577, 0.017619451507925987, 0.017603004351258278, 0.017586547881364822, 0.01757010631263256, 0.017553679645061493, 0.017537280917167664, 0.017520859837532043, 0.017504457384347916, 0.01748807728290558, 0.017471712082624435, 0.01745535060763359, 0.01743900403380394, 0.01742267608642578, 0.017406340688467026, 0.017390016466379166, 0.0173737034201622, 0.017357412725687027, 0.01734110340476036, 0.01732480153441429, 0.017308535054326057, 0.017292264848947525, 0.017275989055633545, 0.017259735614061356, 0.017243491485714912, 0.017227236181497574, 0.01721099205315113, 0.01719476841390133, 0.017178528010845184, 0.017162306234240532, 0.017146088182926178, 0.01712987944483757, 0.017113661393523216, 0.01709744893014431, 0.017081243917346, 0.017065048217773438, 0.017048833891749382, 0.017032643780112267, 0.017016425728797913, 0.01700023002922535]}\n",
            "{'loss': [21.563453674316406, 19.951112747192383, 18.486448287963867, 17.155193328857422, 15.944811820983887, 14.843707084655762, 13.841089248657227, 12.926507949829102, 12.089727401733398, 11.32090950012207, 10.61085033416748, 9.951170921325684, 9.334416389465332, 8.754127502441406, 8.20483684539795, 7.682003021240234, 7.18191385269165, 6.701634883880615, 6.239034652709961, 5.792733192443848, 5.36199426651001, 4.946639060974121, 4.5469651222229, 4.163628578186035, 3.7975382804870605, 3.449751377105713, 3.121396064758301, 2.8135383129119873, 2.527003526687622, 2.262218475341797, 2.019170045852661, 1.7974647283554077, 1.5963890552520752, 1.4149529933929443, 1.2519252300262451, 1.1058752536773682, 0.9752858877182007, 0.8586999177932739, 0.7547734975814819, 0.6622315049171448, 0.5798352956771851, 0.5065056681632996, 0.4413382112979889, 0.3834537863731384, 0.3319706320762634, 0.2861829698085785, 0.24569308757781982, 0.21031412482261658, 0.1798805296421051, 0.15414710342884064, 0.13280335068702698, 0.11547877639532089, 0.10170386731624603, 0.09090764820575714, 0.08247201144695282, 0.07582248747348785, 0.07049547135829926, 0.06614301353693008, 0.06249900907278061, 0.059335507452487946, 0.05644134804606438, 0.05362994223833084, 0.05076650530099869, 0.04779477044939995, 0.04473418742418289, 0.041647110134363174, 0.03859966993331909, 0.03564058989286423, 0.03280483931303024, 0.03012556955218315, 0.027640024200081825, 0.02538343146443367, 0.023378143087029457, 0.021628394722938538, 0.0201247651129961, 0.01885433867573738, 0.01780489832162857, 0.016958680003881454, 0.01628406159579754, 0.0157355684787035, 0.015265258029103279, 0.014835977926850319, 0.01442781649529934, 0.014034807682037354, 0.01365656964480877, 0.013292391784489155, 0.012940267100930214, 0.012599850073456764, 0.012274065054953098, 0.011968225240707397, 0.01168718934059143, 0.011433599516749382, 0.01120771374553442, 0.011008068919181824, 0.01083200890570879, 0.010675818659365177, 0.010535161010921001, 0.01040574163198471, 0.010284529067575932, 0.010170207358896732, 0.010063357651233673, 0.009965558536350727, 0.009878217242658138, 0.009801721200346947, 0.009735077619552612, 0.009676439687609673, 0.009624024853110313, 0.009576844051480293, 0.009534605778753757, 0.009497381746768951, 0.009464910253882408, 0.009436461143195629, 0.009411146864295006, 0.00938817672431469, 0.009366978891193867, 0.009347016923129559, 0.009327873587608337, 0.009309251792728901, 0.009291160851716995, 0.009273865260183811, 0.009257683530449867, 0.009242895059287548, 0.009229512885212898, 0.009217590093612671, 0.00920712947845459, 0.009198145009577274, 0.009190442971885204, 0.009183797053992748, 0.0091777965426445, 0.009172153659164906, 0.009166587144136429, 0.009161016903817654, 0.00915532000362873, 0.00914943777024746, 0.00914331991225481, 0.00913692731410265, 0.009130326099693775, 0.009123601019382477, 0.009116873145103455, 0.009110276587307453, 0.009103861637413502, 0.009097752161324024, 0.009092000313103199, 0.009086621925234795, 0.009081641212105751, 0.009076962247490883, 0.009072563610970974, 0.009068386629223824, 0.00906437635421753, 0.009060497395694256, 0.0090566985309124, 0.009052912704646587, 0.009049122221767902, 0.00904530007392168, 0.009041432291269302, 0.009037574753165245, 0.009033700451254845, 0.009029829874634743, 0.00902596302330494, 0.009022099897265434, 0.009018247947096825, 0.009014395996928215, 0.009010545909404755, 0.009006679989397526, 0.00900278240442276, 0.008998900651931763, 0.00899499747902155, 0.008991079404950142, 0.008987177163362503, 0.008983269333839417, 0.008979374542832375, 0.00897549744695425, 0.00897164922207594, 0.008967813104391098, 0.008963994681835175, 0.008960193954408169, 0.008956403471529484, 0.00895263534039259, 0.008948860689997673, 0.008945098146796227, 0.008941315114498138, 0.008937534876167774, 0.008933762088418007, 0.008929966017603874, 0.008926162496209145, 0.008922365494072437, 0.008918562904000282, 0.008914761245250702, 0.0089109493419528, 0.008907130919396877, 0.008903316222131252, 0.008899503387510777, 0.00889570266008377, 0.008891874924302101, 0.008888063952326775, 0.00888424925506115, 0.00888043362647295, 0.008876627311110497, 0.008872805163264275, 0.008868987672030926, 0.008865172043442726, 0.008861349895596504, 0.008857527747750282, 0.00885370559990406, 0.00884988158941269, 0.008846056647598743, 0.008842241019010544, 0.008838404901325703, 0.00883458647876978, 0.008830754086375237, 0.008826931938529015, 0.008823102340102196, 0.00881926529109478, 0.008815428242087364, 0.008811598643660545, 0.008807764388620853, 0.008803939446806908, 0.008800105191767216, 0.00879625603556633, 0.008792429231107235, 0.008788577280938625, 0.008784748613834381, 0.00878090225160122, 0.008777061477303505, 0.00877322256565094, 0.008769377134740353, 0.008765525184571743, 0.008761687204241753, 0.008757846429944038, 0.008754000067710876, 0.008750146254897118, 0.00874630268663168, 0.008742456324398518, 0.008738609962165356, 0.008734758011996746, 0.00873091071844101, 0.00872706063091755, 0.008723212406039238, 0.008719360455870628, 0.008715510368347168, 0.008711656555533409, 0.008707800880074501, 0.008703954517841339, 0.008700101636350155, 0.00869624875485897, 0.008692390285432339, 0.00868853647261858, 0.008684680797159672, 0.008680824190378189, 0.008676974102854729, 0.008673114702105522, 0.008669259026646614, 0.008665400557219982, 0.008661549538373947, 0.008657689206302166, 0.008653833530843258, 0.008649973198771477, 0.00864611379802227, 0.008642273023724556, 0.008638398721814156, 0.008634544909000397, 0.008630688302218914, 0.00862683542072773, 0.008622979745268822, 0.008619111962616444, 0.008615254424512386, 0.008611395955085754, 0.008607538416981697, 0.008603681810200214, 0.00859983079135418, 0.008595970459282398, 0.008592105470597744, 0.008588247001171112, 0.008584396913647652, 0.008580539375543594, 0.008576678112149239, 0.00857282243669033, 0.008568964898586273, 0.00856509804725647, 0.008561244234442711, 0.008557388558983803, 0.008553529158234596, 0.008549679070711136, 0.008545820601284504, 0.008541961200535297, 0.00853810179978609, 0.008534247986972332, 0.008530392311513424, 0.008526533842086792, 0.008522670716047287, 0.008518831804394722, 0.00851496309041977, 0.008511113002896309, 0.008507249876856804, 0.00850340723991394, 0.00849954318255186, 0.008495686575770378, 0.00849183090031147, 0.008487979881465435, 0.008484121412038803], 'val_loss': [19.951112747192383, 18.4864501953125, 17.155193328857422, 15.944811820983887, 14.843707084655762, 13.841089248657227, 12.926507949829102, 12.089727401733398, 11.32090950012207, 10.610851287841797, 9.951169967651367, 9.334416389465332, 8.754127502441406, 8.20483684539795, 7.682003021240234, 7.181914329528809, 6.701634407043457, 6.239034175872803, 5.792733192443848, 5.36199426651001, 4.946639060974121, 4.546964168548584, 4.163629055023193, 3.7975380420684814, 3.449751377105713, 3.1213958263397217, 2.8135383129119873, 2.527003526687622, 2.262218475341797, 2.019170045852661, 1.7974647283554077, 1.5963890552520752, 1.4149529933929443, 1.2519252300262451, 1.1058752536773682, 0.9752858877182007, 0.8586999177932739, 0.7547734975814819, 0.6622315049171448, 0.5798353552818298, 0.5065056681632996, 0.4413382112979889, 0.3834537863731384, 0.3319706320762634, 0.28618302941322327, 0.24569305777549744, 0.21031410992145538, 0.1798805296421051, 0.15414710342884064, 0.13280335068702698, 0.11547879129648209, 0.10170385986566544, 0.09090764820575714, 0.08247201144695282, 0.07582248747348785, 0.07049547135829926, 0.06614301353693008, 0.06249900907278061, 0.059335507452487946, 0.05644134432077408, 0.05362994223833084, 0.05076650530099869, 0.04779477044939995, 0.04473419114947319, 0.041647110134363174, 0.03859966993331909, 0.035640593618154526, 0.03280483931303024, 0.03012556955218315, 0.027640024200081825, 0.02538343146443367, 0.023378143087029457, 0.021628394722938538, 0.020124763250350952, 0.01885433867573738, 0.01780489645898342, 0.016958680003881454, 0.01628405973315239, 0.0157355684787035, 0.015265258029103279, 0.014835977926850319, 0.01442781649529934, 0.014034805819392204, 0.01365656964480877, 0.013292391784489155, 0.012940267100930214, 0.012599850073456764, 0.012274065986275673, 0.011968226172029972, 0.011687190271914005, 0.011433597654104233, 0.01120771374553442, 0.011008068919181824, 0.01083200890570879, 0.010675817728042603, 0.010535160079598427, 0.01040574163198471, 0.010284529067575932, 0.010170208290219307, 0.010063357651233673, 0.009965559467673302, 0.009878217242658138, 0.009801720269024372, 0.009735076688230038, 0.009676439687609673, 0.009624024853110313, 0.009576844051480293, 0.009534605778753757, 0.009497382678091526, 0.009464911185204983, 0.009436460211873055, 0.009411146864295006, 0.009388175792992115, 0.009366977959871292, 0.009347016923129559, 0.009327872656285763, 0.009309251792728901, 0.009291160851716995, 0.009273864328861237, 0.009257683530449867, 0.009242896921932697, 0.009229512885212898, 0.009217589162290096, 0.009207130409777164, 0.009198145009577274, 0.009190442971885204, 0.009183797053992748, 0.0091777965426445, 0.009172153659164906, 0.009166588075459003, 0.009161016903817654, 0.00915532000362873, 0.00914943777024746, 0.00914331991225481, 0.00913692731410265, 0.0091303251683712, 0.009123601019382477, 0.009116873145103455, 0.009110276587307453, 0.009103861637413502, 0.009097752161324024, 0.009092000313103199, 0.00908662285655737, 0.009081641212105751, 0.009076962247490883, 0.009072563610970974, 0.009068385697901249, 0.009064377285540104, 0.009060497395694256, 0.009056697599589825, 0.009052912704646587, 0.009049122221767902, 0.009045299142599106, 0.009041432291269302, 0.009037574753165245, 0.009033700451254845, 0.009029829874634743, 0.00902596302330494, 0.00902209896594286, 0.009018247947096825, 0.009014395996928215, 0.00901054497808218, 0.009006679989397526, 0.00900278240442276, 0.008998901583254337, 0.00899499747902155, 0.008991079404950142, 0.008987177163362503, 0.008983269333839417, 0.008979374542832375, 0.00897549744695425, 0.00897164922207594, 0.008967812173068523, 0.008963994681835175, 0.008960193954408169, 0.008956403471529484, 0.008952634409070015, 0.008948861621320248, 0.008945098146796227, 0.008941314183175564, 0.008937534876167774, 0.008933762088418007, 0.008929966017603874, 0.008926162496209145, 0.008922364562749863, 0.008918562904000282, 0.008914760313928127, 0.0089109493419528, 0.008907130919396877, 0.008903316222131252, 0.008899503387510777, 0.00889570266008377, 0.008891873992979527, 0.0088880630210042, 0.00888424925506115, 0.00888043362647295, 0.008876627311110497, 0.008872805163264275, 0.008868987672030926, 0.008865172043442726, 0.008861349895596504, 0.008857528679072857, 0.00885370559990406, 0.008849880658090115, 0.008846056647598743, 0.008842241950333118, 0.008838404901325703, 0.00883458647876978, 0.008830754086375237, 0.00882693286985159, 0.00882310327142477, 0.00881926529109478, 0.008815428242087364, 0.008811598643660545, 0.008807764388620853, 0.008803939446806908, 0.00880010612308979, 0.00879625603556633, 0.00879242829978466, 0.0087885782122612, 0.008784748613834381, 0.00878090225160122, 0.008777061477303505, 0.00877322256565094, 0.008769377134740353, 0.008765525184571743, 0.008761687204241753, 0.008757846429944038, 0.008753999136388302, 0.008750146254897118, 0.008746303617954254, 0.008742456324398518, 0.008738609962165356, 0.008734758011996746, 0.00873091071844101, 0.00872706063091755, 0.008723212406039238, 0.008719360455870628, 0.008715509437024593, 0.008711656555533409, 0.008707801811397076, 0.008703953586518764, 0.00870010256767273, 0.00869624875485897, 0.008692390285432339, 0.008688535541296005, 0.008684680797159672, 0.008680823259055614, 0.008676974102854729, 0.008673114702105522, 0.008669259026646614, 0.008665401488542557, 0.008661548607051373, 0.008657689206302166, 0.008653833530843258, 0.008649973198771477, 0.00864611379802227, 0.00864227395504713, 0.008638398721814156, 0.008634544909000397, 0.008630688302218914, 0.008626834489405155, 0.008622979745268822, 0.008619111962616444, 0.008615255355834961, 0.008611395955085754, 0.008607539348304272, 0.00860368087887764, 0.00859983079135418, 0.008595970459282398, 0.008592105470597744, 0.008588247001171112, 0.008584396913647652, 0.008580539375543594, 0.008576679043471813, 0.00857282243669033, 0.008568963967263699, 0.008565098978579044, 0.008561244234442711, 0.008557388558983803, 0.008553529158234596, 0.008549679070711136, 0.008545820601284504, 0.008541961200535297, 0.00853810179978609, 0.008534247986972332, 0.00853039138019085, 0.008526533842086792, 0.008522670716047287, 0.008518831804394722, 0.008514962159097195, 0.008511113002896309, 0.008507249876856804, 0.00850340723991394, 0.00849954318255186, 0.008495686575770378, 0.00849183090031147, 0.008487979881465435, 0.008484120480716228, 0.008480273187160492]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIr-WPy40iWo",
        "outputId": "c66210b8-43af-45fd-f914-71f323b63630"
      },
      "source": [
        "sae = StackedAutoencoder([128, 60], activation='LeakyReLU')\n",
        "sae.fit(bert_embeddings128, bert_embeddings128, False, 100)\n",
        "print(mse(bert_embeddings128, sae.decode(sae.encode(bert_embeddings128))))\n",
        "for hist in sae.model_histories:\n",
        "    print(hist.history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.16816044\n",
            "{'loss': [3.005258798599243, 0.9095508456230164, 0.5216407775878906, 0.36793428659439087, 0.292940616607666, 0.25112423300743103, 0.22712889313697815, 0.21194981038570404, 0.20094965398311615, 0.1945820152759552, 0.18938370048999786, 0.1859092265367508, 0.1835748702287674, 0.18148097395896912, 0.18009361624717712, 0.17882610857486725, 0.1780598908662796, 0.17724743485450745, 0.1768476665019989, 0.17654956877231598, 0.17600736021995544, 0.17589886486530304, 0.17596274614334106, 0.1757695972919464, 0.17557445168495178, 0.17552974820137024, 0.17529211938381195, 0.17527073621749878, 0.17491771280765533, 0.17519521713256836, 0.17472699284553528, 0.17516914010047913, 0.17467117309570312, 0.17483510076999664, 0.1747017353773117, 0.1747927963733673, 0.17462816834449768, 0.17439298331737518, 0.17434479296207428, 0.17439186573028564, 0.17454387247562408, 0.17436771094799042, 0.17440471053123474, 0.17403638362884521, 0.17410293221473694, 0.174106165766716, 0.17418357729911804, 0.17398594319820404, 0.17379148304462433, 0.17401474714279175, 0.1736864596605301, 0.173749640583992, 0.17368391156196594, 0.1736086755990982, 0.17361342906951904, 0.17359404265880585, 0.1735541671514511, 0.1733500212430954, 0.17372216284275055, 0.17321191728115082, 0.173595130443573, 0.1731763482093811, 0.17351900041103363, 0.17319731414318085, 0.17333672940731049, 0.17304423451423645, 0.17339199781417847, 0.17336685955524445, 0.1729159951210022, 0.1731899380683899, 0.17320853471755981, 0.17310991883277893, 0.1730741709470749, 0.1730809509754181, 0.17288215458393097, 0.1729665994644165, 0.1727723926305771, 0.17286303639411926, 0.17265909910202026, 0.1729823499917984, 0.17284004390239716, 0.17279280722141266, 0.17257440090179443, 0.17284183204174042, 0.17287704348564148, 0.1727779507637024, 0.17251591384410858, 0.1724245846271515, 0.1725068837404251, 0.17252129316329956, 0.17218764126300812, 0.17238397896289825, 0.17239224910736084, 0.17230966687202454, 0.17221036553382874, 0.17210069298744202, 0.17228713631629944, 0.17223800718784332, 0.17224587500095367, 0.17210833728313446], 'val_loss': [1.2925300598144531, 0.6485395431518555, 0.4238167107105255, 0.324186235666275, 0.2676195204257965, 0.23691794276237488, 0.21946457028388977, 0.20834897458553314, 0.19498616456985474, 0.19126632809638977, 0.18402478098869324, 0.18268923461437225, 0.18397970497608185, 0.17893578112125397, 0.17739473283290863, 0.17624716460704803, 0.17841480672359467, 0.1769561469554901, 0.17565865814685822, 0.17480139434337616, 0.17391696572303772, 0.17540393769741058, 0.1763053983449936, 0.17541247606277466, 0.17537224292755127, 0.17414109408855438, 0.17586985230445862, 0.17428384721279144, 0.17514988780021667, 0.17374536395072937, 0.1724427491426468, 0.1743074655532837, 0.17431175708770752, 0.17542804777622223, 0.17513670027256012, 0.17590323090553284, 0.17488670349121094, 0.17481844127178192, 0.17285974323749542, 0.17449353635311127, 0.17210356891155243, 0.17504628002643585, 0.1745755821466446, 0.1718091368675232, 0.17278482019901276, 0.17246749997138977, 0.17321504652500153, 0.1726425141096115, 0.17282018065452576, 0.17455445230007172, 0.17241856455802917, 0.1739019900560379, 0.1738780438899994, 0.17289447784423828, 0.17344960570335388, 0.175774484872818, 0.1728346049785614, 0.1724667251110077, 0.17172688245773315, 0.17194101214408875, 0.17265617847442627, 0.1730266958475113, 0.1718461513519287, 0.17411436140537262, 0.17230626940727234, 0.1724805235862732, 0.1742372065782547, 0.1734033077955246, 0.17334845662117004, 0.17239178717136383, 0.17084848880767822, 0.17253577709197998, 0.17305180430412292, 0.17355412244796753, 0.17155154049396515, 0.17236965894699097, 0.17135977745056152, 0.17178219556808472, 0.17296113073825836, 0.17276503145694733, 0.17276489734649658, 0.1737370789051056, 0.17241200804710388, 0.17346759140491486, 0.17141841351985931, 0.17176441848278046, 0.17310112714767456, 0.17161208391189575, 0.17109544575214386, 0.17086957395076752, 0.17241191864013672, 0.17166796326637268, 0.17126578092575073, 0.17097002267837524, 0.17072303593158722, 0.17182743549346924, 0.17198175191879272, 0.1710924357175827, 0.17266793549060822, 0.17052391171455383]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gweugJu-0qx8",
        "outputId": "51318eaf-3cfb-4d8a-b96c-cf808c2d62f2"
      },
      "source": [
        "sae = StackedAutoencoder([128, 100, 80, 60], activation='LeakyReLU')\n",
        "sae.fit(bert_embeddings128, bert_embeddings128, False, 100)\n",
        "print(mse(bert_embeddings128, sae.decode(sae.encode(bert_embeddings128))))\n",
        "for hist in sae.model_histories:\n",
        "    print(hist.history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.16569304\n",
            "{'loss': [2.304692506790161, 0.4354251027107239, 0.20753373205661774, 0.1344689577817917, 0.09947392344474792, 0.07872293889522552, 0.06519557535648346, 0.05540331080555916, 0.04821813479065895, 0.04222717881202698, 0.038041599094867706, 0.03450434282422066, 0.03170615807175636, 0.029536304995417595, 0.027685681357979774, 0.025978758931159973, 0.024632157757878304, 0.023580655455589294, 0.022675756365060806, 0.021928861737251282, 0.021466528996825218, 0.021027905866503716, 0.02086571790277958, 0.02053164690732956, 0.020185116678476334, 0.020177969709038734, 0.019981056451797485, 0.01973128318786621, 0.019723016768693924, 0.019561350345611572, 0.0194565337151289, 0.019412066787481308, 0.018969183787703514, 0.01900487020611763, 0.018810315057635307, 0.018827050924301147, 0.01885385997593403, 0.018622426316142082, 0.018636180087924004, 0.01860751397907734, 0.01844205893576145, 0.01841810904443264, 0.018229275941848755, 0.018429987132549286, 0.01833847351372242, 0.01839747279882431, 0.01822931505739689, 0.018314041197299957, 0.018323061987757683, 0.018189720809459686, 0.01833386905491352, 0.01810956932604313, 0.01813419535756111, 0.01816425658762455, 0.018048008903861046, 0.01805203966796398, 0.017970925197005272, 0.01820048876106739, 0.018113043159246445, 0.017924662679433823, 0.01802736707031727, 0.01784364879131317, 0.017866358160972595, 0.018082136288285255, 0.017835469916462898, 0.01794053241610527, 0.017864983528852463, 0.017869817093014717, 0.018002372235059738, 0.017983682453632355, 0.01792393997311592, 0.017796499654650688, 0.018030501902103424, 0.017699560150504112, 0.017920145764946938, 0.01778334565460682, 0.01777304708957672, 0.017672931775450706, 0.017863677814602852, 0.017867062240839005, 0.01763116382062435, 0.01779569871723652, 0.017882727086544037, 0.017657356336712837, 0.017651516944169998, 0.01781470514833927, 0.017642008140683174, 0.01790320873260498, 0.01744833029806614, 0.01773785427212715, 0.01765698567032814, 0.017661796882748604, 0.017723744735121727, 0.017526138573884964, 0.017562976107001305, 0.01772971637547016, 0.017548400908708572, 0.017699673771858215, 0.017728732898831367, 0.017689673230051994], 'val_loss': [0.7083624601364136, 0.2733418047428131, 0.15952855348587036, 0.11278669536113739, 0.08713464438915253, 0.07093139737844467, 0.05916694924235344, 0.05103575810790062, 0.04526092857122421, 0.03914306312799454, 0.03499023988842964, 0.03303513303399086, 0.02982521988451481, 0.030272774398326874, 0.02663065493106842, 0.025378311052918434, 0.023939069360494614, 0.02329866774380207, 0.021485304459929466, 0.02131706289947033, 0.020842978730797768, 0.020895175635814667, 0.020254377275705338, 0.020169558003544807, 0.020443664863705635, 0.02050217241048813, 0.019681023433804512, 0.01991843618452549, 0.019016116857528687, 0.019551953300833702, 0.019101180136203766, 0.019460681825876236, 0.019429055973887444, 0.018575411289930344, 0.019363809376955032, 0.018155956640839577, 0.01857062056660652, 0.01790560781955719, 0.019267702475190163, 0.01872202940285206, 0.01830320432782173, 0.01816735230386257, 0.018711449578404427, 0.018112363293766975, 0.01884726993739605, 0.017775679007172585, 0.01840592734515667, 0.01769302785396576, 0.018108222633600235, 0.01768215373158455, 0.017984474077820778, 0.017474250867962837, 0.018286097794771194, 0.018993636593222618, 0.01913066767156124, 0.018503982573747635, 0.01786460168659687, 0.01773729920387268, 0.01760956272482872, 0.018032575026154518, 0.01800485886633396, 0.017664887011051178, 0.017858371138572693, 0.017335454002022743, 0.01759783923625946, 0.017986677587032318, 0.017828622832894325, 0.019112378358840942, 0.01847655698657036, 0.018385518342256546, 0.017480047419667244, 0.019300352782011032, 0.01692201755940914, 0.018343111500144005, 0.017624523490667343, 0.018253391608595848, 0.017405705526471138, 0.018282394856214523, 0.017088282853364944, 0.017501218244433403, 0.01726371794939041, 0.017673900350928307, 0.017609773203730583, 0.016994556412100792, 0.017349723726511, 0.019452951848506927, 0.017577726393938065, 0.018626319244503975, 0.017047611996531487, 0.016848405823111534, 0.017125949263572693, 0.017510656267404556, 0.017098287120461464, 0.01704096607863903, 0.01724025420844555, 0.017396429553627968, 0.017563575878739357, 0.017617492005228996, 0.017584946006536484, 0.018132729455828667]}\n",
            "{'loss': [1.8352112770080566, 0.6085776686668396, 0.3046000599861145, 0.17753659188747406, 0.12199480831623077, 0.0913378968834877, 0.07192069292068481, 0.05856025218963623, 0.04909755662083626, 0.04294375702738762, 0.038816120475530624, 0.03619114309549332, 0.03422574698925018, 0.03300339728593826, 0.03183320164680481, 0.031230997294187546, 0.030710525810718536, 0.030351659283041954, 0.030038267374038696, 0.02978082001209259, 0.02957586944103241, 0.02945150062441826, 0.02927437610924244, 0.02898216061294079, 0.028885606676340103, 0.028915908187627792, 0.028774341568350792, 0.02866123802959919, 0.028461996465921402, 0.028437400236725807, 0.028485499322414398, 0.028301507234573364, 0.028271986171603203, 0.028211912140250206, 0.02817956544458866, 0.02816673554480076, 0.028128929436206818, 0.02802659384906292, 0.027891872450709343, 0.028012419119477272, 0.02802138403058052, 0.027867913246154785, 0.02797025814652443, 0.02792169339954853, 0.027874257415533066, 0.027793990448117256, 0.02779933251440525, 0.027690986171364784, 0.02778848633170128, 0.02781675197184086, 0.027840018272399902, 0.027771146968007088, 0.027678485959768295, 0.027683347463607788, 0.027766214683651924, 0.027700552716851234, 0.027677670121192932, 0.02771286852657795, 0.027558855712413788, 0.027697928249835968, 0.027591755613684654, 0.027528678998351097, 0.027452778071165085, 0.02772289514541626, 0.027619389817118645, 0.027502480894327164, 0.027623049914836884, 0.02746175043284893, 0.0275819543749094, 0.027446577325463295, 0.02741233818233013, 0.02756533771753311, 0.027491718530654907, 0.027434302493929863, 0.027411524206399918, 0.027414623647928238, 0.027466749772429466, 0.027352092787623405, 0.027496851980686188, 0.027362344786524773, 0.027366358786821365, 0.027333678677678108, 0.027264991775155067, 0.027373334392905235, 0.027293968945741653, 0.027332728728652, 0.0272570438683033, 0.027378125116229057, 0.027348211035132408, 0.027276815846562386, 0.02727043256163597, 0.02726380154490471, 0.027270620688796043, 0.02723662368953228, 0.02720247209072113, 0.027194129303097725, 0.027222242206335068, 0.02718096598982811, 0.027231452986598015, 0.0272133257240057], 'val_loss': [0.884160578250885, 0.41143929958343506, 0.22252582013607025, 0.14299295842647552, 0.10286257416009903, 0.07970619201660156, 0.06439781934022903, 0.05294540897011757, 0.045327331870794296, 0.04051836207509041, 0.03681258484721184, 0.03526363521814346, 0.03319457173347473, 0.03243502601981163, 0.030928224325180054, 0.030526697635650635, 0.029538560658693314, 0.03068695217370987, 0.02914605848491192, 0.029161889106035233, 0.0289031732827425, 0.029271310195326805, 0.028345145285129547, 0.02908514440059662, 0.029184453189373016, 0.02821405418217182, 0.02839290350675583, 0.02790411189198494, 0.028513379395008087, 0.028355609625577927, 0.02882903441786766, 0.028269700706005096, 0.029213130474090576, 0.028036991134285927, 0.02875402569770813, 0.028574803844094276, 0.028129708021879196, 0.028049280866980553, 0.027678461745381355, 0.028314044699072838, 0.02711240015923977, 0.027808599174022675, 0.028960980474948883, 0.028494609519839287, 0.027547748759388924, 0.027603980153799057, 0.02761458419263363, 0.026888564229011536, 0.02880129963159561, 0.02730775997042656, 0.027389399707317352, 0.028448348864912987, 0.027563685551285744, 0.02841205894947052, 0.027455680072307587, 0.027415096759796143, 0.02805183082818985, 0.02777092345058918, 0.027970625087618828, 0.02748376503586769, 0.027612723410129547, 0.027373863384127617, 0.02737664431333542, 0.028689727187156677, 0.027687791734933853, 0.027085183188319206, 0.02790936268866062, 0.027312081307172775, 0.02715606987476349, 0.02693616785109043, 0.027047496289014816, 0.02790992148220539, 0.027510887011885643, 0.027264665812253952, 0.02693905495107174, 0.027003083378076553, 0.026979228481650352, 0.026812376454472542, 0.02753947675228119, 0.02768917754292488, 0.02761451154947281, 0.027054650709033012, 0.027372129261493683, 0.027683869004249573, 0.027041694149374962, 0.026893753558397293, 0.02678610198199749, 0.0270843468606472, 0.027705729007720947, 0.027275528758764267, 0.026602067053318024, 0.02685566060245037, 0.02743835188448429, 0.02745218202471733, 0.026504402980208397, 0.02643943764269352, 0.027075741440057755, 0.02746248058974743, 0.02791590802371502, 0.027332501485943794]}\n",
            "{'loss': [1.1037019491195679, 0.41999268531799316, 0.23962770402431488, 0.15510879456996918, 0.10918924957513809, 0.08201566338539124, 0.06657124310731888, 0.05766766518354416, 0.052781760692596436, 0.050158631056547165, 0.04854581877589226, 0.04770743101835251, 0.046995021402835846, 0.04659656062722206, 0.046153511852025986, 0.04587280750274658, 0.045642223209142685, 0.04546978324651718, 0.045197196304798126, 0.045085012912750244, 0.0449681282043457, 0.04467882215976715, 0.044567108154296875, 0.04445113241672516, 0.0444047674536705, 0.044220034033060074, 0.04410259425640106, 0.04407750815153122, 0.043960072100162506, 0.043896451592445374, 0.04383980110287666, 0.04368290305137634, 0.043647632002830505, 0.04366442561149597, 0.04357326775789261, 0.04344014823436737, 0.043410882353782654, 0.04334240034222603, 0.043313972651958466, 0.043303269892930984, 0.043342381715774536, 0.04326238855719566, 0.043125055730342865, 0.04312574490904808, 0.04308229312300682, 0.04308426007628441, 0.04302194342017174, 0.04296544939279556, 0.04294031485915184, 0.04296672344207764, 0.04290376231074333, 0.042835235595703125, 0.04279882833361626, 0.04279700666666031, 0.04270217940211296, 0.04274290055036545, 0.042725786566734314, 0.04270415008068085, 0.04264553263783455, 0.04257119446992874, 0.04258269444108009, 0.04261734336614609, 0.042444486171007156, 0.04248621314764023, 0.04242445528507233, 0.042461443692445755, 0.042514555156230927, 0.04239648953080177, 0.04238015040755272, 0.04228086397051811, 0.0423131100833416, 0.04223734140396118, 0.042217426002025604, 0.04214426130056381, 0.0421370230615139, 0.042154472321271896, 0.04217533394694328, 0.04204948619008064, 0.04200921207666397, 0.04197252169251442, 0.04198620840907097, 0.04190164431929588, 0.04191460832953453, 0.041853781789541245, 0.041807521134614944, 0.041754771023988724, 0.041758205741643906, 0.04176108166575432, 0.04172428324818611, 0.04166167601943016, 0.04167339950799942, 0.04165264964103699, 0.041635770350694656, 0.04162545129656792, 0.04159535840153694, 0.04156448692083359, 0.041582245379686356, 0.04151232913136482, 0.04147019609808922, 0.041426368057727814], 'val_loss': [0.5648159980773926, 0.3047611713409424, 0.18773998320102692, 0.12845803797245026, 0.09323325753211975, 0.072733074426651, 0.06154576316475868, 0.055257149040699005, 0.05092053860425949, 0.049275610595941544, 0.047641631215810776, 0.04702135547995567, 0.046171240508556366, 0.04645388945937157, 0.04609834402799606, 0.04592972248792648, 0.045897845178842545, 0.04511384665966034, 0.04487711563706398, 0.04507922753691673, 0.044600650668144226, 0.044969767332077026, 0.0443933829665184, 0.043847549706697464, 0.045008137822151184, 0.0437028631567955, 0.04359034076333046, 0.04378873482346535, 0.04432883858680725, 0.043421365320682526, 0.04326913505792618, 0.04412863403558731, 0.04398217052221298, 0.04344659671187401, 0.04311792179942131, 0.04325789958238602, 0.044078148901462555, 0.043475039303302765, 0.0430474616587162, 0.04404793679714203, 0.0432913601398468, 0.0426168255507946, 0.04289469122886658, 0.04281315952539444, 0.04356881231069565, 0.04298025369644165, 0.04274323955178261, 0.04294212907552719, 0.04298179969191551, 0.042609985917806625, 0.04266396164894104, 0.04251417517662048, 0.04284120723605156, 0.042461954057216644, 0.04289701581001282, 0.04273148253560066, 0.042711976915597916, 0.042175278067588806, 0.04218019172549248, 0.04229913279414177, 0.042855191975831985, 0.04288921505212784, 0.04220547527074814, 0.042128786444664, 0.04233819991350174, 0.04228275641798973, 0.04205622151494026, 0.04243660345673561, 0.04215873405337334, 0.042270682752132416, 0.042085252702236176, 0.04220132902264595, 0.04206112027168274, 0.0416659414768219, 0.04179392009973526, 0.04210575670003891, 0.04171576350927353, 0.04154808074235916, 0.041925098747015, 0.04209638014435768, 0.041597820818424225, 0.04164055734872818, 0.041611216962337494, 0.04172982648015022, 0.04136383906006813, 0.04148669168353081, 0.04146842658519745, 0.041127607226371765, 0.041467200964689255, 0.04139814153313637, 0.04169072210788727, 0.04187730327248573, 0.04228431358933449, 0.041473113000392914, 0.041284672915935516, 0.04160945862531662, 0.04102274030447006, 0.041089970618486404, 0.041525475680828094, 0.041243236511945724]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZWhIVPM0583",
        "outputId": "e9d8b78e-29d9-4dfe-ae29-45e01040fbd1"
      },
      "source": [
        "sae = StackedAutoencoder([128, 60], activation=None)\n",
        "sae.fit(bert_embeddings128, bert_embeddings128, False, 100)\n",
        "print(mse(bert_embeddings128, sae.decode(sae.encode(bert_embeddings128))))\n",
        "for hist in sae.model_histories:\n",
        "    print(hist.history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.16536775\n",
            "{'loss': [3.0614612102508545, 0.7584656476974487, 0.4337124228477478, 0.31008464097976685, 0.25070953369140625, 0.2190694510936737, 0.20081357657909393, 0.1902027428150177, 0.18400628864765167, 0.18015360832214355, 0.17782457172870636, 0.17643608152866364, 0.17533081769943237, 0.17473837733268738, 0.17422747611999512, 0.17411702871322632, 0.17369700968265533, 0.17338915169239044, 0.17346680164337158, 0.17307911813259125, 0.17280453443527222, 0.17298433184623718, 0.17289739847183228, 0.17264413833618164, 0.17257250845432281, 0.1725328117609024, 0.17261305451393127, 0.17232760787010193, 0.17216983437538147, 0.17206670343875885, 0.17219629883766174, 0.17224469780921936, 0.17190630733966827, 0.17197254300117493, 0.171915203332901, 0.1720544546842575, 0.17194178700447083, 0.17168253660202026, 0.17193922400474548, 0.17150966823101044, 0.17171834409236908, 0.17182862758636475, 0.17151787877082825, 0.17164163291454315, 0.17193832993507385, 0.17107535898685455, 0.1724051982164383, 0.1713443249464035, 0.17070643603801727, 0.17117950320243835, 0.171800896525383, 0.170888751745224, 0.17122900485992432, 0.17104995250701904, 0.17114882171154022, 0.17096155881881714, 0.17123468220233917, 0.17069032788276672, 0.17114153504371643, 0.17081214487552643, 0.17090992629528046, 0.17055650055408478, 0.17076164484024048, 0.1709785908460617, 0.17057321965694427, 0.17087912559509277, 0.1705615222454071, 0.17071130871772766, 0.17129544913768768, 0.17027075588703156, 0.17079555988311768, 0.1699339598417282, 0.1710454672574997, 0.17011378705501556, 0.17011584341526031, 0.1705145239830017, 0.1703750044107437, 0.17017298936843872, 0.17022337019443512, 0.17013494670391083, 0.17009548842906952, 0.16997022926807404, 0.17092642188072205, 0.16969755291938782, 0.1697610318660736, 0.17030249536037445, 0.16978205740451813, 0.1697475016117096, 0.17006556689739227, 0.16971455514431, 0.16989533603191376, 0.16943222284317017, 0.17006832361221313, 0.16951406002044678, 0.16952680051326752, 0.17012962698936462, 0.16918601095676422, 0.1695988029241562, 0.1693870574235916, 0.16931188106536865], 'val_loss': [1.0883458852767944, 0.536672055721283, 0.3544618785381317, 0.272559255361557, 0.23019520938396454, 0.2075265347957611, 0.19406622648239136, 0.18540756404399872, 0.18125782907009125, 0.17770707607269287, 0.17580118775367737, 0.1750744879245758, 0.17471036314964294, 0.1730518490076065, 0.17300312221050262, 0.17299452424049377, 0.1723700314760208, 0.17386360466480255, 0.17190946638584137, 0.17256219685077667, 0.17156895995140076, 0.17141494154930115, 0.17153272032737732, 0.17217694222927094, 0.17116712033748627, 0.17137357592582703, 0.17110881209373474, 0.17142026126384735, 0.1723649501800537, 0.17562998831272125, 0.17093992233276367, 0.17115142941474915, 0.1715216338634491, 0.17079971730709076, 0.17053981125354767, 0.17009857296943665, 0.17030440270900726, 0.17043499648571014, 0.17061437666416168, 0.1705528199672699, 0.17034278810024261, 0.1708126813173294, 0.17079715430736542, 0.17088648676872253, 0.17055192589759827, 0.17129532992839813, 0.16998997330665588, 0.1702568531036377, 0.1695227175951004, 0.1700727939605713, 0.17352324724197388, 0.17008456587791443, 0.17038246989250183, 0.1711212694644928, 0.17143972218036652, 0.16980311274528503, 0.17059604823589325, 0.16971389949321747, 0.17225255072116852, 0.17003676295280457, 0.16970118880271912, 0.1699853539466858, 0.169511079788208, 0.17370358109474182, 0.1698489487171173, 0.1692303717136383, 0.16887879371643066, 0.17712244391441345, 0.16897639632225037, 0.16925449669361115, 0.1696196347475052, 0.16974182426929474, 0.1687382161617279, 0.16950644552707672, 0.16866464912891388, 0.16876989603042603, 0.16948610544204712, 0.16915522515773773, 0.1691742092370987, 0.1689937561750412, 0.169234961271286, 0.16891060769557953, 0.16884547472000122, 0.17016582190990448, 0.17007823288440704, 0.1690998673439026, 0.1691187173128128, 0.16873395442962646, 0.16915874183177948, 0.1686655580997467, 0.16852465271949768, 0.16854125261306763, 0.16818423569202423, 0.16922006011009216, 0.16827480494976044, 0.16816651821136475, 0.1682359278202057, 0.1687910556793213, 0.16862286627292633, 0.16809862852096558]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wff9Ee2s1CAG",
        "outputId": "412060cb-519e-45b4-8abd-b74a7771ccb4"
      },
      "source": [
        "sae = StackedAutoencoder([128, 100, 80, 60], activation=None)\n",
        "sae.fit(bert_embeddings128, bert_embeddings128, False, 100)\n",
        "print(mse(bert_embeddings128, sae.decode(sae.encode(bert_embeddings128))))\n",
        "for hist in sae.model_histories:\n",
        "    print(hist.history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.16594523\n",
            "{'loss': [2.3141329288482666, 0.34249982237815857, 0.164223775267601, 0.10391067713499069, 0.07358289510011673, 0.05559741333127022, 0.04416601359844208, 0.03627673536539078, 0.030761288478970528, 0.026803404092788696, 0.02410363219678402, 0.0226656012237072, 0.021320100873708725, 0.020666595548391342, 0.01998809352517128, 0.019657649099826813, 0.019682541489601135, 0.019133001565933228, 0.019129199907183647, 0.018439676612615585, 0.018700234591960907, 0.018732544034719467, 0.018213342875242233, 0.018346363678574562, 0.018640480935573578, 0.01802714914083481, 0.01827395148575306, 0.017702829092741013, 0.018470682203769684, 0.017677266150712967, 0.018187154084444046, 0.017747392877936363, 0.01803511008620262, 0.017968909814953804, 0.017892103642225266, 0.01762782409787178, 0.017924048006534576, 0.017797164618968964, 0.017746716737747192, 0.01776125095784664, 0.01786052994430065, 0.017963990569114685, 0.01767815463244915, 0.017421087250113487, 0.017692087218165398, 0.01776663213968277, 0.017718195915222168, 0.017598949372768402, 0.017644094303250313, 0.017531735822558403, 0.0175994373857975, 0.017681894823908806, 0.017741762101650238, 0.01751009002327919, 0.017637861892580986, 0.017421433702111244, 0.01749049499630928, 0.017626145854592323, 0.017318962141871452, 0.01767057739198208, 0.017548557370901108, 0.01768088899552822, 0.017568323761224747, 0.0173687394708395, 0.017496757209300995, 0.01751621626317501, 0.017611883580684662, 0.01754366233944893, 0.017288826406002045, 0.017432620748877525, 0.01744093932211399, 0.017593149095773697, 0.017169121652841568, 0.01768665201961994, 0.017174823209643364, 0.01757282018661499, 0.017303021624684334, 0.017619917169213295, 0.017234355211257935, 0.017489701509475708, 0.01728828251361847, 0.017373811453580856, 0.017226651310920715, 0.017708320170640945, 0.017313899472355843, 0.017388129606842995, 0.017132775858044624, 0.017513612285256386, 0.017563560977578163, 0.017241157591342926, 0.01716781221330166, 0.017394723370671272, 0.017306046560406685, 0.01734921894967556, 0.01725957542657852, 0.017125342041254044, 0.017388856038451195, 0.01745295152068138, 0.017332253977656364, 0.01712234504520893], 'val_loss': [0.5548552870750427, 0.21597951650619507, 0.12587113678455353, 0.08485260605812073, 0.06280042976140976, 0.04895631968975067, 0.03987422212958336, 0.03271029517054558, 0.028216753154993057, 0.025541074573993683, 0.023730721324682236, 0.02144075557589531, 0.020982088521122932, 0.01994922384619713, 0.01927362009882927, 0.019663967192173004, 0.018535500392317772, 0.018283691257238388, 0.0182565338909626, 0.018189972266554832, 0.018154211342334747, 0.017807407304644585, 0.0179594736546278, 0.017508316785097122, 0.01752699352800846, 0.017594946548342705, 0.017650768160820007, 0.017941003665328026, 0.017455538734793663, 0.017663201317191124, 0.01727515645325184, 0.019706198945641518, 0.018892766907811165, 0.017477624118328094, 0.017166048288345337, 0.017442839220166206, 0.01756608672440052, 0.017529668286442757, 0.017463358119130135, 0.018369361758232117, 0.017129573971033096, 0.017096981406211853, 0.01727522537112236, 0.017784040421247482, 0.017005033791065216, 0.017323629930615425, 0.016991177573800087, 0.018269650638103485, 0.017586171627044678, 0.01738245226442814, 0.017488278448581696, 0.016999833285808563, 0.017145710065960884, 0.01700836606323719, 0.01683610863983631, 0.018034134060144424, 0.016788166016340256, 0.01732277125120163, 0.01838403381407261, 0.016829948872327805, 0.016923071816563606, 0.016807304695248604, 0.016891570761799812, 0.017066484317183495, 0.018496757373213768, 0.017191048711538315, 0.018020277842879295, 0.019028352573513985, 0.01671646721661091, 0.016698764637112617, 0.017092827707529068, 0.017190711572766304, 0.016927098855376244, 0.01654226891696453, 0.017119891941547394, 0.016955481842160225, 0.017784031108021736, 0.01701497659087181, 0.01677473820745945, 0.017091363668441772, 0.017042139545083046, 0.016731344163417816, 0.017943523824214935, 0.016698341816663742, 0.016741685569286346, 0.0179961659014225, 0.018543917685747147, 0.018503960222005844, 0.0164286307990551, 0.017042290419340134, 0.016718655824661255, 0.01684870384633541, 0.016913319006562233, 0.016720689833164215, 0.017591552808880806, 0.017159353941679, 0.01692507229745388, 0.01679479517042637, 0.016863223165273666, 0.016801193356513977]}\n",
            "{'loss': [0.9737851023674011, 0.12293946743011475, 0.0637439489364624, 0.04876235872507095, 0.04333974048495293, 0.040878619998693466, 0.03954117372632027, 0.03881873935461044, 0.03838297724723816, 0.03802148625254631, 0.03785689175128937, 0.037704844027757645, 0.037645574659109116, 0.03765708580613136, 0.03757157921791077, 0.037471216171979904, 0.03752027824521065, 0.03752806782722473, 0.03735045716166496, 0.037408746778964996, 0.03739146515727043, 0.03724603354930878, 0.037217579782009125, 0.037331536412239075, 0.03704497218132019, 0.03723442181944847, 0.037023112177848816, 0.03717096522450447, 0.03692515194416046, 0.03761545196175575, 0.036829300224781036, 0.03687754273414612, 0.03681880608201027, 0.03690953552722931, 0.03742755204439163, 0.036703888326883316, 0.03676312416791916, 0.03674439340829849, 0.036770254373550415, 0.03683595359325409, 0.03675176575779915, 0.036830440163612366, 0.03652358427643776, 0.03677593544125557, 0.03644778952002525, 0.03666936233639717, 0.03651925548911095, 0.03691215068101883, 0.03639277070760727, 0.03633466362953186, 0.03645118698477745, 0.03637867048382759, 0.036623258143663406, 0.036193687468767166, 0.036255430430173874, 0.0364384800195694, 0.036195058375597, 0.036189716309309006, 0.03617806360125542, 0.03618638217449188, 0.03601072356104851, 0.03621549531817436, 0.03618885204195976, 0.03586144000291824, 0.03597937151789665, 0.03582475706934929, 0.036050308495759964, 0.03600334748625755, 0.03564457967877388, 0.03582160919904709, 0.03574872016906738, 0.03569287061691284, 0.0355762243270874, 0.03578253090381622, 0.035593755543231964, 0.03558653965592384, 0.035352397710084915, 0.03580797463655472, 0.035357799381017685, 0.035334255546331406, 0.03537692874670029, 0.035818688571453094, 0.03512393310666084, 0.035254478454589844, 0.03529471904039383, 0.035129934549331665, 0.03520575910806656, 0.035218141973018646, 0.03501450642943382, 0.03510557487607002, 0.0350479893386364, 0.03509167954325676, 0.035020750015974045, 0.03512360155582428, 0.034910205751657486, 0.03487055376172066, 0.034959230571985245, 0.03507618606090546, 0.034821223467588425, 0.03485794737935066], 'val_loss': [0.20476460456848145, 0.07878267765045166, 0.05307083949446678, 0.04508637264370918, 0.041721828281879425, 0.03993276134133339, 0.039198487997055054, 0.038567375391721725, 0.03807206451892853, 0.03764929994940758, 0.03749906271696091, 0.037383660674095154, 0.037236589938402176, 0.037311989814043045, 0.03717873990535736, 0.03711731731891632, 0.03735348582267761, 0.037027377635240555, 0.03731822222471237, 0.03708532080054283, 0.03700410947203636, 0.03695019334554672, 0.03708895668387413, 0.03673499450087547, 0.03674556687474251, 0.03674947842955589, 0.03685890883207321, 0.036952294409275055, 0.03684806078672409, 0.036523401737213135, 0.036483149975538254, 0.036563143134117126, 0.036879923194646835, 0.037021342664957047, 0.036302708089351654, 0.03660133481025696, 0.036736901849508286, 0.03650907427072525, 0.036526329815387726, 0.03840607777237892, 0.036389343440532684, 0.03642032667994499, 0.0363265797495842, 0.03622192144393921, 0.03625953570008278, 0.03646666929125786, 0.036270614713430405, 0.036097776144742966, 0.036237914115190506, 0.035968996584415436, 0.03610948100686073, 0.036152176558971405, 0.03595474362373352, 0.03681749477982521, 0.035899724811315536, 0.035845573991537094, 0.03580128774046898, 0.035970646888017654, 0.03642045333981514, 0.03570258617401123, 0.03573819249868393, 0.035688530653715134, 0.03568876162171364, 0.03569762781262398, 0.03545726090669632, 0.03559119626879692, 0.03566247597336769, 0.035688240081071854, 0.03533094376325607, 0.03534330427646637, 0.036475665867328644, 0.03535052016377449, 0.035321690142154694, 0.03523042052984238, 0.035035230219364166, 0.035176366567611694, 0.035258788615465164, 0.03520036116242409, 0.03490881621837616, 0.034940384328365326, 0.034884773194789886, 0.03478478640317917, 0.03496500477194786, 0.03509865328669548, 0.03509434685111046, 0.034939102828502655, 0.03475392982363701, 0.034775082021951675, 0.03486696258187294, 0.034652356058359146, 0.03479063883423805, 0.03485410660505295, 0.03456491231918335, 0.035028330981731415, 0.03455235809087753, 0.03488771244883537, 0.034912511706352234, 0.0345468670129776, 0.03479073941707611, 0.03443191945552826]}\n",
            "{'loss': [0.313424289226532, 0.0774613618850708, 0.06180041655898094, 0.05751165747642517, 0.05585550516843796, 0.05513913929462433, 0.0546577088534832, 0.05432051792740822, 0.0539897158741951, 0.053644098341464996, 0.05330902710556984, 0.05287841707468033, 0.052411098033189774, 0.05203946679830551, 0.051702115684747696, 0.05152534320950508, 0.051402024924755096, 0.05132032558321953, 0.05126074329018593, 0.05121500417590141, 0.05118805542588234, 0.05116131156682968, 0.051162995398044586, 0.05114712566137314, 0.05110923945903778, 0.051102086901664734, 0.05108565464615822, 0.051106009632349014, 0.051053110510110855, 0.05104348435997963, 0.051043279469013214, 0.051033999770879745, 0.051028359681367874, 0.051037225872278214, 0.05100357532501221, 0.051008280366659164, 0.05102933943271637, 0.050988681614398956, 0.05098611116409302, 0.05098368972539902, 0.05097796022891998, 0.05098889023065567, 0.05098035931587219, 0.05094946548342705, 0.050956860184669495, 0.05095694586634636, 0.05092719569802284, 0.05099475011229515, 0.05092440918087959, 0.050927821546792984, 0.05093654245138168, 0.05092724412679672, 0.05090603604912758, 0.05094962194561958, 0.050900399684906006, 0.05098070576786995, 0.05088846758008003, 0.050904251635074615, 0.05090448632836342, 0.050908852368593216, 0.050883010029792786, 0.05091971904039383, 0.05088643729686737, 0.05087378993630409, 0.05090305581688881, 0.05086955055594444, 0.05087072029709816, 0.05088004097342491, 0.05087922886013985, 0.05087479576468468, 0.05087115988135338, 0.050878651440143585, 0.050850506871938705, 0.05088073015213013, 0.05084618180990219, 0.05086967721581459, 0.05084169656038284, 0.05087821185588837, 0.050843603909015656, 0.05083925276994705, 0.05086195841431618, 0.050836969166994095, 0.05083955079317093, 0.05087563768029213, 0.05083746090531349, 0.05084206163883209, 0.050835754722356796, 0.05082700401544571, 0.05083674192428589, 0.05085122957825661, 0.05084262415766716, 0.05082050338387489, 0.050843171775341034, 0.05082795023918152, 0.050846174359321594, 0.050806235522031784, 0.050814852118492126, 0.050884779542684555, 0.0507781058549881, 0.05080585926771164], 'val_loss': [0.10117755830287933, 0.0653466135263443, 0.05869545787572861, 0.056140556931495667, 0.05516375973820686, 0.05456369370222092, 0.054095059633255005, 0.05379817634820938, 0.05348055064678192, 0.05319606140255928, 0.05281566083431244, 0.05232483893632889, 0.051899950951337814, 0.051488012075424194, 0.051324110478162766, 0.05110051482915878, 0.05091538280248642, 0.050974320620298386, 0.05086522176861763, 0.05086945742368698, 0.050824299454689026, 0.050856199115514755, 0.05076833441853523, 0.050733115524053574, 0.05075468495488167, 0.05075829103589058, 0.050814662128686905, 0.05074043944478035, 0.05072953924536705, 0.05073879286646843, 0.05072483420372009, 0.05067984759807587, 0.0507417693734169, 0.050641078501939774, 0.05066974461078644, 0.05071832612156868, 0.05070436745882034, 0.05063396692276001, 0.050670333206653595, 0.05068793520331383, 0.05060780048370361, 0.050620149821043015, 0.05066092684864998, 0.05068868771195412, 0.0506400428712368, 0.050643257796764374, 0.05061473697423935, 0.05060017108917236, 0.05056565999984741, 0.050612758845090866, 0.05064966902136803, 0.05057130753993988, 0.05058242008090019, 0.050558485090732574, 0.05065150186419487, 0.05055854097008705, 0.050578851252794266, 0.05058692768216133, 0.05058703571557999, 0.05055888369679451, 0.05060867592692375, 0.05060078948736191, 0.05054079368710518, 0.05058591067790985, 0.05057374760508537, 0.05051735043525696, 0.05058508366346359, 0.0504942387342453, 0.0505203939974308, 0.0505509190261364, 0.05053531005978584, 0.05064656585454941, 0.050520818680524826, 0.05054386332631111, 0.050503507256507874, 0.05053020641207695, 0.05050203949213028, 0.05051407963037491, 0.050512708723545074, 0.05052782595157623, 0.05054306983947754, 0.05049806088209152, 0.05055985972285271, 0.05049750953912735, 0.05055081844329834, 0.05050482600927353, 0.050527285784482956, 0.05070370435714722, 0.050540730357170105, 0.05048859864473343, 0.050471656024456024, 0.05048990249633789, 0.05048723146319389, 0.050499726086854935, 0.05051901936531067, 0.05056477710604668, 0.050467636436223984, 0.05052236095070839, 0.05047460272908211, 0.05054616183042526]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdsyH-P_0_Sf",
        "outputId": "284b77b9-ed24-465f-f61d-f2e835d87695"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(60)\n",
        "pca_res = pca.fit_transform(bert_embeddings128)\n",
        "mse(pca.inverse_transform(pca_res), bert_embeddings128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.16024108"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYL1mgZz92b4"
      },
      "source": [
        "Пробую пошумить в автоенкодере"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvDDKXUW95D2"
      },
      "source": [
        "from keras.layers import Dropout\n",
        "\n",
        "INPUT_DIM = 128\n",
        "OUTPUT_DIM = 60\n",
        "ACTIVATION = 'LeakyReLU'\n",
        "\n",
        "encoder = Sequential([\n",
        "    InputLayer(input_shape=(INPUT_DIM,)),\n",
        "    Dense(100, activation=ACTIVATION),\n",
        "    # Dropout(0.03),\n",
        "    Dense(80, activation=ACTIVATION),\n",
        "    # Dense(90, activation='tanh'),\n",
        "    # Dense(32, activation='relu'),\n",
        "    Dense(OUTPUT_DIM, activation=ACTIVATION)\n",
        "])\n",
        "decoder = Sequential([\n",
        "    InputLayer(input_shape=(OUTPUT_DIM,)),\n",
        "    Dense(80, activation=ACTIVATION),\n",
        "    Dense(100, activation=ACTIVATION),\n",
        "    # Dense(32, activation='relu'),\n",
        "    # Dense(90, activation='tanh'),\n",
        "    Dense(INPUT_DIM, activation=None)\n",
        "])\n",
        "autoencoder = Model(inputs=encoder.input, outputs=decoder(encoder.output))\n",
        "autoencoder.compile(loss='mse', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MJzgOJ7J-O_q",
        "outputId": "1c337702-be55-48b7-cf35-913f0e5cf4c6"
      },
      "source": [
        "history = autoencoder.fit(x=bert_embeddings128, y=bert_embeddings128, batch_size=64, epochs=90, verbose=0, validation_split=0.1)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='val')\n",
        "plt.legend()\n",
        "plt.title('Loss for Autoencoder')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FeTnsvnIE5Cl",
        "outputId": "bba51dca-9553-4d82-b228-1e8d0cd8ae44"
      },
      "source": [
        "history = autoencoder.fit(x=bert_embeddings128, y=bert_embeddings128, batch_size=64, epochs=90, verbose=1, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.8709 - val_loss: 0.7184\n",
            "Epoch 2/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.7780 - val_loss: 0.6460\n",
            "Epoch 3/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.7090 - val_loss: 0.6029\n",
            "Epoch 4/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.6636 - val_loss: 0.5825\n",
            "Epoch 5/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.6313 - val_loss: 0.5515\n",
            "Epoch 6/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.6051 - val_loss: 0.5117\n",
            "Epoch 7/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.5766 - val_loss: 0.4912\n",
            "Epoch 8/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.5568 - val_loss: 0.4757\n",
            "Epoch 9/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.5424 - val_loss: 0.4531\n",
            "Epoch 10/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.5313 - val_loss: 0.4467\n",
            "Epoch 11/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.5195 - val_loss: 0.4228\n",
            "Epoch 12/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.5106 - val_loss: 0.4155\n",
            "Epoch 13/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.4991 - val_loss: 0.4195\n",
            "Epoch 14/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.4913 - val_loss: 0.4388\n",
            "Epoch 15/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.4817 - val_loss: 0.3898\n",
            "Epoch 16/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.4755 - val_loss: 0.3822\n",
            "Epoch 17/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.4704 - val_loss: 0.3798\n",
            "Epoch 18/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.4665 - val_loss: 0.3623\n",
            "Epoch 19/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.4644 - val_loss: 0.3741\n",
            "Epoch 20/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.4581 - val_loss: 0.3627\n",
            "Epoch 21/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.4492 - val_loss: 0.3648\n",
            "Epoch 22/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.4453 - val_loss: 0.3418\n",
            "Epoch 23/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.4396 - val_loss: 0.3436\n",
            "Epoch 24/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.4360 - val_loss: 0.3420\n",
            "Epoch 25/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.4319 - val_loss: 0.3385\n",
            "Epoch 26/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.4290 - val_loss: 0.3299\n",
            "Epoch 27/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.4242 - val_loss: 0.3243\n",
            "Epoch 28/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.4206 - val_loss: 0.3360\n",
            "Epoch 29/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.4167 - val_loss: 0.3205\n",
            "Epoch 30/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.4111 - val_loss: 0.3054\n",
            "Epoch 31/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.4053 - val_loss: 0.3134\n",
            "Epoch 32/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.4032 - val_loss: 0.3176\n",
            "Epoch 33/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.3955 - val_loss: 0.2938\n",
            "Epoch 34/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.3918 - val_loss: 0.2935\n",
            "Epoch 35/90\n",
            "430/430 [==============================] - 1s 3ms/step - loss: 0.3894 - val_loss: 0.2983\n",
            "Epoch 36/90\n",
            "276/430 [==================>...........] - ETA: 0s - loss: 0.3869"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-283ad0feb504>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbert_embeddings128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbert_embeddings128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 _r=1):\n\u001b[1;32m   1157\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6CIV4Jt-fwV",
        "outputId": "7188a6af-a44c-4569-8b00-5cc742b89e12"
      },
      "source": [
        "mse(bert_embeddings128, decoder(encoder(bert_embeddings128)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.15966973"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWMfbblaAEUD",
        "outputId": "5a0e7b96-577a-4d12-d4e1-608d555954b4"
      },
      "source": [
        "encoder(bert_embeddings128).numpy().mean(axis=1).max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.1568203"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    }
  ]
}